name: Build library

on:
  push:

env:
  LLAMACPP_VERSION: 8faa1d4d
  CMAKE_COMMON_JOBS: '-DLLAMA_BUILD_COMMON=ON -DGGML_STATIC=ON -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF -DBUILD_SHARED_LIBS=OFF'
  CMAKE_COMMON_DIR: -DCMAKE_RUNTIME_OUTPUT_DIRECTORY=${{ github.workspace }}/build/libs -DCMAKE_LIBRARY_OUTPUT_DIRECTORY=${{ github.workspace }}/build/libs

jobs:
  linux-build:
    runs-on: ubuntu-22.04

    env:
      CMAKE_COMMON: '-DBUILD_UNDREAMAI_BINARIES=ON -DGGML_NATIVE=OFF -DLLAMA_SERVER_SSL=ON -DCMAKE_BUILD_RPATH_USE_ORIGIN=ON'
      PLATFORM: linux

    strategy:
      matrix:
        include:
          - build: 'cuda-cu12.2.0'
            defines: '-DGGML_CUDA=ON -DCUDAToolkit_ROOT="$GITHUB_WORKSPACE/build/cuda" -DGGML_MINIMIZE_CODE_SIZE=ON -DGGML_NO_IQUANTS=ON'
            tinyBLAS: 'ON'
            
    steps:
        - id: setup_libs_linux
          name: Dependencies
          run: |
            sudo apt-get update
            sudo apt-get install -y build-essential cmake zip libssl-dev git

        - id: checkout
          name: Clone
          uses: actions/checkout@v4

        - id: setup_llama_cpp
          name: Setup llama.cpp
          run: |
            git clone https://github.com/ggerganov/llama.cpp llama.cpp
            cd llama.cpp
            git checkout ${{ env.LLAMACPP_VERSION }}
        
            git apply ../llama.cpp.patch
            if [[ "${{ matrix.tinyBLAS }}" != "" ]]; then
              echo "tinyBLAS"
              mv ggml/src/ggml-cuda/CMakeLists.txt CMakeLists.txt.ggml-cuda
              rm -r ggml/src/ggml-cuda
              cp -R ../tinyBLAS ggml/src/ggml-cuda
              mv CMakeLists.txt.ggml-cuda ggml/src/ggml-cuda/CMakeLists.txt
            fi
        
            for f in examples/server/public/*;do
                cmake -DINPUT="$f" -DOUTPUT="$(echo "$f" | sed -e 's:public/::g').hpp" -P "scripts/xxd.cmake"
            done
          shell: bash

        - id: add_licenses
          name: Add licenses
          run: |
            mkdir -p build/licenses build/libs
            cp llama.cpp/LICENSE build/licenses/llama.cpp.LICENSE.txt
            if [[ "${{ matrix.tinyBLAS }}" != "" ]]; then
              curl -o build/licenses/llamafile.LICENSE.txt -L https://raw.githubusercontent.com/Mozilla-Ocho/llamafile/main/LICENSE
            fi
          shell: bash

        - id: set_name
          name: Set name
          run: |
            LIBRARY_SUFFIX="${{ env.PLATFORM }}"
            if [ "${{ matrix.build }}" != "" ];then LIBRARY_SUFFIX="$LIBRARY_SUFFIX-${{ matrix.build }}"; fi
            NAME=undreamai-${{ github.ref_name }}-llamacpp-$LIBRARY_SUFFIX.zip
            echo "LIBRARY_SUFFIX=$LIBRARY_SUFFIX" >> $GITHUB_ENV
            echo "UPLOAD_NAME=$NAME" >> $GITHUB_ENV
            echo "UPLOAD_PATH=$NAME" >> $GITHUB_ENV
          shell: bash


        - id: setup_vulkan_linux
          if: matrix.build == 'vulkan'
          name: Dependencies Vulcan
          run: |
            wget -qO - https://packages.lunarg.com/lunarg-signing-key-pub.asc | sudo apt-key add -
            sudo wget -qO /etc/apt/sources.list.d/lunarg-vulkan-jammy.list https://packages.lunarg.com/vulkan/lunarg-vulkan-jammy.list
            sudo apt-get update -y
            sudo apt-get install -y build-essential vulkan-sdk
            cp /lib/x86_64-linux-gnu/libvulkan.so.1 build/libs/

        - id: set_cuda_var
          if: startsWith(matrix.build, 'cuda')
          name: Set CUDA variable
          run: |
            echo "CUDA=$(echo "${{ matrix.build }}" | cut -d '-' -f2 | cut -c 3- )" >> $GITHUB_ENV
            echo "LD_LIBRARY_PATH=''" >> $GITHUB_ENV
          shell: bash

        - id: setup_cuda_linux
          if: startsWith(matrix.build, 'cuda')
          uses: Jimver/cuda-toolkit@v0.2.15
          with:
            cuda: ${{ env.CUDA }}
            linux-local-args: '["--toolkit"]'
            method: network

        - id: link_cuda_linux
          if: startsWith(matrix.build, 'cuda')
          name: Link Cuda
          run: |
            ln -s ${{ env.CUDA_PATH }} ${{ github.workspace }}/build/cuda


        - id: cpu-cores
          name: Get number of CPU cores
          uses: SimenB/github-actions-cpu-cores@v2

        - id: cmake_build
          name: Build
          run: |
            cd build
            cmake .. -DLIBRARY_SUFFIX="${{ env.LIBRARY_SUFFIX }}" ${{ matrix.defines }} ${{ env.CMAKE_COMMON }} ${{ env.CMAKE_COMMON_JOBS }} ${{ env.CMAKE_COMMON_DIR }}
            cmake --build . --config Release -j ${{ steps.cpu-cores.outputs.count }}

        - id: test_build_unix
          if: matrix.build == 'noavx' || matrix.build == 'avx' || matrix.build == 'avx2' ||
            matrix.build == 'acc' || matrix.build == 'no_acc'
          name: Test
          run: |
            cd build/libs
            curl -L -o model.gguf https://huggingface.co/afrideva/smol_llama-220M-openhermes-GGUF/resolve/main/smol_llama-220m-openhermes.q4_k_m.gguf?download=true
            ./undreamai_test -m model.gguf -np 1 --log-disable
            rm model.gguf


        - id: pack_artifacts_unix
          name: Pack artifacts
          run: |
            rm -f build/libs/undreamai_test
            zip -j ${{ env.UPLOAD_NAME }} build/licenses/* build/libs/*

        - id: upload
          name: Upload Artifacts
          uses: actions/upload-artifact@v4
          with:
            name: ${{ env.UPLOAD_NAME }}
            path: ${{ env.UPLOAD_PATH }}
