name: Build library

on:
  push:
    paths:
      - .github/workflows/build_library.yaml
      - CMakeLists.txt
      - undreamai.h
      - undreamai.cpp
    tags:
      - 'v*'

env:
  LLAMACPP_VERSION: b3623
  CMAKE_COMMON_JOBS: '-DGGML_STATIC=ON -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF -DBUILD_UNDREAMAI_SERVER=ON -DBUILD_SHARED_LIBS=OFF -DLLAMA_SERVER_SSL=ON'
  CMAKE_COMMON_DIR: -DCMAKE_RUNTIME_OUTPUT_DIRECTORY=${{ github.workspace }}/build/libs -DCMAKE_LIBRARY_OUTPUT_DIRECTORY=${{ github.workspace }}/build/libs

jobs:
  ################################ ArchChecker ################################

  archchecker_linux_build:
    name: Build ArchChecker Linux
    runs-on: ubuntu-latest

    env:
        UPLOAD_NAME: linux-archchecker
        UPLOAD_PATH: archchecker/build/libarchchecker.so

    steps:
        - id: checkout_recursive
          name: Clone
          uses: actions/checkout@v4
          with:
            submodules: recursive

        - id: build_archchecker
          name: Build
          run: |
            mkdir archchecker/build
            cd archchecker/build
            cmake ..
            cmake --build . --config Release -j $(nproc)

        - id: upload
          name: Upload Artifacts
          uses: actions/upload-artifact@v4
          with:
            name: ${{ env.UPLOAD_NAME }}
            path: ${{ env.UPLOAD_PATH }}


  archchecker_windows_build:
    name: Build ArchChecker Windows
    runs-on: windows-2019

    env:
        UPLOAD_NAME: windows-archchecker
        UPLOAD_PATH: archchecker/build/Release/archchecker.dll

    steps:
        - id: checkout_recursive
          name: Clone
          uses: actions/checkout@v4
          with:
            submodules: recursive

        - id: build_archchecker
          name: Build
          run: |
            mkdir archchecker/build
            cd archchecker/build
            cmake ..
            cmake --build . --config Release -j $(nproc)

        - id: upload
          name: Upload Artifacts
          uses: actions/upload-artifact@v4
          with:
            name: ${{ env.UPLOAD_NAME }}
            path: ${{ env.UPLOAD_PATH }}



  ################################ Linux ################################

  linux-build:
    runs-on: ubuntu-22.04

    env:
      CMAKE_COMMON: '-DGGML_NATIVE=OFF -DCMAKE_BUILD_RPATH_USE_ORIGIN=ON'
      PLATFORM: linux

    strategy:
      matrix:
        include:
          - build: 'noavx'
            defines: '-DGGML_AVX=OFF -DGGML_AVX2=OFF -DGGML_FMA=OFF'
          - build: 'avx2'
            defines: ''
          - build: 'avx'
            defines: '-DGGML_AVX2=OFF'
          - build: 'avx512'
            defines: '-DGGML_AVX512=ON'
          - build: 'vulkan'
            defines: '-DGGML_VULKAN=ON'
          - build: 'cuda-cu11.7.1'
            defines: '-DGGML_CUDA=ON -DCUDAToolkit_ROOT="$GITHUB_WORKSPACE/build/cuda"'
            minimise: 'GGML_MINIMIZE_CODE_SIZE GGML_NO_IQUANTS'
          - build: 'cuda-cu12.2.0'
            defines: '-DGGML_CUDA=ON -DCUDAToolkit_ROOT="$GITHUB_WORKSPACE/build/cuda"'
            minimise: 'GGML_MINIMIZE_CODE_SIZE GGML_NO_IQUANTS'
          - build: 'cuda-cu11.7.1-full'
            defines: '-DGGML_CUDA=ON -DCUDAToolkit_ROOT="$GITHUB_WORKSPACE/build/cuda"'
          - build: 'cuda-cu12.2.0-full'
            defines: '-DGGML_CUDA=ON -DCUDAToolkit_ROOT="$GITHUB_WORKSPACE/build/cuda"'
            
    steps:
        - id: checkout
          name: Clone
          uses: actions/checkout@v4

        - id: setup_libs_linux
          name: Dependencies
          run: |
            sudo apt-get update
            sudo apt-get install -y build-essential cmake zip libssl-dev

        - id: setup_llama_cpp_linux
          name: Clone llama.cpp
          run: |
            git clone https://github.com/ggerganov/llama.cpp llama.cpp
            cd llama.cpp
            git checkout ${{ env.LLAMACPP_VERSION }}
        
            sed -i.bak 's:utils.hpp:utils_callback.hpp:g' examples/server/server.cpp
            sed -i.bak 's:main(int argc:main_server(int argc:g' examples/server/server.cpp
            sed -i.bak 's:exit(1):std\:\:terminate():g' ggml/src/ggml-vulkan.cpp
            sed -i.bak 's:abort():raise(SIGSEGV):g' ggml/src/ggml.c
            sed -i.bak 's:GGML_NORETURN GGML_ATTRIBUTE_FORMAT://GGML_NORETURN GGML_ATTRIBUTE_FORMAT:g' ggml/include/ggml.h
        
            for f in examples/server/public/*;do
              cmake -DINPUT=$f -DOUTPUT=`echo $f|sed -e 's:public/::g'`.hpp -P scripts/xxd.cmake
            done
        
            cd ..
            mkdir -p build/licenses build/libs
            cp llama.cpp/LICENSE build/licenses/llama.cpp.LICENSE.txt

        - id: setup_vulcan_linux
          if: matrix.build == 'vulkan'
          name: Dependencies Vulcan
          run: |
            wget -qO - https://packages.lunarg.com/lunarg-signing-key-pub.asc | sudo apt-key add -
            sudo wget -qO /etc/apt/sources.list.d/lunarg-vulkan-jammy.list https://packages.lunarg.com/vulkan/lunarg-vulkan-jammy.list
            sudo apt-get update -y
            sudo apt-get install -y build-essential vulkan-sdk
            cp /lib/x86_64-linux-gnu/libvulkan.so.1 build/libs/

        - id: set_cuda_var_linux
          if: startsWith(matrix.build, 'cuda')
          name: Set CUDA variable
          run: |
            echo "CUDA=$(echo "${{ matrix.build }}" | cut -d '-' -f2 | cut -c 3- )" >> $GITHUB_ENV

        - id: prepare_cuda
          if: startsWith(matrix.build, 'cuda') || startsWith(matrix.build, 'hip')
          name: Prepare CUDA
          run: |
            cp tinyBLAS/* llama.cpp/ggml/src/
            rm -r llama.cpp/ggml/src/ggml-cuda
            sed -i 's:GGML_USE_CUDA:GGML_USE_CUDA GGML_USE_TINYBLAS NDEBUG ${{ matrix.minimise }}:g' llama.cpp/ggml/src/CMakeLists.txt
            curl -o build/licenses/llamafile.LICENSE.txt -L https://raw.githubusercontent.com/Mozilla-Ocho/llamafile/main/LICENSE

        - id: setup_cuda_linux
          if: startsWith(matrix.build, 'cuda')
          uses: Jimver/cuda-toolkit@v0.2.15
          with:
            cuda: ${{ env.CUDA }}
            linux-local-args: '["--toolkit"]'
            method: network

        - id: link_cuda_linux
          if: startsWith(matrix.build, 'cuda')
          name: Link Cuda
          run: |
            ln -s ${{ env.CUDA_PATH }} ${{ github.workspace }}/build/cuda


        - id: set_build_params_linux
          name: Set build parameters
          run: |
            echo "BUILD_PARAMS=${{ matrix.defines }} ${{ env.CMAKE_COMMON }} ${{ env.CMAKE_COMMON_JOBS }} ${{ env.CMAKE_COMMON_DIR }}" >> $GITHUB_ENV

        - id: cmake_build_linux
          name: Build
          run: |
            sed -i "s:LIBRARY undreamai:LIBRARY undreamai_${{ env.PLATFORM }}-${{ matrix.build }}:g" CMakeLists.txt
            export LD_LIBRARY_PATH=""
            cd build
            cmake .. ${{ env.BUILD_PARAMS }}
            cmake --build . --config Release -j $(nproc)

        - id: test_build_unix
          if: matrix.build == 'noavx' || matrix.build == 'avx' || matrix.build == 'avx2' ||
            matrix.build == 'acc' || matrix.build == 'no_acc'
          name: Test
          run: |
            cd build/libs
            curl -L -o model.gguf https://huggingface.co/afrideva/smol_llama-220M-openhermes-GGUF/resolve/main/smol_llama-220m-openhermes.q4_k_m.gguf?download=true
            ./undreamai_test -m model.gguf -np 1 --log-disable
            rm model.gguf


        - id: set_upload_name_unix
          name: Set upload name
          run: |
            NAME=undreamai-${{ github.ref_name }}-llamacpp-${{ env.PLATFORM }}
            if [ "${{ matrix.build }}" != "" ];then NAME=$NAME-${{ matrix.build }}; fi
            NAME=$NAME.zip
            echo "UPLOAD_NAME=$NAME" >> $GITHUB_ENV
            echo "UPLOAD_PATH=$NAME" >> $GITHUB_ENV

        - id: pack_artifacts_unix
          name: Pack artifacts
          run: |
            rm build/libs/undreamai_test
            zip -j ${{ env.UPLOAD_NAME }} build/licenses/* build/libs/*

        - id: upload
          name: Upload Artifacts
          uses: actions/upload-artifact@v4
          with:
            name: ${{ env.UPLOAD_NAME }}
            path: ${{ env.UPLOAD_PATH }}


  linux-hip:
    runs-on: ubuntu-22.04
    container: rocm/dev-ubuntu-22.04:6.0.2

    env:
        CMAKE_COMMON: '-DAMDGPU_TARGETS="gfx1030,gfx1031,gfx1032,gfx1100,gfx1101,gfx1102" -DGGML_NATIVE=OFF -DCMAKE_BUILD_RPATH_USE_ORIGIN=ON'
        PLATFORM: linux

    strategy:
      matrix:
        include:
          - build: 'hip'
            minimise: 'GGML_MINIMIZE_CODE_SIZE GGML_NO_IQUANTS'
          - build: 'hip-full'
            minimise: ''

    steps:
        - id: checkout
          name: Clone
          uses: actions/checkout@v4

        - id: setup_libs_linux
          name: Dependencies
          run: |
            sudo apt-get update
            sudo apt-get install -y build-essential cmake zip libssl-dev

        - id: setup_hip_linux
          name: Dependencies
          run: |
            sudo apt-get install -y git rocblas-dev hipblas-dev

        - id: setup_llama_cpp_linux
          name: Clone llama.cpp
          run: |
            git clone https://github.com/ggerganov/llama.cpp llama.cpp
            cd llama.cpp
            git checkout ${{ env.LLAMACPP_VERSION }}
        
            sed -i.bak 's:utils.hpp:utils_callback.hpp:g' examples/server/server.cpp
            sed -i.bak 's:main(int argc:main_server(int argc:g' examples/server/server.cpp
            sed -i.bak 's:exit(1):std\:\:terminate():g' ggml/src/ggml-vulkan.cpp
            sed -i.bak 's:abort():raise(SIGSEGV):g' ggml/src/ggml.c
            sed -i.bak 's:GGML_NORETURN GGML_ATTRIBUTE_FORMAT://GGML_NORETURN GGML_ATTRIBUTE_FORMAT:g' ggml/include/ggml.h
        
            for f in examples/server/public/*;do
              cmake -DINPUT=$f -DOUTPUT=`echo $f|sed -e 's:public/::g'`.hpp -P scripts/xxd.cmake
            done
        
            cd ..
            mkdir -p build/licenses build/libs
            cp llama.cpp/LICENSE build/licenses/llama.cpp.LICENSE.txt

        - id: set_cuda_var_linux
          if: startsWith(matrix.build, 'cuda')
          name: Set CUDA variable
          run: |
            echo "CUDA=$(echo "${{ matrix.build }}" | cut -d '-' -f2 | cut -c 3- )" >> $GITHUB_ENV

        - id: prepare_cuda
          if: startsWith(matrix.build, 'cuda') || startsWith(matrix.build, 'hip')
          name: Prepare CUDA
          run: |
            cp tinyBLAS/* llama.cpp/ggml/src/
            rm -r llama.cpp/ggml/src/ggml-cuda
            sed -i 's:GGML_USE_CUDA:GGML_USE_CUDA GGML_USE_TINYBLAS NDEBUG ${{ matrix.minimise }}:g' llama.cpp/ggml/src/CMakeLists.txt
            curl -o build/licenses/llamafile.LICENSE.txt -L https://raw.githubusercontent.com/Mozilla-Ocho/llamafile/main/LICENSE


        - id: set_build_params_hip_linux
          name: Set build parameters
          run: |
            sed -i "s:FATAL_ERROR:WARNING:g" llama.cpp/ggml/src/CMakeLists.txt
            echo "BUILD_PARAMS=${{ env.CMAKE_COMMON }} ${{ env.CMAKE_COMMON_JOBS }} -DCMAKE_HIP_COMPILER="$(hipconfig -l)/clang" -DGGML_HIPBLAS=ON -DCMAKE_RUNTIME_OUTPUT_DIRECTORY=`pwd`/build/libs -DCMAKE_LIBRARY_OUTPUT_DIRECTORY=`pwd`/build/libs" >> $GITHUB_ENV

        - id: cmake_build_linux
          name: Build
          run: |
            sed -i "s:LIBRARY undreamai:LIBRARY undreamai_${{ env.PLATFORM }}-${{ matrix.build }}:g" CMakeLists.txt
            export LD_LIBRARY_PATH=""
            cd build
            cmake .. ${{ env.BUILD_PARAMS }}
            cmake --build . --config Release -j $(nproc)


        - id: set_upload_name_unix
          name: Set upload name
          run: |
            NAME=undreamai-${{ github.ref_name }}-llamacpp-${{ env.PLATFORM }}
            if [ "${{ matrix.build }}" != "" ];then NAME=$NAME-${{ matrix.build }}; fi
            NAME=$NAME.zip
            echo "UPLOAD_NAME=$NAME" >> $GITHUB_ENV
            echo "UPLOAD_PATH=$NAME" >> $GITHUB_ENV

        - id: pack_artifacts_unix
          name: Pack artifacts
          run: |
            rm build/libs/undreamai_test
            zip -j ${{ env.UPLOAD_NAME }} build/licenses/* build/libs/*

        - id: upload
          name: Upload Artifacts
          uses: actions/upload-artifact@v4
          with:
            name: ${{ env.UPLOAD_NAME }}
            path: ${{ env.UPLOAD_PATH }}


################################ macOS ################################

  macOS-arm64-build:
    runs-on: macos-14

    env:
        PLATFORM: macos-arm64

    strategy:
      matrix:
        include:
          - build: 'acc'
            defines: ''
          - build: 'no_acc'
            defines: '-DGGML_ACCELERATE=OFF -DGGML_BLAS=OFF'

    steps:
        - id: checkout
          name: Clone
          uses: actions/checkout@v4

        - continue-on-error: true
          id: setup_libs_macos
          name: Dependencies
          run: |
            brew update

        - id: setup_llama_cpp_linux
          name: Clone llama.cpp
          run: |
            git clone https://github.com/ggerganov/llama.cpp llama.cpp
            cd llama.cpp
            git checkout ${{ env.LLAMACPP_VERSION }}
        
            sed -i.bak 's:utils.hpp:utils_callback.hpp:g' examples/server/server.cpp
            sed -i.bak 's:main(int argc:main_server(int argc:g' examples/server/server.cpp
            sed -i.bak 's:exit(1):std\:\:terminate():g' ggml/src/ggml-vulkan.cpp
            sed -i.bak 's:abort():raise(SIGSEGV):g' ggml/src/ggml.c
            sed -i.bak 's:GGML_NORETURN GGML_ATTRIBUTE_FORMAT://GGML_NORETURN GGML_ATTRIBUTE_FORMAT:g' ggml/include/ggml.h
        
            for f in examples/server/public/*;do
              cmake -DINPUT=$f -DOUTPUT=`echo $f|sed -e 's:public/::g'`.hpp -P scripts/xxd.cmake
            done
        
            cd ..
            mkdir -p build/licenses build/libs
            cp llama.cpp/LICENSE build/licenses/llama.cpp.LICENSE.txt


        - id: set_build_params_macos_arm64
          name: Set build parameters
          run: |
            echo "BUILD_PARAMS=-DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL_EMBED_LIBRARY=ON -DLLAMA_CURL=ON ${{ matrix.defines }} ${{ env.CMAKE_COMMON_JOBS }} ${{ env.CMAKE_COMMON_DIR }}" >> $GITHUB_ENV

        - id: cmake_build_macos
          name: Build
          run: |
            sed -i.bak "s:LIBRARY undreamai:LIBRARY undreamai_${{ env.PLATFORM }}-${{ matrix.build }}:g" CMakeLists.txt
            cd build
            cmake .. ${{ env.BUILD_PARAMS }}
            cmake --build . --config Release -j $(sysctl -n hw.logicalcpu)


        - id: set_upload_name_unix
          name: Set upload name
          run: |
            NAME=undreamai-${{ github.ref_name }}-llamacpp-${{ env.PLATFORM }}
            if [ "${{ matrix.build }}" != "" ];then NAME=$NAME-${{ matrix.build }}; fi
            NAME=$NAME.zip
            echo "UPLOAD_NAME=$NAME" >> $GITHUB_ENV
            echo "UPLOAD_PATH=$NAME" >> $GITHUB_ENV

        - id: pack_artifacts_unix
          name: Pack artifacts
          run: |
            rm build/libs/undreamai_test
            zip -j ${{ env.UPLOAD_NAME }} build/licenses/* build/libs/*

        - id: upload
          name: Upload Artifacts
          uses: actions/upload-artifact@v4
          with:
            name: ${{ env.UPLOAD_NAME }}
            path: ${{ env.UPLOAD_PATH }}


  macOS-x64-build:
    runs-on: macos-12

    env:
        PLATFORM: macos-x64

    strategy:
      matrix:
        include:
          - build: 'acc'
            defines: ''
          - build: 'no_acc'
            defines: '-DGGML_ACCELERATE=OFF -DGGML_BLAS=OFF'

    steps:
        - id: checkout
          name: Clone
          uses: actions/checkout@v4

        - continue-on-error: true
          id: setup_libs_macos
          name: Dependencies
          run: |
            brew update

        - id: setup_llama_cpp_linux
          name: Clone llama.cpp
          run: |
            git clone https://github.com/ggerganov/llama.cpp llama.cpp
            cd llama.cpp
            git checkout ${{ env.LLAMACPP_VERSION }}
        
            sed -i.bak 's:utils.hpp:utils_callback.hpp:g' examples/server/server.cpp
            sed -i.bak 's:main(int argc:main_server(int argc:g' examples/server/server.cpp
            sed -i.bak 's:exit(1):std\:\:terminate():g' ggml/src/ggml-vulkan.cpp
            sed -i.bak 's:abort():raise(SIGSEGV):g' ggml/src/ggml.c
            sed -i.bak 's:GGML_NORETURN GGML_ATTRIBUTE_FORMAT://GGML_NORETURN GGML_ATTRIBUTE_FORMAT:g' ggml/include/ggml.h
        
            for f in examples/server/public/*;do
              cmake -DINPUT=$f -DOUTPUT=`echo $f|sed -e 's:public/::g'`.hpp -P scripts/xxd.cmake
            done
        
            cd ..
            mkdir -p build/licenses build/libs
            cp llama.cpp/LICENSE build/licenses/llama.cpp.LICENSE.txt


        - id: set_build_params_macos_x64
          name: Set build parameters
          run: |
            echo "BUILD_PARAMS=-DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=OFF -DLLAMA_CURL=ON ${{ matrix.defines }} ${{ env.CMAKE_COMMON_JOBS }} ${{ env.CMAKE_COMMON_DIR }}" >> $GITHUB_ENV

        - id: cmake_build_macos
          name: Build
          run: |
            sed -i.bak "s:LIBRARY undreamai:LIBRARY undreamai_${{ env.PLATFORM }}-${{ matrix.build }}:g" CMakeLists.txt
            cd build
            cmake .. ${{ env.BUILD_PARAMS }}
            cmake --build . --config Release -j $(sysctl -n hw.logicalcpu)

        - id: test_build_unix
          if: matrix.build == 'noavx' || matrix.build == 'avx' || matrix.build == 'avx2' ||
            matrix.build == 'acc' || matrix.build == 'no_acc'
          name: Test
          run: |
            cd build/libs
            curl -L -o model.gguf https://huggingface.co/afrideva/smol_llama-220M-openhermes-GGUF/resolve/main/smol_llama-220m-openhermes.q4_k_m.gguf?download=true
            ./undreamai_test -m model.gguf -np 1 --log-disable
            rm model.gguf


        - id: set_upload_name_unix
          name: Set upload name
          run: |
            NAME=undreamai-${{ github.ref_name }}-llamacpp-${{ env.PLATFORM }}
            if [ "${{ matrix.build }}" != "" ];then NAME=$NAME-${{ matrix.build }}; fi
            NAME=$NAME.zip
            echo "UPLOAD_NAME=$NAME" >> $GITHUB_ENV
            echo "UPLOAD_PATH=$NAME" >> $GITHUB_ENV

        - id: pack_artifacts_unix
          name: Pack artifacts
          run: |
            rm build/libs/undreamai_test
            zip -j ${{ env.UPLOAD_NAME }} build/licenses/* build/libs/*

        - id: upload
          name: Upload Artifacts
          uses: actions/upload-artifact@v4
          with:
            name: ${{ env.UPLOAD_NAME }}
            path: ${{ env.UPLOAD_PATH }}



################################ Windows ################################

  windows-build:
    runs-on: windows-2019

    env:
      CMAKE_COMMON: '-DGGML_NATIVE=OFF'
      VULKAN_VERSION: 1.3.261.1
      PLATFORM: windows

    strategy:
      matrix:
        include:
          - build: 'noavx'
            defines: '-DGGML_AVX=OFF -DGGML_AVX2=OFF -DGGML_FMA=OFF'
          - build: 'avx2'
            defines: ''
          - build: 'avx'
            defines: '-DGGML_AVX2=OFF'
          - build: 'avx512'
            defines: '-DGGML_AVX512=ON'
          - build: 'vulkan'
            defines: '-DGGML_VULKAN=ON'
          - build: 'arm64'
            defines: '-G Ninja -D CMAKE_TOOLCHAIN_FILE=../llama.cpp/cmake/arm64-windows-llvm.cmake'
          - build: 'cuda-cu11.7.1'
            defines: '-DGGML_CUDA=ON'
            minimise: 'GGML_MINIMIZE_CODE_SIZE GGML_NO_IQUANTS'
          - build: 'cuda-cu12.2.0'
            defines: '-DGGML_CUDA=ON'
            minimise: 'GGML_MINIMIZE_CODE_SIZE GGML_NO_IQUANTS'
          - build: 'cuda-cu11.7.1-full'
            defines: '-DGGML_CUDA=ON'
          - build: 'cuda-cu12.2.0-full'
            defines: '-DGGML_CUDA=ON'

    steps:
        - id: checkout
          name: Clone
          uses: actions/checkout@v4

        - id: setup_llama_cpp_windows
          name: Clone llama.cpp
          run: |
            git clone https://github.com/ggerganov/llama.cpp llama.cpp
            cd llama.cpp
            git checkout ${{ env.LLAMACPP_VERSION }}
            (Get-Content examples/server/server.cpp ) -replace 'utils.hpp', 'utils_callback.hpp' | Set-Content examples/server/server.cpp
            (Get-Content examples/server/server.cpp ) -replace 'main\(int argc', 'main_server(int argc' | Set-Content examples/server/server.cpp
            (Get-Content ggml/src/ggml-vulkan.cpp ) -replace 'exit\(1', 'std::terminate(' | Set-Content ggml/src/ggml-vulkan.cpp
            (Get-Content ggml/src/ggml.c ) -replace 'abort\(\)', 'raise(SIGSEGV)' | Set-Content ggml/src/ggml.c
            (Get-Content ggml/include/ggml.h ) -replace 'GGML_NORETURN GGML_ATTRIBUTE_FORMAT', '//GGML_NORETURN GGML_ATTRIBUTE_FORMAT' | Set-Content ggml/include/ggml.h
        
            Get-ChildItem examples/server/public/* | ForEach-Object {
              $f = $_.FullName
              $output = $f -replace "public\\", ""
              echo "cmake -DINPUT=$f -DOUTPUT=$output.hpp -P scripts/xxd.cmake"
              cmake -DINPUT="$f" -DOUTPUT="$output.hpp" -P "scripts/xxd.cmake"
            }
        
            cd ..
            mkdir -p build/licenses/

        - id: setup_vulcan_windows
          if: matrix.build == 'vulkan'
          name: Install Vulkan SDK
          run: |
            curl.exe -o $env:RUNNER_TEMP/VulkanSDK-Installer.exe -L "https://sdk.lunarg.com/sdk/download/${env:VULKAN_VERSION}/windows/VulkanSDK-${env:VULKAN_VERSION}-Installer.exe"
            & "$env:RUNNER_TEMP\VulkanSDK-Installer.exe" --accept-licenses --default-answer --confirm-command install
            Add-Content $env:GITHUB_ENV "VULKAN_SDK=C:\VulkanSDK\${env:VULKAN_VERSION}"
            Add-Content $env:GITHUB_PATH "C:\VulkanSDK\${env:VULKAN_VERSION}\bin"
            curl.exe -o $env:RUNNER_TEMP/VulkanRT-Components.zip -L "https://sdk.lunarg.com/sdk/download/1.3.283.0/windows/VulkanRT-1.3.283.0-Components.zip"
            7z x "-o${env:RUNNER_TEMP}" $env:RUNNER_TEMP/VulkanRT-Components.zip
            mkdir .\build\libs\Release
            cp ${env:RUNNER_TEMP}/VulkanRT*\x64\vulkan-1.dll .\build\libs\Release

        - id: setup_ninja_windows
          if: matrix.build == 'arm64'
          name: Install Ninja
          run: |
            choco install ninja

        - id: set_cuda_var_windows
          if: startsWith(matrix.build, 'cuda')
          name: Set CUDA variable
          run: |
            $pattern = "\d+\.\d+\.\d+"
            $CUDA = [regex]::Match("${{ matrix.build }}", $pattern).Value
            Add-Content $env:GITHUB_ENV "CUDA=$CUDA"

        - id: prepare_cuda
          if: startsWith(matrix.build, 'cuda') || startsWith(matrix.build, 'hip')
          name: Prepare CUDA
          run: |
            cp tinyBLAS/* llama.cpp/ggml/src/
            rm -r llama.cpp/ggml/src/ggml-cuda
            sed -i 's:GGML_USE_CUDA:GGML_USE_CUDA GGML_USE_TINYBLAS NDEBUG ${{ matrix.minimise }}:g' llama.cpp/ggml/src/CMakeLists.txt
            curl -o build/licenses/llamafile.LICENSE.txt -L https://raw.githubusercontent.com/Mozilla-Ocho/llamafile/main/LICENSE

        - id: setup_cuda_windows
          if: startsWith(matrix.build, 'cuda')
          uses: Jimver/cuda-toolkit@v0.2.15
          with:
            cuda: ${{ env.CUDA }}
            method: network
            sub-packages: '["nvcc", "cudart", "cublas", "cublas_dev", "thrust", "visual_studio_integration"]'


        - id: cpu-cores
          name: Get number of CPU cores
          uses: SimenB/github-actions-cpu-cores@v2

        - id: cmake_build_windows
          name: Build
          run: |
            (Get-Content CMakeLists.txt ) -replace 'LIBRARY undreamai', 'LIBRARY undreamai_windows-${{ matrix.build }}' | Set-Content CMakeLists.txt
            cd build
            cmake .. ${{ matrix.defines }} ${{ env.CMAKE_COMMON }} ${{ env.CMAKE_COMMON_JOBS }} ${{ env.CMAKE_COMMON_DIR }}
            cmake --build . --config Release -j ${{ steps.cpu-cores.outputs.count }}


        - id: move_files_windows
          if: matrix.build == 'arm64' || startsWith(matrix.build, 'hip')
          name: Move files to Release folder
          run: |
            mkdir build/Release
            mkdir build/libs/Release
            move build/libs/*exe build/libs/Release
            move build/libs/*dll build/libs/Release
            move build/libs/*exp build/libs/Release
            move build/libs/*lib build/libs/Release

        - id: set_upload_name_windows
          name: Set upload name
          run: |
            $NAME = "undreamai-${{ github.ref_name }}-llamacpp-${{ env.PLATFORM }}-${{ matrix.build }}.zip"
            Add-Content $env:GITHUB_ENV "UPLOAD_NAME=$NAME"
            Add-Content $env:GITHUB_ENV "UPLOAD_PATH=$NAME"

        - id: pack_artifacts_windows
          name: Pack artifacts
          run: |
            ls -R build
            del build/libs/Release/undreamai_test.exe
            mkdir artifacts
            Copy-Item .\build\licenses\* .\artifacts\
            Copy-Item .\build\Release\* .\artifacts\
            Copy-Item .\build\libs\Release\*dll .\artifacts\
            $serverPath = ".\build\libs\Release\undreamai_server.exe"
            if (Test-Path $serverPath) {
                Copy-Item $serverPath -Destination ".\artifacts\"
            }
            cd artifacts
            7z a ../${{ env.UPLOAD_NAME }} *

        - id: upload
          name: Upload Artifacts
          uses: actions/upload-artifact@v4
          with:
            name: ${{ env.UPLOAD_NAME }}
            path: ${{ env.UPLOAD_PATH }}


  windows-hip:
    runs-on: windows-2019

    env:
      CMAKE_COMMON: '-DAMDGPU_TARGETS="gfx1030,gfx1031,gfx1032,gfx1100,gfx1101,gfx1102" -DGGML_NATIVE=OFF -DCMAKE_RUNTIME_OUTPUT_DIRECTORY=${{ github.workspace }}/build/libs -DCMAKE_LIBRARY_OUTPUT_DIRECTORY=${{ github.workspace }}/build/libs'
      PLATFORM: windows
    
    strategy:
      matrix:
        include:
          - build: 'hip'
            minimise: 'GGML_MINIMIZE_CODE_SIZE GGML_NO_IQUANTS'
          - build: 'hip-full'
            minimise: ''

    steps:
        - id: checkout
          name: Clone
          uses: actions/checkout@v4

        - id: setup_llama_cpp_windows
          name: Clone llama.cpp
          run: |
            git clone https://github.com/ggerganov/llama.cpp llama.cpp
            cd llama.cpp
            git checkout ${{ env.LLAMACPP_VERSION }}
            (Get-Content examples/server/server.cpp ) -replace 'utils.hpp', 'utils_callback.hpp' | Set-Content examples/server/server.cpp
            (Get-Content examples/server/server.cpp ) -replace 'main\(int argc', 'main_server(int argc' | Set-Content examples/server/server.cpp
            (Get-Content ggml/src/ggml-vulkan.cpp ) -replace 'exit\(1', 'std::terminate(' | Set-Content ggml/src/ggml-vulkan.cpp
            (Get-Content ggml/src/ggml.c ) -replace 'abort\(\)', 'raise(SIGSEGV)' | Set-Content ggml/src/ggml.c
            (Get-Content ggml/include/ggml.h ) -replace 'GGML_NORETURN GGML_ATTRIBUTE_FORMAT', '//GGML_NORETURN GGML_ATTRIBUTE_FORMAT' | Set-Content ggml/include/ggml.h
        
            Get-ChildItem examples/server/public/* | ForEach-Object {
              $f = $_.FullName
              $output = $f -replace "public\\", ""
              echo "cmake -DINPUT=$f -DOUTPUT=$output.hpp -P scripts/xxd.cmake"
              cmake -DINPUT="$f" -DOUTPUT="$output.hpp" -P "scripts/xxd.cmake"
            }
        
            cd ..
            mkdir -p build/licenses/

        - id: set_cuda_var_windows
          if: startsWith(matrix.build, 'cuda')
          name: Set CUDA variable
          run: |
            $pattern = "\d+\.\d+\.\d+"
            $CUDA = [regex]::Match("${{ matrix.build }}", $pattern).Value
            Add-Content $env:GITHUB_ENV "CUDA=$CUDA"

        - id: prepare_cuda
          if: startsWith(matrix.build, 'cuda') || startsWith(matrix.build, 'hip')
          name: Prepare CUDA
          run: |
            cp tinyBLAS/* llama.cpp/ggml/src/
            rm -r llama.cpp/ggml/src/ggml-cuda
            sed -i 's:GGML_USE_CUDA:GGML_USE_CUDA GGML_USE_TINYBLAS NDEBUG ${{ matrix.minimise }}:g' llama.cpp/ggml/src/CMakeLists.txt
            curl -o build/licenses/llamafile.LICENSE.txt -L https://raw.githubusercontent.com/Mozilla-Ocho/llamafile/main/LICENSE

        - id: setup_hip_windows
          name: Install
          run: |
            $ErrorActionPreference = "Stop"
            write-host "Downloading AMD HIP SDK Installer"
            Invoke-WebRequest -Uri "https://download.amd.com/developer/eula/rocm-hub/AMD-Software-PRO-Edition-23.Q4-WinSvr2022-For-HIP.exe" -OutFile "${env:RUNNER_TEMP}\rocm-install.exe"
            write-host "Installing AMD HIP SDK"
            Start-Process "${env:RUNNER_TEMP}\rocm-install.exe" -ArgumentList '-install' -NoNewWindow -Wait
            write-host "Completed AMD HIP SDK installation"
            & 'C:\Program Files\AMD\ROCm\*\bin\clang.exe' --version


        - id: cpu-cores
          name: Get number of CPU cores
          uses: SimenB/github-actions-cpu-cores@v2

        - id: cmake_build_hip_windows
          name: Build
          run: |
            $env:HIP_PATH=$(Resolve-Path 'C:\Program Files\AMD\ROCm\*\bin\clang.exe' | split-path | split-path)
            $env:CMAKE_PREFIX_PATH="${env:HIP_PATH}"
            (Get-Content CMakeLists.txt ) -replace 'LIBRARY undreamai', 'LIBRARY undreamai_windows-hip' | Set-Content CMakeLists.txt
            (Get-Content llama.cpp/ggml/src/CMakeLists.txt ) -replace 'FATAL_ERROR', 'WARNING' | Set-Content llama.cpp/ggml/src/CMakeLists.txt
            cd build
            cmake -G "Unix Makefiles" .. -DCMAKE_C_COMPILER="${env:HIP_PATH}\bin\clang.exe" -DCMAKE_CXX_COMPILER="${env:HIP_PATH}\bin\clang++.exe" -DGGML_HIPBLAS=ON ${{ env.CMAKE_COMMON }} ${{ env.CMAKE_COMMON_JOBS }} ${{ env.CMAKE_COMMON_DIR }}
            cmake --build . --config Release -j ${{ steps.cpu-cores.outputs.count }}


        - id: move_files_windows
          if: matrix.build == 'arm64' || startsWith(matrix.build, 'hip')
          name: Move files to Release folder
          run: |
            mkdir build/Release
            mkdir build/libs/Release
            move build/libs/*exe build/libs/Release
            move build/libs/*dll build/libs/Release
            move build/libs/*exp build/libs/Release
            move build/libs/*lib build/libs/Release

        - id: set_upload_name_windows
          name: Set upload name
          run: |
            $NAME = "undreamai-${{ github.ref_name }}-llamacpp-${{ env.PLATFORM }}-${{ matrix.build }}.zip"
            Add-Content $env:GITHUB_ENV "UPLOAD_NAME=$NAME"
            Add-Content $env:GITHUB_ENV "UPLOAD_PATH=$NAME"

        - id: pack_artifacts_windows
          name: Pack artifacts
          run: |
            ls -R build
            del build/libs/Release/undreamai_test.exe
            mkdir artifacts
            Copy-Item .\build\licenses\* .\artifacts\
            Copy-Item .\build\Release\* .\artifacts\
            Copy-Item .\build\libs\Release\*dll .\artifacts\
            $serverPath = ".\build\libs\Release\undreamai_server.exe"
            if (Test-Path $serverPath) {
                Copy-Item $serverPath -Destination ".\artifacts\"
            }
            cd artifacts
            7z a ../${{ env.UPLOAD_NAME }} *

        - id: upload
          name: Upload Artifacts
          uses: actions/upload-artifact@v4
          with:
            name: ${{ env.UPLOAD_NAME }}
            path: ${{ env.UPLOAD_PATH }}


################################ Android ################################

  android-build:
    runs-on: ubuntu-22.04

    env:
      CMAKE_COMMON: '-DGGML_NATIVE=OFF -DGGML_OPENMP=OFF'
      VULKAN_VERSION: 1.3.261.1
      PLATFORM: android

    steps:
        - id: checkout
          name: Clone
          uses: actions/checkout@v4

        - id: setup_libs_linux
          name: Dependencies
          run: |
            sudo apt-get update
            sudo apt-get install -y build-essential cmake zip libssl-dev

        - id: setup_llama_cpp_linux
          name: Clone llama.cpp
          run: |
            git clone https://github.com/ggerganov/llama.cpp llama.cpp
            cd llama.cpp
            git checkout ${{ env.LLAMACPP_VERSION }}
        
            sed -i.bak 's:utils.hpp:utils_callback.hpp:g' examples/server/server.cpp
            sed -i.bak 's:main(int argc:main_server(int argc:g' examples/server/server.cpp
            sed -i.bak 's:exit(1):std\:\:terminate():g' ggml/src/ggml-vulkan.cpp
            sed -i.bak 's:abort():raise(SIGSEGV):g' ggml/src/ggml.c
            sed -i.bak 's:GGML_NORETURN GGML_ATTRIBUTE_FORMAT://GGML_NORETURN GGML_ATTRIBUTE_FORMAT:g' ggml/include/ggml.h
        
            for f in examples/server/public/*;do
              cmake -DINPUT=$f -DOUTPUT=`echo $f|sed -e 's:public/::g'`.hpp -P scripts/xxd.cmake
            done
        
            cd ..
            mkdir -p build/licenses build/libs
            cp llama.cpp/LICENSE build/licenses/llama.cpp.LICENSE.txt

        - id: set_ndk_var_linux
          name: Set NDK variable
          run: |
            echo "NDK=`ls -d /usr/local/lib/android/sdk/ndk/26.*`" >> $GITHUB_ENV


        - id: cmake_build_android
          name: Build
          run: |
            sed -i "s:LIBRARY undreamai:LIBRARY undreamai_android:g" CMakeLists.txt
            export LD_LIBRARY_PATH=""
            cd build
            cmake .. -DCMAKE_TOOLCHAIN_FILE=$NDK/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a -DANDROID_PLATFORM=android-23 -DCMAKE_C_FLAGS=-march=armv8.4a+dotprod ${{ env.CMAKE_COMMON }} ${{ env.CMAKE_COMMON_JOBS }} ${{ env.CMAKE_COMMON_DIR }}
            cmake --build . --config Release -j $(nproc)


        - id: set_upload_name_unix
          name: Set upload name
          run: |
            NAME=undreamai-${{ github.ref_name }}-llamacpp-${{ env.PLATFORM }}
            if [ "${{ matrix.build }}" != "" ];then NAME=$NAME-${{ matrix.build }}; fi
            NAME=$NAME.zip
            echo "UPLOAD_NAME=$NAME" >> $GITHUB_ENV
            echo "UPLOAD_PATH=$NAME" >> $GITHUB_ENV

        - id: remove_server
          name: Delete server
          run: |
            rm build/libs/undreamai_server

        - id: pack_artifacts_unix
          name: Pack artifacts
          run: |
            rm build/libs/undreamai_test
            zip -j ${{ env.UPLOAD_NAME }} build/licenses/* build/libs/*

        - id: upload
          name: Upload Artifacts
          uses: actions/upload-artifact@v4
          with:
            name: ${{ env.UPLOAD_NAME }}
            path: ${{ env.UPLOAD_PATH }}


################################ Release ################################

  create_release:
    name: Create Release
    runs-on: ubuntu-latest
    needs:
      - archchecker_linux_build
      - archchecker_windows_build
      - linux-build
      - linux-hip
      - macOS-arm64-build
      - macOS-x64-build
      - windows-build
      - windows-hip
      - android-build
    steps:
        - id: checkout
          name: Clone
          uses: actions/checkout@v4

        - id: download_artifacts
          name: Download Artifacts
          uses: actions/download-artifact@v4
          with:
            path: artifacts

        - id: merge_artifacts
          name: Merge artifacts
          run: |
            cd artifacts
            ls -R
            PREFIX=undreamai-${{ github.ref_name }}-llamacpp
        
            for d in `ls | grep $PREFIX`;do
              cd $d; unzip $d; rm $d; cd ..;
              new_name=`echo $d | sed -e "s:$PREFIX-::g" | sed -e 's:.zip::g'`
              mv $d $new_name;
              echo $new_name >> bundle
            done
        
            servers=`find . -name undreamai_server*`
            if [ "$servers" != "" ];then
              zip -r undreamai-${{ github.ref_name }}-server.zip $servers
              rm $servers
            fi
        
            zip -r $PREFIX.zip `cat bundle | grep -v full` linux-archchecker windows-archchecker
            zip -r $PREFIX-full.zip `cat bundle | grep full`

        - id: release
          name: Release
          uses: softprops/action-gh-release@v2
          with:
            files: artifacts/*.zip
            name: Release ${{ github.ref_name }}

