
<p align="center">
<picture>
  <source media="(prefers-color-scheme: dark)" srcset=".github/logo_white.png">
  <source media="(prefers-color-scheme: light)" srcset=".github/logo.png">
  <img src=".github/logo.png" height="150"/>
</picture>
</p>

<h3 align="center">Cross-Platform High-Level LLM Library</h3>

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
<a href="https://discord.gg/RwXKQb6zdv"><img src="https://discordapp.com/api/guilds/1194779009284841552/widget.png?style=shield"/></a>
[![Reddit](https://img.shields.io/badge/Reddit-%23FF4500.svg?style=flat&logo=Reddit&logoColor=white)](https://www.reddit.com/user/UndreamAI)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-blue?style=flat&logo=linkedin&labelColor=blue)](https://www.linkedin.com/company/undreamai)
[![GitHub Repo stars](https://img.shields.io/github/stars/undreamai/LlamaLib?style=flat&logo=github&color=f5f5f5)](https://github.com/undreamai/LlamaLib)
[![Documentation](https://img.shields.io/badge/Docs-white.svg?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwEAYAAAAHkiXEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABl0RVh0U29mdHdhcmUAd3d3Lmlua3NjYXBlLm9yZ5vuPBoAAATqSURBVHic7ZtbiE1RGMc349K4M5EwklwjzUhJCMmTJPJAYjQXJJcH8+Blkry4lPJA8aAoJbekDLmUS6E8SHJL5AW5JPf77eHv93C22Wfttc/ee+0zc/4vv+bMXvusvfZa3/q+b33H80oqqaSSSmqrKnPdgXjUvbvYq5f4+7f486eb/rRajRsn7t4tPngg/vol/vkj/vghXr0q7tghzpyZ//79+on79omXLombNondukXrd9GoSxdx8mSxqUm8eVNkgAvl0aPioEFip07i6dP52z15Ig4fbvVY2VVFhbhokXjrlogJiWvAg/jwoXjqVO73+leUny9eiFVV5mfMlLDRBw+KX76ISQ+0LZ8/F00v4uJFsWPHFh83O+rdWzx3TnQ9wCZ+/Sqyl5iux1RmTu3aiYcPi64H1pasALypoOv4/8SJXraEbXc9kLbECxo2TKyuFj9/zt9u+XIvG8LWv3wpuh5QW86f3/JznT+fv93s2S23C1Z72wbhtH692LdvMvdPSgzkhAkiJhT16ZO/PRPOmcr+Rda4aa5nclTeuZP7PDgRpr1g40bPrQYOFF0PYKHEC+raVVy8OFy7R49EArvURU4mrUAqaTY0iB8/2rXD+XCm5mbR9QAWylevorV7/VpkL0ld06eLpkiyWPj9u93179+LpFZwZ1PXtGnitWui64GMStPmG7SH1NSIJBNHjvTSFZvRvHlise0N9JcBtW1/44Y4dqx45IjnU0JxAGLpklPx+9VZFwPp/9v/eZDGjxcZh7dv4+mXtch+up7Rca+MsJvxiRNi6nvBhg25HWprZMaPGeOlqxEjxGKz+XGRTAAmyJnq6sR370TXA2NLW+8HNjZ62dLOnaLrAQ1r2zmqPH482n0mTfJCKmEvCJHUooNZE/369Elct06kqiKsONRfulTEFDsX8QDlIa5nup9374pE8IiZHPY+ly+LZE/37/cM6mC6IB6Vl4urV6fzfUG6d0/csyf37wsXRFInaM4ckTjGdPg+apTYs6dI3RIWwH//1DV1qkiuxNY2FzrTd+2y6y8z2HQU6efZs+KBAyJZ4v+V0h6ArlwROaQP0uPH4ooV4sqV8Xz/4MF211M2wwoOq1mzRAq5Pnywa5+4KDHE9mI7ly0TO3fOvZ6/eZCoKwB32HS0SMFV1DNtImBKHYstBROoQ4fEQk2RaS+qrxejmj5M7NatIhWARS82xUJfAKahzFcdPnq0GLYgy7Rnbd8e6rGKRyzpuNzPBQty709RcNSZf/KkuHCh2GpMDyKbGNcLYE+YMkVks336NFx7XhTZ3szXiBaqtWvFuAOxM2dEZiyH8UErgc8JLNun7E0aFffSI7RP6owZmz9kSO73HjsmXr8ukppYsybSYyQvBp5QfOjQ3M9tRR496pGgLf1JtLlzRZJzlFzGp4SWDnUxFCrdvy+uWiWa3DJe3N69oj8uSEq8CER88uaNOGBAOv2ILGY69TBBJoM8O0t72zaRoztXBzlLlrT8XARW/IQq82JTMv3mKmv0/9CC4mJMYPwrMSETxAyurRUxQVmXP1fEid7mzeK3b+n2Jzb16CFu2SIWmtNJiriVxANsyq0uoCJfTk4G9y4t24/bSQ0rTkP6gVTG3mz//uKMGSK/ucId5Xe9lZUi5eMMLGUgz56J5Hxu3xZ50Xg3RMIltVn9BRja26PYsBHgAAAAAElFTkSuQmCC)](https://undream.ai/LLMUnity)

LlamaLib is a **high-level C++ and C#** library for running Large Language Models (LLMs) **anywhere** - from PCs to mobile devices and VR headsets.

---

## At a glance


- âœ… **High-Level API**  
C++ and C# implementations with intuitive object-oriented design 

- ðŸ“¦ **Self-Contained and Embedded**  
Runs embedded within your application.  
No need for a separate server or external processes. Zero external dependencies.

- ðŸŒ **Runs Anywhere**  
  Cross-platform and cross-device.  
  Works on all major platforms:
    - Desktop: `Windows`, `macOS`, `Linux`
    - Mobile: `Android`, `iOS`
    - VR/AR: `Meta Quest`, `Apple Vision`, `Magic Leap`

  and hardware architectures:
    - CPU: Intel, AMD, Apple Silicon
    - GPU: NVIDIA, AMD, Metal

- ðŸ” **Architecture Detection at runtime**  
  Automatically selects the optimal backend at runtime supporting all major GPU and CPU architectures.

- ðŸ¤ **Tiny footprint**  
  Integration requires only 10-200 MB depending on the embedded architectures.  
  Custom implementation of tinyBLAS reduces CUDA integration from 1.3GB to 130MB (cuBLAS also supported).

- ðŸ›  **Production ready**  
  Designed for easy integration into C++ and C# applications.  
  Supports both local and client-server deployment.

### Why choose LlamaLib?
  - **Developer experience**:  
  Direct implementation of LLM operations (completion, tokenisation, embeddings).  
  Clean implementation of LLM service and clients, server-client architecture and LLM agents.

  - **Universal deployment**:  
  LlamaLib is the only library that lets you build your application for any hardware.  
  Unlike alternatives that allow you to only build for specific GPU vendor or CPU-only execution, our architecture detection happens at runtime.   
  If your application is developed for GPU: the GPU backend of the user hardware will be automatically selected (Nvidia, AMD, Metal) or fallback to CPU.  
  CPU detection will automatically identify the CPU hardware (CPU instruction set) of the user to select the optimal backend.  
  LlamaLib works for all platforms from PC to mobile and VR.  
  - ðŸ›  **Production ready**  
  Embeds directly in your application without opening ports or starting external servers.  
  LlamaLib has minimal disk space requirements allowing compact buids e.g. for mobile deployment.


## How to help
- [â­ Star](https://github.com/undreamai/LlamaLib) the repo and spread the word!
- [![](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/amakropoulos) developent!
- Join our [Discord](https://discord.gg/RwXKQb6zdv) community.
- [Contribute](CONTRIBUTING.md) with feature requests, bug reports, or pull requests.

## Projects using LlamaLib
- [LLM for Unity](https://github.com/undreamai/LLMUnity): The most widely used solution to integate LLMs in games

## Quick Start
For complete tutorials, see the [C++ guide](README_C.md) and [C# guide](README_csharp.md).

### C++

```cpp
#include "LlamaLib.h"

// LlamaLib automatically detects your hardware and selects optimal backend
LLMService llm = new LLMService("path/to/model.gguf");
/* You can also specify:
  threads=-1,        // number of CPU threads to use
  gpu_layers=0,      // number of layers to offload to GPU (if 0, GPU is not used),
  num_parallel=1     // number of slots / clients supported in parallel
*/

// Start service
llm->start();

// Generate completion
std::string response = llm->completion("Hello, how are you?");
std::cout << response << std::endl;

// Clean-up (be nice)
delete llm;
```

### C#

```csharp
using LlamaLib;

// Same API, different language
LLMService llm = new LLMService("path/to/model.gguf");
/* You can also specify:
  threads=-1,        // number of CPU threads to use
  gpu_layers=0,      // number of layers to offload to GPU (if 0, GPU is not used),
  num_parallel=1     // number of slots / clients supported in parallel
*/
llm.Start();

string response = llm.Completion("Hello, how are you?");
Console.WriteLine(response);
```


## Architecture

LlamaLib implements the following classes:
- **LLMService**: Implementation of LLM service. For desktop environments implements runtime detection for multiple GPU and CPU architectures
- **LLMClient**: Unified client for local providers or remote servers
- **LLMAgent**: High-level conversational AI with persistent chat history

### Core Methods

- `completion(prompt)`: Generate text completion (streaming supported)
- `embeddings(text)`: Generate text embeddings
- `tokenize(text)`: Convert text to tokens
- `detokenize(tokens)`: Convert tokens to text
- `apply_template(messages)`: Apply chat template

### Agent Methods

- `chat(message)`: Send message and get response (streaming supported)
- `add_message(role, content)`: Add message to history
- `clear_history()`: Clear conversation history
- `save_history(path)`: Save history to file
- `load_history(path)`: Load history from file

### Provider Methods

- `start()`: Start service
- `stop()`: Stop service
- `set_template(template)`: Use a specific chat template (otherwise determined by the model)
- `lora_weight(loras)`: Use LORAs with custom weightings

### Server Methods

- `start_server(host, port)`: Start HTTP server
- `stop_server()`: Stop HTTP server
- `set_SSL(cert, key)`: Configure SSL/TLS
