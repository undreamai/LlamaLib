diff --git a/ggml/src/ggml-cuda/CMakeLists.txt b/ggml/src/ggml-cuda/CMakeLists.txt
index bce07ac3..20dd47f1 100644
--- a/ggml/src/ggml-cuda/CMakeLists.txt
+++ b/ggml/src/ggml-cuda/CMakeLists.txt
@@ -46,9 +46,16 @@ if (CUDAToolkit_FOUND)
     list(APPEND GGML_HEADERS_CUDA "../../include/ggml-cuda.h")
 
     file(GLOB   GGML_SOURCES_CUDA "*.cu")
-    file(GLOB   SRCS "template-instances/fattn-mma*.cu")
+    file(GLOB   SRCS "template-instances/mmq-instance-q*.cu")
     list(APPEND GGML_SOURCES_CUDA ${SRCS})
-    file(GLOB   SRCS "template-instances/mmq*.cu")
+
+    if (NOT GGML_CUDA_NO_IQUANTS)
+    file(GLOB   SRCS "template-instances/mmq-instance-i*.cu")
+    list(APPEND GGML_SOURCES_CUDA ${SRCS})
+    endif()
+
+    if (GGML_CUDA_FA)
+    file(GLOB   SRCS "template-instances/fattn-mma*.cu")
     list(APPEND GGML_SOURCES_CUDA ${SRCS})
 
     if (GGML_CUDA_FA_ALL_QUANTS)
@@ -63,6 +70,7 @@ if (CUDAToolkit_FOUND)
         file(GLOB   SRCS "template-instances/fattn-vec*f16-f16.cu")
         list(APPEND GGML_SOURCES_CUDA ${SRCS})
     endif()
+    endif()
 
     ggml_add_backend_library(ggml-cuda
                              ${GGML_HEADERS_CUDA}
@@ -91,6 +99,10 @@ if (CUDAToolkit_FOUND)
         add_compile_definitions(GGML_CUDA_NO_FA)
     endif()
 
+    if (GGML_CUDA_NO_IQUANTS)
+        add_compile_definitions(GGML_CUDA_NO_IQUANTS)
+    endif()
+
     if (GGML_CUDA_F16 OR GGML_CUDA_DMMV_F16)
         add_compile_definitions(GGML_CUDA_F16)
     endif()
diff --git a/ggml/src/ggml-cuda/common.cuh b/ggml/src/ggml-cuda/common.cuh
index a23da57e..b215e037 100644
--- a/ggml/src/ggml-cuda/common.cuh
+++ b/ggml/src/ggml-cuda/common.cuh
@@ -145,7 +145,7 @@ void ggml_cuda_error(const char * stmt, const char * func, const char * file, in
 
 #define CUDA_CHECK(err) CUDA_CHECK_GEN(err, cudaSuccess, cudaGetErrorString)
 
-#if CUDART_VERSION >= 12000 || defined(GGML_USE_MUSA)
+#if CUDART_VERSION >= 12000 || defined(GGML_USE_MUSA) || defined(GGML_USE_TINYBLAS)
     static const char * cublas_get_error_str(const cublasStatus_t err) {
         return cublasGetStatusString(err);
     }
@@ -244,7 +244,7 @@ typedef float2 dfloat2;
 #define CP_ASYNC_AVAILABLE
 #endif // !defined(GGML_USE_HIP) && __CUDA_ARCH__ >= GGML_CUDA_CC_AMPERE
 
-#if !defined(GGML_CUDA_NO_FA) && !(defined(GGML_USE_MUSA) && __MUSA_ARCH__ < 220)
+#if !defined(GGML_CUDA_NO_FA) && !(defined(GGML_USE_MUSA) && __MUSA_ARCH__ < 220) && !defined(GGML_CUDA_NO_FA)
 #define FLASH_ATTN_AVAILABLE
 #endif // !defined(GGML_CUDA_NO_FA) && !(defined(GGML_USE_MUSA) && __MUSA_ARCH__ < 220)
 
diff --git a/ggml/src/ggml-cuda/ggml-cuda.cu b/ggml/src/ggml-cuda/ggml-cuda.cu
index d9110491..49055346 100644
--- a/ggml/src/ggml-cuda/ggml-cuda.cu
+++ b/ggml/src/ggml-cuda/ggml-cuda.cu
@@ -1237,7 +1237,9 @@ static void ggml_cuda_op_mul_mat_cublas(
         (GGML_CUDA_CC_IS_MTHREADS(cc) && cc >= GGML_CUDA_CC_QY2);
 
     const bool use_fp16 = (src0->type == GGML_TYPE_F16 || ggml_is_quantized(src0->type)) && ggml_is_contiguous(src0) && row_diff == src0->ne[1] && dst->op_params[0] == GGML_PREC_DEFAULT;
-
+#ifdef GGML_USE_TINYBLAS
+    if (0) {
+#else
     if (supports_bf16 && src0->type == GGML_TYPE_BF16 && ggml_is_contiguous(src0) && row_diff == src0->ne[1]) {
         ggml_cuda_pool_alloc<nv_bfloat16> src1_as_bf16(ctx.pool(id));
         if (src1->type != GGML_TYPE_BF16) {
@@ -1266,6 +1268,7 @@ static void ggml_cuda_op_mul_mat_cublas(
 
         const to_fp32_cuda_t to_fp32_cuda = ggml_get_to_fp32_cuda(GGML_TYPE_BF16);
         to_fp32_cuda(dst_bf16.get(), dst_dd_i, row_diff*src1_ncols, stream);
+#endif //GGML_USE_TINYBLAS
     } else if (fast_fp16_hardware_available(cc) && use_fp16) {
         // convert src0 and src1 to fp16, multiply as fp16, convert dst to fp32
         ggml_cuda_pool_alloc<half> src0_as_f16(ctx.pool(id));
@@ -2482,9 +2485,11 @@ static bool ggml_cuda_compute_forward(ggml_backend_cuda_context & ctx, struct gg
         case GGML_OP_ARGSORT:
             ggml_cuda_op_argsort(ctx, dst);
             break;
+#ifndef GGML_CUDA_NO_FA
         case GGML_OP_FLASH_ATTN_EXT:
             ggml_cuda_flash_attn_ext(ctx, dst);
             break;
+#endif
         case GGML_OP_CROSS_ENTROPY_LOSS:
             ggml_cuda_cross_entropy_loss(ctx, dst);
             break;
diff --git a/ggml/src/ggml-cuda/mmq.cu b/ggml/src/ggml-cuda/mmq.cu
index 384ee761..2e2ab603 100644
--- a/ggml/src/ggml-cuda/mmq.cu
+++ b/ggml/src/ggml-cuda/mmq.cu
@@ -38,6 +38,7 @@ static void ggml_cuda_mul_mat_q_switch_type(ggml_backend_cuda_context & ctx, con
         case GGML_TYPE_Q6_K:
             mul_mat_q_case<GGML_TYPE_Q6_K>(ctx, args, stream);
             break;
+#ifndef GGML_CUDA_NO_IQUANTS
         case GGML_TYPE_IQ2_XXS:
             mul_mat_q_case<GGML_TYPE_IQ2_XXS>(ctx, args, stream);
             break;
@@ -62,6 +63,7 @@ static void ggml_cuda_mul_mat_q_switch_type(ggml_backend_cuda_context & ctx, con
         case GGML_TYPE_IQ4_NL:
             mul_mat_q_case<GGML_TYPE_IQ4_NL>(ctx, args, stream);
             break;
+#endif // GGML_CUDA_NO_IQUANTS
         default:
             GGML_ABORT("fatal error");
             break;
diff --git a/ggml/src/ggml-cuda/mmq.cuh b/ggml/src/ggml-cuda/mmq.cuh
index 96129bd8..0e9fa269 100644
--- a/ggml/src/ggml-cuda/mmq.cuh
+++ b/ggml/src/ggml-cuda/mmq.cuh
@@ -3725,6 +3725,7 @@ extern DECL_MMQ_CASE(GGML_TYPE_Q3_K);
 extern DECL_MMQ_CASE(GGML_TYPE_Q4_K);
 extern DECL_MMQ_CASE(GGML_TYPE_Q5_K);
 extern DECL_MMQ_CASE(GGML_TYPE_Q6_K);
+#ifndef GGML_CUDA_NO_IQUANTS
 extern DECL_MMQ_CASE(GGML_TYPE_IQ2_XXS);
 extern DECL_MMQ_CASE(GGML_TYPE_IQ2_XS);
 extern DECL_MMQ_CASE(GGML_TYPE_IQ2_S);
@@ -3733,6 +3734,7 @@ extern DECL_MMQ_CASE(GGML_TYPE_IQ3_S);
 extern DECL_MMQ_CASE(GGML_TYPE_IQ1_S);
 extern DECL_MMQ_CASE(GGML_TYPE_IQ4_NL);
 extern DECL_MMQ_CASE(GGML_TYPE_IQ4_XS);
+#endif // GGML_CUDA_NO_IQUANTS
 
 // -------------------------------------------------------------------------------------------------------------------------
 
diff --git a/ggml/src/ggml-cuda/mmvq.cu b/ggml/src/ggml-cuda/mmvq.cu
index 5c8e5c4a..afcaaa22 100644
--- a/ggml/src/ggml-cuda/mmvq.cu
+++ b/ggml/src/ggml-cuda/mmvq.cu
@@ -428,6 +428,7 @@ static void mul_mat_vec_q_switch_type(
                  nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,
                  stream);
             break;
+#ifndef GGML_CUDA_NO_IQUANTS
         case GGML_TYPE_IQ2_XXS:
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ2_XXS>
                 (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
@@ -491,6 +492,7 @@ static void mul_mat_vec_q_switch_type(
                  nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,
                  stream);
             break;
+#endif // GGML_CUDA_NO_IQUANTS
         default:
             GGML_ABORT("fatal error");
             break;
diff --git a/ggml/src/ggml-cuda/tinyblas.cu b/ggml/src/ggml-cuda/tinyblas.cu
new file mode 100644
index 00000000..1d4da084
--- /dev/null
+++ b/ggml/src/ggml-cuda/tinyblas.cu
@@ -0,0 +1,981 @@
+// -*- mode:c++;indent-tabs-mode:nil;c-basic-offset:4;coding:utf-8 -*-
+// vi: set et ft=cpp ts=4 sts=4 sw=4 fenc=utf-8 :vi
+//
+// Copyright 2024 Mozilla Foundation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#include "tinyblas.h"
+
+//
+//
+//                                ██████╗ ██╗   █████╗ ██████╗
+//         ██████╗██╗██╗ ██╗██═██╗██╔══██╗██║  ██╔══██╗██╔═══╝
+//         ╚═██╔═╝██║███▄██║██ ██║██████╔╝██║  ███████║██████╗
+//           ██║  ██║██▀███║╚███╔╝██╔══██╗██║  ██╔══██║╔═══██║
+//           ██║  ██║██║ ██║ ███║ ██████╔╝████╗██║  ██║██████║
+//           ╚═╝  ╚═╝╚═╝ ╚═╝ ╚══╝ ╚═════╝ ╚═══╝╚═╝  ╚═╝╚═════╝
+//
+//                   BASIC LINEAR ALGEBRA SUBPROGRAMS
+//
+//
+// In this file you'll find GPU subroutines implementing general matrix
+// multiplication, that are API compatible with NVIDIA's cuBLAS library
+// and implement similar tricks[1] for performance.
+//
+// [1] S. Boehm, ‘How to Optimize a CUDA Matmul Kernel for cuBLAS-like
+//     Performance’, 2022. [Online]. Available:
+//     https://siboehm.com/articles/22/CUDA-MMM. [Accessed:
+//     05-Mar-2024].
+
+#include <algorithm>
+#include <cstdlib>
+#include <type_traits>
+
+#ifndef __HIP__
+#include <cuda_fp16.h>
+#include <cuda_runtime.h>
+#define __shfl_down(var, srcLane, warpSize) __shfl_down_sync(-1u, var, srcLane, warpSize)
+#else
+#include <hip/hip_fp16.h>
+#include <hip/hip_runtime.h>
+#define cudaSuccess hipSuccess
+#define cudaStream_t hipStream_t
+#define cudaGetLastError hipGetLastError
+#endif
+
+#define WARPSIZE 32
+#define THREAD_COUNT ((BM * BN) / (TM * TN))
+#define KERNEL __launch_bounds__(THREAD_COUNT)
+#define CEIL_DIV(M, N) (((M) + (N) - 1) / (N))
+
+#define IGNORE_BETA 1
+#define IGNORE_ALPHA 2
+#define ASSUME_A_OP_N 4
+#define ASSUME_B_OP_T 8
+#define ASSUME_M_SAFE 16
+#define ASSUME_N_SAFE 32
+#define ASSUME_K_SAFE 64
+#define ASSUME_A_OP_T 128
+#define ASSUME_B_OP_N 256
+
+struct tinyblasContext {
+    cudaStream_t stream;
+};
+
+inline bool isone(float x) {
+    return x == 1;
+}
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+// tinyBLAS specialized matrix vector product kernel
+
+__forceinline__ __device__ float warpSum(float x) {
+    for (int i = WARPSIZE / 2; i > 0; i /= 2)
+        x += __shfl_down(x, i, WARPSIZE);
+    return x;
+}
+
+template <typename WORD, typename SRC>
+__device__ __forceinline__ void madd(WORD *tally, WORD *kahan, SRC a, SRC b) {
+    WORD x = a;
+    WORD y = b;
+    WORD z = x * y - *kahan;
+    WORD t = *tally + z;
+    *kahan = (t - *tally) - z;
+    *tally = t;
+}
+
+template <typename WORD, typename SRC, typename DST>
+static __device__ void matvec(int m, int k, const SRC *A, int lda, const SRC *B, DST *C) {
+    WORD Ct[WARPSIZE] = {0};
+    WORD Ce[WARPSIZE] = {0};
+    int i = blockIdx.y * WARPSIZE;
+    for (int l = threadIdx.x; l < k; l += WARPSIZE)
+        for (int j = 0; j < WARPSIZE; ++j)
+            madd(&Ct[j], &Ce[j], A[lda * (i + j) + l], B[l]);
+    for (int j = 0; j < WARPSIZE; ++j) {
+        WORD c = warpSum(Ct[j]);
+        if (!threadIdx.x)
+            C[i + j] = c;
+    }
+}
+
+template <typename WORD, typename SRC, typename DST>
+static __global__ __launch_bounds__(WARPSIZE) void matvec_entry(int m, int k, const SRC *A, int lda,
+                                                                const SRC *B, DST *C) {
+    matvec<WORD>(m, k, A, lda, B, C);
+}
+
+template <typename WORD, typename SRC, typename DST>
+static tinyblasStatus_t matvec_launch(tinyblasHandle_t handle, int m, int k, const SRC *A, int lda,
+                                      const SRC *B, DST *C) {
+    dim3 blocks(WARPSIZE, m / WARPSIZE);
+    matvec_entry<WORD><<<blocks, WARPSIZE, 0, handle->stream>>>(m, k, A, lda, B, C);
+    if (cudaGetLastError() != cudaSuccess)
+        return TINYBLAS_STATUS_EXECUTION_FAILED;
+    return TINYBLAS_STATUS_SUCCESS;
+}
+
+template <typename WORD>
+static bool can_use_matvec(tinyblasOperation_t aT, tinyblasOperation_t bT, int m, int n, int k,
+                           WORD alpha, WORD beta) {
+    return n == 1 && k >= 4096 && aT && !bT && //
+           !(m % WARPSIZE) && !(k % WARPSIZE) && //
+           isone(alpha) && !beta;
+}
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+// tinyBLAS block tiling outer product GEMM kernel
+
+template <int CONFIG, int BM, int BN, int TM, int TN, typename WORD, typename SRC, typename DST>
+static __device__ void matmul_block2d(tinyblasOperation_t transa, tinyblasOperation_t transb, int m,
+                                      int n, int k, WORD alpha, const SRC *A, int lda, const SRC *B,
+                                      int ldb, WORD beta, DST *C, int ldc) {
+
+    constexpr int BK = THREAD_COUNT;
+    static_assert(BM % TM == 0, "can't divide work for threads");
+    static_assert(BN % TN == 0, "can't divide work for threads");
+    static_assert(BM > 0 && BN > 0 && BK > 0 && TM > 0 && TN > 0,
+                  "one of the constexpr configuration values was non-positive");
+    static_assert((BK * BM * sizeof(SRC)) + (BK * BN * sizeof(SRC)) <= 65536,
+                  "you're almost almost certainly using too much shared memory");
+
+    constexpr bool msafe = !!(CONFIG & ASSUME_M_SAFE);
+    constexpr bool nsafe = !!(CONFIG & ASSUME_N_SAFE);
+    constexpr bool ksafe = !!(CONFIG & ASSUME_K_SAFE);
+
+    const int th = threadIdx.x;
+    const int ii = blockIdx.x * BM;
+    const int jj = blockIdx.y * BN;
+    const int ti = th / (BN / TN) * TM;
+    const int tj = th % (BN / TN) * TN;
+
+    __shared__ SRC As[BK * BM];
+    __shared__ SRC Bs[BK * BN];
+
+    WORD At[TM];
+    WORD Bt[TN];
+    WORD Ct[TM * TN] = {0};
+
+    if (CONFIG & ASSUME_A_OP_T)
+        transa = TINYBLAS_OP_T;
+    if (CONFIG & ASSUME_A_OP_N)
+        transa = TINYBLAS_OP_N;
+    if (CONFIG & ASSUME_B_OP_N)
+        transb = TINYBLAS_OP_N;
+    if (CONFIG & ASSUME_B_OP_T)
+        transb = TINYBLAS_OP_T;
+
+    for (int ll = 0; ll < k; ll += BK) {
+
+        if (!ksafe || !msafe)
+            for (int i = 0; i < BM; ++i)
+                As[BM * th + i] = 0;
+        for (int i = 0; i < BM && (ll + th < k || ksafe) && (ii + i < m || msafe); ++i)
+            As[BM * th + i] = A[transa ? lda * (ii + i) + (ll + th) : lda * (ll + th) + (ii + i)];
+
+        if (!ksafe || !nsafe)
+            for (int j = 0; j < BN; ++j)
+                Bs[BN * th + j] = 0;
+        for (int j = 0; j < BN && (ll + th < k || ksafe) && (jj + j < n || nsafe); ++j)
+            Bs[BN * th + j] = B[transb ? ldb * (ll + th) + (jj + j) : ldb * (jj + j) + (ll + th)];
+
+        __syncthreads();
+
+        for (int l = 0; l < BK; ++l) {
+            for (int j = 0; j < TM; ++j)
+                At[j] = As[BM * l + ti + j];
+            for (int h = 0; h < TN; ++h)
+                Bt[h] = Bs[BN * l + tj + h];
+            for (int j = 0; j < TM; ++j)
+                for (int h = 0; h < TN; ++h)
+                    Ct[TN * j + h] += At[j] * Bt[h];
+        }
+
+        __syncthreads();
+    }
+
+    for (int j = 0; j < TN && (jj + tj + j < n || nsafe); ++j)
+        for (int i = 0; i < TM && (ii + ti + i < m || msafe); ++i) {
+            WORD r, d = Ct[TN * i + j];
+            if ((CONFIG & IGNORE_BETA) || !beta) {
+                if (CONFIG & IGNORE_ALPHA)
+                    r = d;
+                else
+                    r = alpha * d;
+            } else {
+                WORD c = C[ldc * (jj + tj + j) + (ii + ti + i)];
+                if (CONFIG & IGNORE_ALPHA)
+                    r = beta * c + d;
+                else
+                    r = alpha * d + beta * c;
+            }
+            C[ldc * (jj + tj + j) + (ii + ti + i)] = r;
+        }
+}
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+// tinyBLAS warp block tiling outer product GEMM kernel
+
+template <int CONFIG, int BM, int BN, int BK, int VE, int WM, int WN, int WNI, int TM, int TN,
+          int TT, typename WORD, typename SRC, typename DST>
+static __device__ void matmul_warp2d(tinyblasOperation_t aT, //
+                                     tinyblasOperation_t bT, //
+                                     int m, int n, int k, WORD alpha, //
+                                     const SRC *A, int lda, //
+                                     const SRC *B, int ldb, WORD beta, //
+                                     DST *C, int ldc) {
+
+    const SRC zero = 0;
+    const int warpIdx = threadIdx.x / WARPSIZE;
+    const int warpCol = warpIdx % (BN / WN);
+    const int warpRow = warpIdx / (BN / WN);
+
+    constexpr int WARPS = TT / WARPSIZE;
+    constexpr int WMI = (WM * WN) / (WARPSIZE * TM * TN * WNI);
+    constexpr int WSUBM = WM / WMI;
+    constexpr int WSUBN = WN / WNI;
+
+    constexpr bool msafe = !!(CONFIG & ASSUME_M_SAFE);
+    constexpr bool nsafe = !!(CONFIG & ASSUME_N_SAFE);
+    constexpr bool ksafe = !!(CONFIG & ASSUME_K_SAFE);
+
+    const int threadIdxInWarp = threadIdx.x % WARPSIZE;
+    const int threadColInWarp = threadIdxInWarp % (WSUBN / TN);
+    const int threadRowInWarp = threadIdxInWarp / (WSUBN / TN);
+
+    // want to tune these magic numbers?
+    // use llamafile/pick_a_warp_kernel.c
+    static_assert(!(BN % WN) && !(BM % WM), "");
+    static_assert(!(WM % WMI) && !(WN % WNI), "");
+    static_assert((BN / WN) * (BM / WM) == WARPS, "");
+    static_assert((WM * WN) % (WARPSIZE * TM * TN * WNI) == 0, "");
+    static_assert((BM * BK) % (VE * TT) == 0, "");
+    static_assert((BN * BK) % (VE * TT) == 0, "");
+    static_assert(BK % VE == 0, "");
+    static_assert(BN % VE == 0, "");
+
+    __shared__ SRC As[BK * BM];
+    __shared__ SRC Bs[BK * BN];
+
+    WORD Ar[WMI * TM] = {0};
+    WORD Br[WNI * TN] = {0};
+    WORD Ct[WMI * TM * WNI * TN] = {0};
+
+    if (CONFIG & ASSUME_A_OP_T)
+        aT = TINYBLAS_OP_T;
+    if (CONFIG & ASSUME_A_OP_N)
+        aT = TINYBLAS_OP_N;
+    if (CONFIG & ASSUME_B_OP_N)
+        bT = TINYBLAS_OP_N;
+    if (CONFIG & ASSUME_B_OP_T)
+        bT = TINYBLAS_OP_T;
+
+    for (int ll = 0; ll < k; ll += BK) {
+
+        for (int h = 0; h < BM; h += (TT * VE) / BK)
+            for (int v = 0; v < VE; ++v) {
+                int l = ll + threadIdx.x % (BK / VE) * VE + v;
+                int i = blockIdx.y * BM + threadIdx.x / (BK / VE) + h;
+                As[BM * (threadIdx.x % (BK / VE) * VE + v) + (threadIdx.x / (BK / VE) + h)] =
+                    (((i < m || msafe) && //
+                      (l < k || ksafe))
+                         ? A[aT ? lda * l + i : lda * i + l]
+                         : zero);
+            }
+
+        for (int h = 0; h < BK; h += TT / (BN / VE))
+            for (int v = 0; v < VE; ++v) {
+                int l = ll + threadIdx.x / (BN / VE) + h;
+                int j = blockIdx.x * BN + threadIdx.x % (BN / VE) * VE + v;
+                Bs[BN * (threadIdx.x / (BN / VE) + h) + (threadIdx.x % (BN / VE) * VE + v)] =
+                    (((j < n || nsafe) && //
+                      (l < k || ksafe))
+                         ? B[bT ? ldb * j + l : ldb * l + j]
+                         : zero);
+            }
+
+        __syncthreads();
+
+        for (int l = 0; l < BK; ++l) {
+            for (int ii = 0; ii < WMI; ++ii)
+                for (int i = 0; i < TM; ++i)
+                    Ar[TM * ii + i] =
+                        As[BM * l + WM * warpRow + WSUBM * ii + TM * threadRowInWarp + i];
+            for (int jj = 0; jj < WNI; ++jj)
+                for (int j = 0; j < TN; ++j)
+                    Br[TN * jj + j] =
+                        Bs[BN * l + WN * warpCol + WSUBN * jj + TN * threadColInWarp + j];
+            for (int ii = 0; ii < WMI; ++ii)
+                for (int jj = 0; jj < WNI; ++jj)
+                    for (int i = 0; i < TM; ++i)
+                        for (int j = 0; j < TN; ++j)
+                            Ct[(WNI * TN) * (TM * ii + i) + (TN * jj) + j] +=
+                                Ar[TM * ii + i] * Br[TN * jj + j];
+        }
+
+        __syncthreads();
+    }
+
+    for (int ii = 0; ii < WMI; ++ii)
+        for (int jj = 0; jj < WNI; ++jj)
+            for (int i = 0; i < TM; i += 1)
+                for (int j = 0; j < TN; j += 1) {
+                    int row = (BM * blockIdx.y + WM * warpRow) + (WSUBM * ii) +
+                              (threadRowInWarp * TM + i);
+                    int col = (BN * blockIdx.x + WN * warpCol) + (WSUBN * jj) +
+                              (threadColInWarp * TN + j);
+                    if ((row < m || msafe) && (col < n || nsafe)) {
+                        WORD r, d = Ct[(WNI * TN) * (TM * ii + i) + (TN * jj + j)];
+                        if ((CONFIG & IGNORE_BETA) || !beta) {
+                            if (CONFIG & IGNORE_ALPHA)
+                                r = d;
+                            else
+                                r = alpha * d;
+                        } else {
+                            WORD c = C[ldc * row + col];
+                            if (CONFIG & IGNORE_ALPHA)
+                                r = beta * c + d;
+                            else
+                                r = alpha * d + beta * c;
+                        }
+                        C[ldc * row + col] = r;
+                    }
+                }
+}
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+// tinyBLAS canonical cuBLAS-like interface
+
+/**
+ * Creates new tinyBLAS handle.
+ *
+ * Before calling tinyBLAS GEMM functions a handle must first be
+ * created, using this function. It should be freed later, using
+ * tinyblasDestroy(). After a handle is created the caller needs
+ * tinyblasSetStream() to specify the CUDA stream.
+ *
+ * @param out_handle receives pointer to newly created handle
+ * @return TINYBLAS_STATUS_SUCCESS on success otherwise error
+ */
+tinyblasStatus_t tinyblasCreate(tinyblasHandle_t *out_handle) {
+    tinyblasHandle_t handle;
+    if ((handle = (tinyblasHandle_t)malloc(sizeof(struct tinyblasContext)))) {
+        *out_handle = handle;
+        return TINYBLAS_STATUS_SUCCESS;
+    } else {
+        return TINYBLAS_STATUS_ALLOC_FAILED;
+    }
+}
+
+/**
+ * Destroys tinyBLAS handle.
+ *
+ * @param handle is pointer to handle created by tinyblasCreate()
+ * @return TINYBLAS_STATUS_SUCCESS on success otherwise error
+ */
+tinyblasStatus_t tinyblasDestroy(tinyblasHandle_t handle) {
+    free(handle);
+    return TINYBLAS_STATUS_SUCCESS;
+}
+
+/**
+ * Associates CUDA handle with tinyBLAS handle.
+ *
+ * The provided stream will be used when tinyBLAS launches kernels.
+ *
+ * @param handle is pointer to handle created by tinyblasCreate()
+ * @param stream is pointer to stream created by cudaStreamCreate()
+ * @return TINYBLAS_STATUS_SUCCESS on success otherwise error
+ */
+tinyblasStatus_t tinyblasSetStream(tinyblasHandle_t handle, void *stream) {
+    handle->stream = (cudaStream_t)stream;
+    return TINYBLAS_STATUS_SUCCESS;
+}
+
+/**
+ * Gets CUDA stream associated with tinyBLAS handle.
+ *
+ * @param handle is pointer to handle created by tinyblasCreate()
+ * @param out_stream receives pointer to any cudaStream_t object
+ * @return TINYBLAS_STATUS_SUCCESS on success otherwise error
+ */
+tinyblasStatus_t tinyblasGetStream(tinyblasHandle_t handle, void **out_stream) {
+    *out_stream = handle->stream;
+    return TINYBLAS_STATUS_SUCCESS;
+}
+
+/**
+ * Returns string describing tinyBLAS status code.
+ */
+const char *tinyblasGetStatusString(tinyblasStatus_t err) {
+    switch (err) {
+    case TINYBLAS_STATUS_SUCCESS:
+        return "Success";
+    case TINYBLAS_STATUS_ALLOC_FAILED:
+        return "Alloc failed";
+    case TINYBLAS_STATUS_INVALID_VALUE:
+        return "Invalid value";
+    case TINYBLAS_STATUS_NOT_SUPPORTED:
+        return "Not supported";
+    case TINYBLAS_STATUS_EXECUTION_FAILED:
+        return "Execution failed";
+    case TINYBLAS_STATUS_DIMENSION_OVERLAP:
+        return "Dimension overlap";
+    case TINYBLAS_STATUS_DIMENSION_OVERFLOW:
+        return "Dimension overflow";
+    default:
+        return "Unknown error";
+    }
+}
+
+/**
+ * Performs single-precision general matrix multiplication.
+ *
+ * This is a column major GEMM subroutine for computing C = α*A*B + β*C.
+ *
+ * @param handle was created by tinyblasCreate()
+ * @param transa if `A` should be transposed
+ * @param transb if `B` should be transposed
+ * @param m is rows in `A` and `C`
+ * @param n is cols in `B` and `C`
+ * @param k is cols in `A` and rows in `B`
+ * @param alpha points to scalar that's multiplied against input
+ * @param A is input array of first matrix
+ * @param lda is row stride of `A`
+ * @param B is input array of second matrix
+ * @param ldb is row stride of `B`
+ * @param beta points to scalar that's multiplied against the existing
+ *     output, but this multiplication only happens if beta is nonzero
+ * @param C is input/output array of output matrix
+ * @param ldc is row stride of `C`
+ */
+tinyblasStatus_t tinyblasSgemm(tinyblasHandle_t handle, tinyblasOperation_t transa,
+                               tinyblasOperation_t transb, int m, int n, int k, const float *alpha,
+                               const float *A, int lda, const float *B, int ldb, const float *beta,
+                               float *C, int ldc) {
+    return tinyblasGemmEx(handle, transa, transb, m, n, k, alpha, A, TINYBLAS_R_32F, lda, B,
+                          TINYBLAS_R_32F, ldb, beta, C, TINYBLAS_R_32F, ldc, TINYBLAS_COMPUTE_32F,
+                          TINYBLAS_GEMM_DEFAULT);
+}
+
+template <int CONFIG, int BM, int BN, int BK, int VE, int WM, int WN, int WNI, int TM, int TN,
+          int TT, typename WORD, typename SRC, typename DST>
+static __global__ void __launch_bounds__(TT) tinyblasGE_entry(tinyblasOperation_t aT, //
+                                                              tinyblasOperation_t bT, //
+                                                              int m, int n, int k, WORD alpha, //
+                                                              const SRC *A, int lda, //
+                                                              const SRC *B, int ldb, //
+                                                              WORD beta, DST *C, int ldc) {
+    matmul_warp2d<CONFIG, BM, BN, BK, VE, WM, WN, WNI, TM, TN, TT>(aT, bT, m, n, k, alpha, A, lda,
+                                                                   B, ldb, beta, C, ldc);
+}
+
+template <int BM, int BN, int BK, int VE, int WM, int WN, int WNI, int TM, int TN, int TT,
+          typename WORD, typename SRC, typename DST>
+static tinyblasStatus_t tinyblasGE_launcher(tinyblasHandle_t handle, tinyblasOperation_t aT,
+                                            tinyblasOperation_t bT, int m, int n, int k, WORD alpha,
+                                            const SRC *A, int lda, const SRC *B, int ldb, WORD beta,
+                                            DST *C, int ldc) {
+    dim3 blocks(CEIL_DIV(n, BN), CEIL_DIV(m, BM));
+    if ((!beta && //
+         isone(alpha) && //
+         n % BN == 0 && //
+         k % BK == 0 && //
+         aT == TINYBLAS_OP_N && //
+         bT == TINYBLAS_OP_T)) {
+        constexpr int CONFIG = IGNORE_BETA | IGNORE_ALPHA | ASSUME_A_OP_N | ASSUME_B_OP_T |
+                               ASSUME_N_SAFE | ASSUME_K_SAFE;
+        tinyblasGE_entry<CONFIG, BM, BN, BK, VE, WM, WN, WNI, TM, TN, TT>
+            <<<blocks, TT, 0, handle->stream>>>(aT, bT, m, n, k, alpha, A, lda, B, ldb, beta, C,
+                                                ldc);
+    } else {
+        tinyblasGE_entry<0, BM, BN, BK, VE, WM, WN, WNI, TM, TN, TT>
+            <<<blocks, TT, 0, handle->stream>>>(aT, bT, m, n, k, alpha, A, lda, B, ldb, beta, C,
+                                                ldc);
+    }
+    if (cudaGetLastError() != cudaSuccess)
+        return TINYBLAS_STATUS_EXECUTION_FAILED;
+    return TINYBLAS_STATUS_SUCCESS;
+}
+
+template <typename WORD, typename SRC, typename DST>
+tinyblasStatus_t tinyblasGE_launch(tinyblasHandle_t handle, tinyblasOperation_t aT,
+                                   tinyblasOperation_t bT, int m, int n, int k, WORD alpha,
+                                   const SRC *A, int lda, const SRC *B, int ldb, WORD beta, DST *C,
+                                   int ldc) {
+    if (can_use_matvec(aT, bT, m, n, k, alpha, beta))
+        return matvec_launch<WORD>(handle, m, k, A, lda, B, C);
+    constexpr int TT = 256;
+    constexpr int BM = 128;
+    constexpr int BN = 64;
+    constexpr int BK = 64;
+    constexpr int VE = 16;
+    constexpr int WM = 32;
+    constexpr int WN = 32;
+    constexpr int WNI = 1;
+    constexpr int TM = 8;
+    constexpr int TN = 4;
+    return tinyblasGE_launcher<BM, BN, BK, VE, WM, WN, WNI, TM, TN, TT>(
+        handle, bT, aT, n, m, k, alpha, B, ldb, A, lda, beta, C, ldc);
+}
+
+/**
+ * Performs extended general matrix multiplication.
+ *
+ * This is a column major GEMM subroutine for computing C = α*A*B + β*C.
+ *
+ * @param handle was created by tinyblasCreate()
+ * @param transa if `A` should be transposed
+ * @param transb if `B` should be transposed
+ * @param m is rows in `A` and `C`
+ * @param n is cols in `B` and `C`
+ * @param k is cols in `A` and rows in `B`
+ * @param alpha points to scalar that's multiplied against input
+ * @param A is input array of first matrix
+ * @param Atype is data type of `C`
+ * @param lda is row stride of `A`
+ * @param B is input array of second matrix
+ * @param Btype is data type of `C`
+ * @param ldb is row stride of `B`
+ * @param beta points to scalar that's multiplied against the existing
+ *     output, but this multiplication only happens if beta is nonzero
+ * @param C is input/output array of output matrix
+ * @param Ctype is data type of `C`
+ * @param ldc is row stride of `C`
+ * @param computeType is data type of `alpha`, `beta`, and dot product
+ * @param algo specifies algorithm to use
+ */
+tinyblasStatus_t tinyblasGemmEx(tinyblasHandle_t handle, //
+                                tinyblasOperation_t transa, //
+                                tinyblasOperation_t transb, //
+                                int m, int n, int k, //
+                                const void *alpha, //
+                                const void *A, tinyblasDataType_t Atype, int lda, //
+                                const void *B, tinyblasDataType_t Btype, int ldb, //
+                                const void *beta, //
+                                void *C, tinyblasDataType_t Ctype, int ldc, //
+                                tinyblasComputeType_t computeType, //
+                                tinyblasGemmAlgo_t algo) {
+
+    if (m < 0 || n < 0 || k < 0)
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (lda < std::max(1, transa ? k : m))
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (ldb < std::max(1, transb ? n : k))
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (ldc < std::max(1, m))
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (1ll * lda * ((transa ? k : m) - 1) + ((transa ? m : k) - 1) > INT_MAX)
+        return TINYBLAS_STATUS_DIMENSION_OVERFLOW;
+    if (1ll * ldb * ((transb ? n : k) - 1) + ((transb ? k : n) - 1) > INT_MAX)
+        return TINYBLAS_STATUS_DIMENSION_OVERFLOW;
+    if (1ll * ldc * (n - 1) + (m - 1) > INT_MAX)
+        return TINYBLAS_STATUS_DIMENSION_OVERFLOW;
+    if (algo != TINYBLAS_GEMM_DEFAULT)
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (Atype != Btype)
+        return TINYBLAS_STATUS_NOT_SUPPORTED;
+
+    switch (Atype) {
+    case TINYBLAS_R_16F:
+        switch (Ctype) {
+        case TINYBLAS_R_16F:
+            switch (computeType) {
+            case TINYBLAS_COMPUTE_16F:
+                return tinyblasGE_launch(
+                    handle, transa, transb, m, n, k, (float)*(const half *)alpha, (const half *)A,
+                    lda, (const half *)B, ldb, (float)*(const half *)beta, (half *)C, ldc);
+            case TINYBLAS_COMPUTE_32F:
+                return tinyblasGE_launch(handle, transa, transb, m, n, k, *(const float *)alpha,
+                                         (const half *)A, lda, (const half *)B, ldb,
+                                         *(const float *)beta, (half *)C, ldc);
+            default:
+                return TINYBLAS_STATUS_INVALID_VALUE;
+            }
+        case TINYBLAS_R_32F:
+            switch (computeType) {
+            case TINYBLAS_COMPUTE_16F:
+                return TINYBLAS_STATUS_NOT_SUPPORTED;
+            case TINYBLAS_COMPUTE_32F:
+                return tinyblasGE_launch(handle, transa, transb, m, n, k, *(const float *)alpha,
+                                         (const half *)A, lda, (const half *)B, ldb,
+                                         *(const float *)beta, (float *)C, ldc);
+            default:
+                return TINYBLAS_STATUS_INVALID_VALUE;
+            }
+        default:
+            return TINYBLAS_STATUS_INVALID_VALUE;
+        }
+    case TINYBLAS_R_32F:
+        switch (Ctype) {
+        case TINYBLAS_R_16F:
+            return TINYBLAS_STATUS_NOT_SUPPORTED;
+        case TINYBLAS_R_32F:
+            switch (computeType) {
+            case TINYBLAS_COMPUTE_16F:
+                return TINYBLAS_STATUS_NOT_SUPPORTED;
+            case TINYBLAS_COMPUTE_32F:
+                return tinyblasGE_launch(handle, transa, transb, m, n, k, *(const float *)alpha,
+                                         (const float *)A, lda, (const float *)B, ldb,
+                                         *(const float *)beta, (float *)C, ldc);
+            default:
+                return TINYBLAS_STATUS_INVALID_VALUE;
+            }
+        default:
+            return TINYBLAS_STATUS_INVALID_VALUE;
+        }
+    default:
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    }
+}
+
+template <typename WORD, typename SRC, typename DST>
+static __global__ __launch_bounds__(WARPSIZE) void matvecGBE_entry(int m, int k, //
+                                                                   const SRC *const A[], int lda,
+                                                                   const SRC *const B[],
+                                                                   DST *const C[]) {
+    matvec<WORD>(m, k, A[blockIdx.z], lda, B[blockIdx.z], C[blockIdx.z]);
+}
+
+template <int BM, int BN, int TM, int TN, typename WORD, typename SRC, typename DST>
+static __global__ void KERNEL tinyblasGBE_entry(tinyblasOperation_t transa,
+                                                tinyblasOperation_t transb, int m, int n, int k,
+                                                WORD alpha, const SRC *const Aarray[], int lda,
+                                                const SRC *const Barray[], int ldb, WORD beta,
+                                                DST *const Carray[], int ldc, int batchCount) {
+    matmul_block2d<0, BM, BN, TM, TN>(transa, transb, m, n, k, alpha, Aarray[blockIdx.z], lda,
+                                      Barray[blockIdx.z], ldb, beta, Carray[blockIdx.z], ldc);
+}
+
+template <typename WORD, typename SRC, typename DST>
+static tinyblasStatus_t tinyblasGBE_launch(tinyblasHandle_t handle, tinyblasOperation_t transa,
+                                           tinyblasOperation_t transb, int m, int n, int k,
+                                           WORD alpha, const SRC *const *Aarray, int lda,
+                                           const SRC *const *Barray, int ldb, WORD beta,
+                                           DST *const *Carray, int ldc, int batchCount) {
+    if (can_use_matvec(transa, transb, m, n, k, alpha, beta)) {
+        dim3 blocks(WARPSIZE, m / WARPSIZE, batchCount);
+        matvecGBE_entry<WORD>
+            <<<blocks, WARPSIZE, 0, handle->stream>>>(m, k, Aarray, lda, Barray, Carray);
+    } else {
+        constexpr int BM = 16;
+        constexpr int BN = 16;
+        constexpr int TM = 4;
+        constexpr int TN = 4;
+        dim3 blocks(CEIL_DIV(m, BM), CEIL_DIV(n, BN), batchCount);
+        tinyblasGBE_entry<BM, BN, TM, TN><<<blocks, THREAD_COUNT, 0, handle->stream>>>(
+            transa, transb, m, n, k, alpha, Aarray, lda, Barray, ldb, beta, Carray, ldc,
+            batchCount);
+    }
+    if (cudaGetLastError() != cudaSuccess)
+        return TINYBLAS_STATUS_EXECUTION_FAILED;
+    return TINYBLAS_STATUS_SUCCESS;
+}
+
+/**
+ * Multiplies matrices.
+ *
+ * This is a column major GEMM subroutine for computing C = α*A*B + β*C.
+ *
+ * @param handle was created by tinyblasCreate()
+ * @param transa if `A` should be transposed
+ * @param transb if `B` should be transposed
+ * @param m is rows in `A` and `C`
+ * @param n is cols in `B` and `C`
+ * @param k is cols in `A` and rows in `B`
+ * @param alpha points to scalar that's multiplied against input
+ * @param A is input array of device memory pointing to first matrices
+ * @param Atype is data type of `C`
+ * @param lda is row stride of `A`
+ * @param B is input array of device memory pointing to second matrices
+ * @param Btype is data type of `C`
+ * @param ldb is row stride of `B`
+ * @param beta points to scalar that's multiplied against the existing
+ *     output, but this multiplication only happens if beta is nonzero
+ * @param C is input/output array of output matrices
+ * @param Ctype is data type of `C`
+ * @param ldc is row stride of `C`
+ * @param batchCount is number of elements in `A`, `B`, and `C`
+ * @param computeType is data type of `alpha`, `beta`, and dot product
+ * @param algo specifies algorithm to use
+ */
+tinyblasStatus_t tinyblasGemmBatchedEx(tinyblasHandle_t handle, tinyblasOperation_t transa,
+                                       tinyblasOperation_t transb, int m, int n, int k,
+                                       const void *alpha, const void *const Aarray[],
+                                       tinyblasDataType_t Atype, int lda,
+                                       const void *const Barray[], tinyblasDataType_t Btype,
+                                       int ldb, const void *beta, void *const Carray[],
+                                       tinyblasDataType_t Ctype, int ldc, int batchCount,
+                                       tinyblasComputeType_t computeType, tinyblasGemmAlgo_t algo) {
+
+    if (m < 0 || n < 0 || k < 0)
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (lda < std::max(1, transa ? k : m))
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (ldb < std::max(1, transb ? n : k))
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (ldc < std::max(1, m))
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (1ll * lda * ((transa ? k : m) - 1) + ((transa ? m : k) - 1) > INT_MAX)
+        return TINYBLAS_STATUS_DIMENSION_OVERFLOW;
+    if (1ll * ldb * ((transb ? n : k) - 1) + ((transb ? k : n) - 1) > INT_MAX)
+        return TINYBLAS_STATUS_DIMENSION_OVERFLOW;
+    if (1ll * ldc * (n - 1) + (m - 1) > INT_MAX)
+        return TINYBLAS_STATUS_DIMENSION_OVERFLOW;
+    if (algo != TINYBLAS_GEMM_DEFAULT)
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (Atype != Btype)
+        return TINYBLAS_STATUS_NOT_SUPPORTED;
+
+    switch (Atype) {
+    case TINYBLAS_R_16F:
+        switch (Ctype) {
+        case TINYBLAS_R_16F:
+            switch (computeType) {
+            case TINYBLAS_COMPUTE_16F:
+                return tinyblasGBE_launch(
+                    handle, transa, transb, m, n, k, (float)*(const half *)alpha,
+                    (const half *const *)Aarray, lda, (const half *const *)Barray, ldb,
+                    (float)*(const half *)beta, (half *const *)Carray, ldc, batchCount);
+            case TINYBLAS_COMPUTE_32F:
+                return tinyblasGBE_launch(handle, transa, transb, m, n, k, *(const float *)alpha,
+                                          (const half *const *)Aarray, lda,
+                                          (const half *const *)Barray, ldb, *(const float *)beta,
+                                          (half *const *)Carray, ldc, batchCount);
+            default:
+                return TINYBLAS_STATUS_INVALID_VALUE;
+            }
+        case TINYBLAS_R_32F:
+            switch (computeType) {
+            case TINYBLAS_COMPUTE_16F:
+                return TINYBLAS_STATUS_NOT_SUPPORTED;
+            case TINYBLAS_COMPUTE_32F:
+                return tinyblasGBE_launch(handle, transa, transb, m, n, k, *(const float *)alpha,
+                                          (const half *const *)Aarray, lda,
+                                          (const half *const *)Barray, ldb, *(const float *)beta,
+                                          (float *const *)Carray, ldc, batchCount);
+            default:
+                return TINYBLAS_STATUS_INVALID_VALUE;
+            }
+        default:
+            return TINYBLAS_STATUS_INVALID_VALUE;
+        }
+    case TINYBLAS_R_32F:
+        switch (Ctype) {
+        case TINYBLAS_R_16F:
+            return TINYBLAS_STATUS_NOT_SUPPORTED;
+        case TINYBLAS_R_32F:
+            switch (computeType) {
+            case TINYBLAS_COMPUTE_16F:
+                return TINYBLAS_STATUS_NOT_SUPPORTED;
+            case TINYBLAS_COMPUTE_32F:
+                return tinyblasGBE_launch(handle, transa, transb, m, n, k, *(const float *)alpha,
+                                          (const float *const *)Aarray, lda,
+                                          (const float *const *)Barray, ldb, *(const float *)beta,
+                                          (float *const *)Carray, ldc, batchCount);
+            default:
+                return TINYBLAS_STATUS_INVALID_VALUE;
+            }
+        default:
+            return TINYBLAS_STATUS_INVALID_VALUE;
+        }
+    default:
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    }
+}
+
+template <typename WORD, typename SRC, typename DST>
+static __global__ __launch_bounds__(WARPSIZE) void matvecGSBE_entry(int m, int k, const SRC *A,
+                                                                    int lda, long long strideA,
+                                                                    const SRC *B, long long strideB,
+                                                                    DST *C, long long strideC) {
+    matvec<WORD>(m, k, A + blockIdx.z * strideA, lda, B + blockIdx.z * strideB,
+                 C + blockIdx.z * strideC);
+}
+
+template <int CONFIG, int BM, int BN, int TM, int TN, typename SRC, typename DST, typename WORD>
+static __global__ void KERNEL tinyblasGSBE_entry(tinyblasOperation_t transa,
+                                                 tinyblasOperation_t transb, int m, int n, int k,
+                                                 WORD alpha, const SRC *A, int lda,
+                                                 long long strideA, const SRC *B, int ldb,
+                                                 long long strideB, WORD beta, DST *C, int ldc,
+                                                 long long strideC, int batchCount) {
+    matmul_block2d<CONFIG, BM, BN, TM, TN>(transa, transb, m, n, k, alpha, A + strideA * blockIdx.z,
+                                           lda, B + strideB * blockIdx.z, ldb, beta,
+                                           C + strideC * blockIdx.z, ldc);
+}
+
+template <typename WORD, typename SRC, typename DST>
+static tinyblasStatus_t tinyblasGSBE_launch(tinyblasHandle_t handle, tinyblasOperation_t transa,
+                                            tinyblasOperation_t transb, int m, int n, int k,
+                                            WORD alpha, const SRC *A, int lda, long long strideA,
+                                            const SRC *B, int ldb, long long strideB, WORD beta,
+                                            DST *C, int ldc, long long strideC, int batchCount) {
+    if (can_use_matvec(transa, transb, m, n, k, alpha, beta)) {
+        dim3 blocks(WARPSIZE, m / WARPSIZE, batchCount);
+        matvecGSBE_entry<WORD><<<blocks, WARPSIZE, 0, handle->stream>>>(m, k, A, lda, strideA, B,
+                                                                        strideB, C, strideC);
+    } else {
+        constexpr int BM = 16;
+        constexpr int BN = 16;
+        constexpr int TM = 4;
+        constexpr int TN = 4;
+        constexpr int BK = THREAD_COUNT;
+        dim3 blocks(CEIL_DIV(m, BM), CEIL_DIV(n, BN), batchCount);
+        if ((!beta && //
+             isone(alpha) && //
+             m % BM == 0 && //
+             k % BK == 0 && //
+             transa == TINYBLAS_OP_T && //
+             transb == TINYBLAS_OP_N)) {
+            constexpr int CONFIG = IGNORE_BETA | IGNORE_ALPHA | ASSUME_A_OP_T | ASSUME_B_OP_N |
+                                   ASSUME_M_SAFE | ASSUME_K_SAFE;
+            tinyblasGSBE_entry<CONFIG, BM, BN, TM, TN><<<blocks, THREAD_COUNT, 0, handle->stream>>>(
+                transa, transb, m, n, k, alpha, A, lda, strideA, B, ldb, strideB, beta, C, ldc,
+                strideC, batchCount);
+        } else {
+            tinyblasGSBE_entry<0, BM, BN, TM, TN><<<blocks, THREAD_COUNT, 0, handle->stream>>>(
+                transa, transb, m, n, k, alpha, A, lda, strideA, B, ldb, strideB, beta, C, ldc,
+                strideC, batchCount);
+        }
+    }
+    if (cudaGetLastError() != cudaSuccess)
+        return TINYBLAS_STATUS_EXECUTION_FAILED;
+    return TINYBLAS_STATUS_SUCCESS;
+}
+
+/**
+ * Multiplies matrices.
+ *
+ * This is a column major GEMM subroutine for computing C = α*A*B + β*C.
+ *
+ * @param handle was created by tinyblasCreate()
+ * @param transa if `A` should be transposed
+ * @param transb if `B` should be transposed
+ * @param m is rows in `A` and `C`
+ * @param n is cols in `B` and `C`
+ * @param k is cols in `A` and rows in `B`
+ * @param alpha points to scalar that's multiplied against input
+ * @param A is input array of first matrices
+ * @param Atype is data type of `A`
+ * @param lda is row stride of `A`
+ * @param strideA is distance between matrices in `A`
+ * @param B is input array of second matrices
+ * @param Btype is data type of `B`
+ * @param ldb is row stride of `B`
+ * @param strideB is distance between matrices in `B`
+ * @param beta points to scalar that's multiplied against the existing
+ *     output, but this multiplication only happens if beta is nonzero
+ * @param C is input/output array of output matrices
+ * @param Ctype is data type of `C`
+ * @param ldc is row stride of `C`
+ * @param strideC is distance between matrices in `C`, which must not overlap
+ * @param batchCount is number of matrices to multiply
+ * @param computeType is data type of `alpha`, `beta`, and dot product
+ * @param algo specifies algorithm to use
+ */
+tinyblasStatus_t tinyblasGemmStridedBatchedEx(tinyblasHandle_t handle, //
+                                              tinyblasOperation_t transa, //
+                                              tinyblasOperation_t transb, //
+                                              int m, int n, int k, //
+                                              const void *alpha, //
+                                              const void *A, tinyblasDataType_t Atype, int lda,
+                                              long long strideA, //
+                                              const void *B, tinyblasDataType_t Btype, int ldb,
+                                              long long strideB, //
+                                              const void *beta, //
+                                              void *C, tinyblasDataType_t Ctype, int ldc,
+                                              long long strideC, //
+                                              int batchCount, //
+                                              tinyblasComputeType_t computeType, //
+                                              tinyblasGemmAlgo_t algo) {
+
+    if (m < 0 || n < 0 || k < 0)
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (lda < std::max(1, transa ? k : m))
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (ldb < std::max(1, transb ? n : k))
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (ldc < std::max(1, m))
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (std::max(0ll, strideC) < std::min(1ll * ldc * n, strideC * 2))
+        return TINYBLAS_STATUS_DIMENSION_OVERLAP;
+    if (1ll * lda * ((transa ? k : m) - 1) + ((transa ? m : k) - 1) > INT_MAX)
+        return TINYBLAS_STATUS_DIMENSION_OVERFLOW;
+    if (1ll * ldb * ((transb ? n : k) - 1) + ((transb ? k : n) - 1) > INT_MAX)
+        return TINYBLAS_STATUS_DIMENSION_OVERFLOW;
+    if (1ll * ldc * (n - 1) + (m - 1) > INT_MAX)
+        return TINYBLAS_STATUS_DIMENSION_OVERFLOW;
+    if (algo != TINYBLAS_GEMM_DEFAULT)
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (Atype != Btype)
+        return TINYBLAS_STATUS_NOT_SUPPORTED;
+
+    switch (Atype) {
+    case TINYBLAS_R_16F:
+        switch (Ctype) {
+        case TINYBLAS_R_16F:
+            switch (computeType) {
+            case TINYBLAS_COMPUTE_16F:
+                return tinyblasGSBE_launch(
+                    handle, transa, transb, m, n, k, (float)*(const half *)alpha, (const half *)A,
+                    lda, strideA, (const half *)B, ldb, strideB, (float)*(const half *)beta,
+                    (half *)C, ldc, strideC, batchCount);
+            case TINYBLAS_COMPUTE_32F:
+                return tinyblasGSBE_launch(handle, transa, transb, m, n, k, *(const float *)alpha,
+                                           (const half *)A, lda, strideA, (const half *)B, ldb,
+                                           strideB, *(const float *)beta, (half *)C, ldc, strideC,
+                                           batchCount);
+            default:
+                return TINYBLAS_STATUS_INVALID_VALUE;
+            }
+        case TINYBLAS_R_32F:
+            switch (computeType) {
+            case TINYBLAS_COMPUTE_16F:
+                return TINYBLAS_STATUS_NOT_SUPPORTED;
+            case TINYBLAS_COMPUTE_32F:
+                return tinyblasGSBE_launch(handle, transa, transb, m, n, k, *(const float *)alpha,
+                                           (const half *)A, lda, strideA, (const half *)B, ldb,
+                                           strideB, *(const float *)beta, (float *)C, ldc, strideC,
+                                           batchCount);
+            default:
+                return TINYBLAS_STATUS_INVALID_VALUE;
+            }
+        default:
+            return TINYBLAS_STATUS_INVALID_VALUE;
+        }
+    case TINYBLAS_R_32F:
+        switch (Ctype) {
+        case TINYBLAS_R_16F:
+            return TINYBLAS_STATUS_NOT_SUPPORTED;
+        case TINYBLAS_R_32F:
+            switch (computeType) {
+            case TINYBLAS_COMPUTE_16F:
+                return TINYBLAS_STATUS_NOT_SUPPORTED;
+            case TINYBLAS_COMPUTE_32F:
+                return tinyblasGSBE_launch(handle, transa, transb, m, n, k, *(const float *)alpha,
+                                           (const float *)A, lda, strideA, (const float *)B, ldb,
+                                           strideB, *(const float *)beta, (float *)C, ldc, strideC,
+                                           batchCount);
+            default:
+                return TINYBLAS_STATUS_INVALID_VALUE;
+            }
+        default:
+            return TINYBLAS_STATUS_INVALID_VALUE;
+        }
+    default:
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    }
+}
diff --git a/ggml/src/ggml-cuda/tinyblas.h b/ggml/src/ggml-cuda/tinyblas.h
new file mode 100644
index 00000000..d6c2aae1
--- /dev/null
+++ b/ggml/src/ggml-cuda/tinyblas.h
@@ -0,0 +1,66 @@
+// -*- mode:c++;indent-tabs-mode:nil;c-basic-offset:4;coding:utf-8 -*-
+// vi: set et ft=cpp ts=4 sts=4 sw=4 fenc=utf-8 :vi
+#pragma once
+
+typedef enum tinyblasOperation {
+    TINYBLAS_OP_N,
+    TINYBLAS_OP_T,
+} tinyblasOperation_t;
+
+typedef enum tinyblasDataType {
+    TINYBLAS_R_32F,
+    TINYBLAS_R_16F,
+} tinyblasDataType_t;
+
+typedef enum tinyblasComputeType {
+    TINYBLAS_COMPUTE_32F,
+    TINYBLAS_COMPUTE_16F,
+} tinyblasComputeType_t;
+
+typedef enum tinyblasGemmAlgo {
+    TINYBLAS_GEMM_DEFAULT,
+} tinyblasGemmAlgo_t;
+
+typedef enum tinyblasStatus {
+    TINYBLAS_STATUS_SUCCESS,
+    TINYBLAS_STATUS_ALLOC_FAILED,
+    TINYBLAS_STATUS_INVALID_VALUE,
+    TINYBLAS_STATUS_NOT_SUPPORTED,
+    TINYBLAS_STATUS_EXECUTION_FAILED,
+    TINYBLAS_STATUS_DIMENSION_OVERLAP,
+    TINYBLAS_STATUS_DIMENSION_OVERFLOW,
+} tinyblasStatus_t;
+
+struct tinyblasContext;
+typedef struct tinyblasContext *tinyblasHandle_t;
+
+const char *tinyblasGetStatusString(tinyblasStatus_t);
+
+tinyblasStatus_t tinyblasCreate(tinyblasHandle_t *);
+tinyblasStatus_t tinyblasDestroy(tinyblasHandle_t);
+tinyblasStatus_t tinyblasSetStream(tinyblasHandle_t, void *);
+tinyblasStatus_t tinyblasGetStream(tinyblasHandle_t, void **);
+
+tinyblasStatus_t tinyblasSgemm(tinyblasHandle_t, tinyblasOperation_t, tinyblasOperation_t, int, int,
+                               int, const float *, const float *, int, const float *, int,
+                               const float *, float *, int);
+
+tinyblasStatus_t tinyblasGemmEx(tinyblasHandle_t, tinyblasOperation_t, tinyblasOperation_t, int,
+                                int, int, const void *, const void *, tinyblasDataType_t, int,
+                                const void *, tinyblasDataType_t, int, const void *, void *,
+                                tinyblasDataType_t, int, tinyblasComputeType_t, tinyblasGemmAlgo_t);
+
+tinyblasStatus_t tinyblasGemmBatchedEx(tinyblasHandle_t, tinyblasOperation_t, tinyblasOperation_t,
+                                       int, int, int, const void *, const void *const[],
+                                       tinyblasDataType_t, int, const void *const[],
+                                       tinyblasDataType_t, int, const void *, void *const[],
+                                       tinyblasDataType_t, int, int, tinyblasComputeType_t,
+                                       tinyblasGemmAlgo_t);
+
+tinyblasStatus_t tinyblasGemmStridedBatchedEx(tinyblasHandle_t, tinyblasOperation_t,
+                                              tinyblasOperation_t, int, int, int, const void *,
+                                              const void *, tinyblasDataType_t, int, long long,
+                                              const void *, tinyblasDataType_t, int, long long,
+                                              const void *, void *, tinyblasDataType_t, int,
+                                              long long, int, tinyblasComputeType_t,
+                                              tinyblasGemmAlgo_t);
diff --git a/ggml/src/ggml-cuda/vendors/cuda.h b/ggml/src/ggml-cuda/vendors/cuda.h
index 3b308677..5a69c0ec 100644
--- a/ggml/src/ggml-cuda/vendors/cuda.h
+++ b/ggml/src/ggml-cuda/vendors/cuda.h
@@ -1,10 +1,22 @@
-#pragma once
-
 #include <cuda_runtime.h>
 #include <cuda.h>
+
+#ifndef GGML_USE_TINYBLAS // 1
 #include <cublas_v2.h>
-#include <cuda_bf16.h>
+#endif //GGML_USE_TINYBLAS 1
+
 #include <cuda_fp16.h>
+#include <cuda_bf16.h>
+
+#ifdef GGML_USE_TINYBLAS // 2
+
+#include "tinyblas_vendor.h"
+#define cublasGemmAlgo_t tinyblasGemmAlgo_t
+#define cublasOperation_t tinyblasOperation_t
+
+#else
+
+#define BLAS_NAME GGML_CUBLAS_NAME
 
 #if CUDART_VERSION >= 12050
 #include <cuda_fp8.h>
@@ -17,3 +29,5 @@
 #define CUBLAS_COMPUTE_32F CUDA_R_32F
 #define cublasComputeType_t cudaDataType_t
 #endif // CUDART_VERSION < 11020
+
+#endif // GGML_USE_TINYBLAS 2
diff --git a/ggml/src/ggml-cuda/vendors/hip.h b/ggml/src/ggml-cuda/vendors/hip.h
index c31f3192..058912de 100644
--- a/ggml/src/ggml-cuda/vendors/hip.h
+++ b/ggml/src/ggml-cuda/vendors/hip.h
@@ -1,12 +1,22 @@
-#pragma once
-
-#define HIP_ENABLE_WARP_SYNC_BUILTINS 1
 #include <hip/hip_runtime.h>
-#include <hipblas/hipblas.h>
 #include <hip/hip_fp16.h>
 #include <hip/hip_bfloat16.h>
+
+#ifdef GGML_USE_TINYBLAS
+
+#include "tinyblas_vendor.h"
+#define __nv_bfloat16 hip_bfloat16
+#define CUBLAS_COMPUTE_32F_FAST_16F TINYBLAS_COMPUTE_32F
+#define CUBLAS_TF32_TENSOR_OP_MATH 0
+
+#else
+
+#define BLAS_NAME GGML_CUBLAS_NAME
+
+#include <hipblas/hipblas.h>
 // for rocblas_initialize()
 #include "rocblas/rocblas.h"
+typedef hip_bfloat16 nv_bfloat16;
 
 #define CUBLAS_GEMM_DEFAULT HIPBLAS_GEMM_DEFAULT
 #define CUBLAS_GEMM_DEFAULT_TENSOR_OP HIPBLAS_GEMM_DEFAULT
@@ -17,14 +27,21 @@
 #define CUDA_R_16F  HIPBLAS_R_16F
 #define CUDA_R_16BF HIPBLAS_R_16B
 #define CUDA_R_32F  HIPBLAS_R_32F
+#endif //GGML_USE_TINYBLAS
+
+#ifndef GGML_USE_TINYBLAS //2
 #define CU_DEVICE_ATTRIBUTE_VIRTUAL_MEMORY_MANAGEMENT_SUPPORTED hipDeviceAttributeVirtualMemoryManagementSupported
 #define CU_MEM_ALLOC_GRANULARITY_RECOMMENDED hipMemAllocationGranularityRecommended
 #define CU_MEM_ALLOCATION_TYPE_PINNED hipMemAllocationTypePinned
 #define CU_MEM_LOCATION_TYPE_DEVICE hipMemLocationTypeDevice
 #define CU_MEM_ACCESS_FLAGS_PROT_READWRITE hipMemAccessFlagsProtReadWrite
 #define CU_CHECK(fn) {hipError_t err = fn; if(err != hipSuccess) { GGML_ABORT("HipVMM Failure: %s\n", hipGetErrorString(err)); }}
+#endif //GGML_USE_TINYBLAS 2
+
 #define __shfl_sync(mask, var, laneMask, width) __shfl(var, laneMask, width)
 #define __shfl_xor_sync(mask, var, laneMask, width) __shfl_xor(var, laneMask, width)
+
+#ifndef GGML_USE_TINYBLAS //3
 #define cublasCreate hipblasCreate
 #define cublasDestroy hipblasDestroy
 #define cublasGemmEx hipblasGemmEx
@@ -36,6 +53,8 @@
 #define cublasSgemm hipblasSgemm
 #define cublasStatus_t hipblasStatus_t
 #define cublasOperation_t hipblasOperation_t
+#endif //GGML_USE_TINYBLAS 3
+
 #define cudaDeviceCanAccessPeer hipDeviceCanAccessPeer
 #define cudaDeviceDisablePeerAccess hipDeviceDisablePeerAccess
 #define cudaDeviceEnablePeerAccess hipDeviceEnablePeerAccess
@@ -79,6 +98,8 @@
 #define cudaMemGetInfo hipMemGetInfo
 #define cudaOccupancyMaxPotentialBlockSize hipOccupancyMaxPotentialBlockSize
 #define cudaSetDevice hipSetDevice
+
+#ifndef GGML_USE_TINYBLAS //4
 #define cuDeviceGet hipDeviceGet
 #define CUdevice hipDevice_t
 #define CUdeviceptr hipDeviceptr_t
@@ -94,6 +115,8 @@
 #define cuMemGetAllocationGranularity hipMemGetAllocationGranularity
 #define CUmemAllocationProp hipMemAllocationProp
 #define cuDeviceGetAttribute hipDeviceGetAttribute
+#endif //GGML_USE_TINYBLAS 4
+
 #define cudaStreamCreateWithFlags hipStreamCreateWithFlags
 #define cudaStreamDestroy hipStreamDestroy
 #define cudaStreamFireAndForget hipStreamFireAndForget
@@ -101,6 +124,8 @@
 #define cudaStreamPerThread hipStreamPerThread
 #define cudaStreamSynchronize hipStreamSynchronize
 #define cudaStreamWaitEvent(stream, event, flags) hipStreamWaitEvent(stream, event, flags)
+
+#ifndef GGML_USE_TINYBLAS //5
 #define cudaGraphExec_t hipGraphExec_t
 #define cudaGraphNode_t hipGraphNode_t
 #define cudaKernelNodeParams hipKernelNodeParams
@@ -123,10 +148,14 @@
 #define cudaStreamCaptureModeRelaxed hipStreamCaptureModeRelaxed
 #define cudaStreamBeginCapture hipStreamBeginCapture
 #define cudaGraph_t hipGraph_t
+#endif //GGML_USE_TINYBLAS 5
+
 #define cudaStream_t hipStream_t
 #define cudaSuccess hipSuccess
 #define cudaOccupancyMaxActiveBlocksPerMultiprocessor hipOccupancyMaxActiveBlocksPerMultiprocessor
 #define __trap() do { abort(); __builtin_unreachable(); } while(0)
+
+#ifndef GGML_USE_TINYBLAS //6
 #define CUBLAS_STATUS_SUCCESS HIPBLAS_STATUS_SUCCESS
 #define CUBLAS_STATUS_NOT_INITIALIZED HIPBLAS_STATUS_NOT_INITIALIZED
 #define CUBLAS_STATUS_ALLOC_FAILED HIPBLAS_STATUS_ALLOC_FAILED
@@ -136,6 +165,7 @@
 #define CUBLAS_STATUS_EXECUTION_FAILED HIPBLAS_STATUS_EXECUTION_FAILED
 #define CUBLAS_STATUS_INTERNAL_ERROR HIPBLAS_STATUS_INTERNAL_ERROR
 #define CUBLAS_STATUS_NOT_SUPPORTED HIPBLAS_STATUS_NOT_SUPPORTED
+#endif //GGML_USE_TINYBLAS 6
 
 #if HIP_VERSION >= 70000000
 #define CUBLAS_COMPUTE_16F HIPBLAS_COMPUTE_16F
@@ -199,7 +229,6 @@
     #define __has_builtin(x) 0
 #endif
 
-typedef hip_bfloat16 nv_bfloat16;
 typedef short2 nv_bfloat162; // FIXME there is no 2x BF16 type being defined in bfloat16.h, ad-hoc compilation fix
 
 typedef int8_t int8x4_t __attribute__((ext_vector_type(4)));
diff --git a/ggml/src/ggml-cuda/vendors/tinyblas_vendor.h b/ggml/src/ggml-cuda/vendors/tinyblas_vendor.h
new file mode 100644
index 00000000..0ddcf366
--- /dev/null
+++ b/ggml/src/ggml-cuda/vendors/tinyblas_vendor.h
@@ -0,0 +1,27 @@
+#pragma once
+
+#include "../tinyblas.h"
+#define CUBLAS_COMPUTE_16F TINYBLAS_COMPUTE_16F
+#define CUBLAS_COMPUTE_32F TINYBLAS_COMPUTE_32F
+#define CUBLAS_OP_N TINYBLAS_OP_N
+#define CUBLAS_OP_T TINYBLAS_OP_T
+#define CUDA_R_16F TINYBLAS_R_16F
+#define CUDA_R_32F TINYBLAS_R_32F
+#define CUBLAS_GEMM_DEFAULT TINYBLAS_GEMM_DEFAULT
+#define CUBLAS_GEMM_DEFAULT_TENSOR_OP TINYBLAS_GEMM_DEFAULT
+#define CUBLAS_STATUS_SUCCESS TINYBLAS_STATUS_SUCCESS
+#define cublasGemmAlgo_t tinyblasGemmAlgo_t
+#define cublasOperation_t tinyblasOperation_t
+#define cublasComputeType_t tinyblasComputeType_t
+#define cublasHandle_t tinyblasHandle_t
+#define cublasStatus_t tinyblasStatus_t
+#define cublasSgemm tinyblasSgemm
+#define cublasGemmEx tinyblasGemmEx
+#define cublasCreate tinyblasCreate
+#define cublasDestroy tinyblasDestroy
+#define cublasSetStream tinyblasSetStream
+#define cublasGemmBatchedEx tinyblasGemmBatchedEx
+#define cublasGemmStridedBatchedEx tinyblasGemmStridedBatchedEx
+#define cublasGetStatusString tinyblasGetStatusString
+#define cudaDataType_t tinyblasDataType_t
+#define cublasSetMathMode(handle, mode) CUBLAS_STATUS_SUCCESS
diff --git a/ggml/src/ggml-hip/CMakeLists.txt b/ggml/src/ggml-hip/CMakeLists.txt
index 852de973..6b948461 100644
--- a/ggml/src/ggml-hip/CMakeLists.txt
+++ b/ggml/src/ggml-hip/CMakeLists.txt
@@ -59,9 +59,17 @@ file(GLOB   GGML_HEADERS_ROCM "../ggml-cuda/*.cuh")
 list(APPEND GGML_HEADERS_ROCM "../../include/ggml-cuda.h")
 
 file(GLOB   GGML_SOURCES_ROCM "../ggml-cuda/*.cu")
-file(GLOB   SRCS "../ggml-cuda/template-instances/fattn-mma*.cu")
+
+file(GLOB   SRCS "../ggml-cuda/template-instances/mmq-instance-q*.cu")
 list(APPEND GGML_SOURCES_ROCM ${SRCS})
-file(GLOB   SRCS "../ggml-cuda/template-instances/mmq*.cu")
+
+if (NOT GGML_CUDA_NO_IQUANTS)
+file(GLOB   SRCS "../ggml-cuda/template-instances/mmq-instance-i*.cu")
+list(APPEND GGML_SOURCES_ROCM ${SRCS})
+endif()
+
+if (GGML_CUDA_FA)
+file(GLOB   SRCS "../ggml-cuda/template-instances/fattn-mma*.cu")
 list(APPEND GGML_SOURCES_ROCM ${SRCS})
 
 if (GGML_CUDA_FA_ALL_QUANTS)
@@ -76,6 +84,7 @@ else()
     file(GLOB   SRCS "../ggml-cuda/template-instances/fattn-vec*f16-f16.cu")
     list(APPEND GGML_SOURCES_ROCM ${SRCS})
 endif()
+endif()
 
 ggml_add_backend_library(ggml-hip
                          ${GGML_HEADERS_ROCM}
@@ -129,6 +138,10 @@ if (NOT GGML_CUDA_FA)
     add_compile_definitions(GGML_CUDA_NO_FA)
 endif()
 
+if (GGML_CUDA_NO_IQUANTS)
+    add_compile_definitions(GGML_CUDA_NO_IQUANTS)
+endif()
+
 if (CXX_IS_HIPCC)
     set_source_files_properties(${GGML_SOURCES_ROCM} PROPERTIES LANGUAGE CXX)
     target_link_libraries(ggml-hip PRIVATE hip::device)
