diff --git a/ggml/src/ggml-cuda/CMakeLists.txt b/ggml/src/ggml-cuda/CMakeLists.txt
index 67af1d8cc..6ebb264a7 100644
--- a/ggml/src/ggml-cuda/CMakeLists.txt
+++ b/ggml/src/ggml-cuda/CMakeLists.txt
@@ -44,12 +44,27 @@ if (CUDAToolkit_FOUND)
     list(APPEND GGML_HEADERS_CUDA "../../include/ggml-cuda.h")
 
     file(GLOB   GGML_SOURCES_CUDA "*.cu")
+
+    if (GGML_USE_TINYBLAS)
+        file(GLOB REMOVE_SOURCES_CUDA "mm*f.cu")
+    else()
+        file(GLOB REMOVE_SOURCES_CUDA "mmv.cu")
+    endif()
+    list(REMOVE_ITEM GGML_SOURCES_CUDA ${REMOVE_SOURCES_CUDA})
+
+    file(GLOB   SRCS "template-instances/mmq-instance-q*.cu")
+    list(APPEND GGML_SOURCES_CUDA "${SRCS};template-instances/mmq-instance-mxfp4.cu")
+
+    if (NOT GGML_CUDA_NO_IQUANTS)
+    file(GLOB   SRCS "template-instances/mmq-instance-i*.cu")
+    list(APPEND GGML_SOURCES_CUDA ${SRCS})
+    endif()
+
+    if (GGML_CUDA_FA)
     file(GLOB   SRCS "template-instances/fattn-tile*.cu")
     list(APPEND GGML_SOURCES_CUDA ${SRCS})
     file(GLOB   SRCS "template-instances/fattn-mma*.cu")
     list(APPEND GGML_SOURCES_CUDA ${SRCS})
-    file(GLOB   SRCS "template-instances/mmq*.cu")
-    list(APPEND GGML_SOURCES_CUDA ${SRCS})
     file(GLOB   SRCS "template-instances/mmf*.cu")
     list(APPEND GGML_SOURCES_CUDA ${SRCS})
 
@@ -65,6 +80,9 @@ if (CUDAToolkit_FOUND)
         file(GLOB   SRCS "template-instances/fattn-vec*f16-f16.cu")
         list(APPEND GGML_SOURCES_CUDA ${SRCS})
     endif()
+    endif()
+
+    message(STATUS "GGML_SOURCES_CUDA: ${GGML_SOURCES_CUDA}")
 
     ggml_add_backend_library(ggml-cuda
                              ${GGML_HEADERS_CUDA}
@@ -93,6 +111,10 @@ if (CUDAToolkit_FOUND)
         add_compile_definitions(GGML_CUDA_NO_FA)
     endif()
 
+    if (GGML_CUDA_NO_IQUANTS)
+        add_compile_definitions(GGML_CUDA_NO_IQUANTS)
+    endif()
+
     if (GGML_CUDA_NO_PEER_COPY)
         add_compile_definitions(GGML_CUDA_NO_PEER_COPY)
     endif()
diff --git a/ggml/src/ggml-cuda/common.cuh b/ggml/src/ggml-cuda/common.cuh
index 25e9308d7..e2e23cc92 100644
--- a/ggml/src/ggml-cuda/common.cuh
+++ b/ggml/src/ggml-cuda/common.cuh
@@ -153,7 +153,7 @@ void ggml_cuda_error(const char * stmt, const char * func, const char * file, in
 
 #define CUDA_CHECK(err) CUDA_CHECK_GEN(err, cudaSuccess, cudaGetErrorString)
 
-#if CUDART_VERSION >= 12000 || defined(GGML_USE_MUSA)
+#if CUDART_VERSION >= 12000 || defined(GGML_USE_MUSA) || defined(GGML_USE_TINYBLAS)
     static const char * cublas_get_error_str(const cublasStatus_t err) {
         return cublasGetStatusString(err);
     }
@@ -241,7 +241,7 @@ static const char * cu_get_error_str(CUresult err) {
 #define CP_ASYNC_AVAILABLE
 #endif // !defined(GGML_USE_HIP) && __CUDA_ARCH__ >= GGML_CUDA_CC_AMPERE
 
-#if !defined(GGML_CUDA_NO_FA) && !(defined(GGML_USE_MUSA) && __MUSA_ARCH__ < 220)
+#if !defined(GGML_CUDA_NO_FA) && !(defined(GGML_USE_MUSA) && __MUSA_ARCH__ < 220) && !defined(GGML_CUDA_NO_FA)
 #define FLASH_ATTN_AVAILABLE
 #endif // !defined(GGML_CUDA_NO_FA) && !(defined(GGML_USE_MUSA) && __MUSA_ARCH__ < 220)
 
diff --git a/ggml/src/ggml-cuda/ggml-cuda.cu b/ggml/src/ggml-cuda/ggml-cuda.cu
index 68dc57843..3a89b0dd2 100644
--- a/ggml/src/ggml-cuda/ggml-cuda.cu
+++ b/ggml/src/ggml-cuda/ggml-cuda.cu
@@ -23,9 +23,13 @@
 #include "ggml-cuda/fattn.cuh"
 #include "ggml-cuda/getrows.cuh"
 #include "ggml-cuda/im2col.cuh"
+#ifndef GGML_USE_TINYBLAS
 #include "ggml-cuda/mmf.cuh"
-#include "ggml-cuda/mmq.cuh"
 #include "ggml-cuda/mmvf.cuh"
+#else
+#include "ggml-cuda/mmv.cuh"
+#endif
+#include "ggml-cuda/mmq.cuh"
 #include "ggml-cuda/mmvq.cuh"
 #include "ggml-cuda/norm.cuh"
 #include "ggml-cuda/opt-step-adamw.cuh"
@@ -1247,7 +1251,9 @@ static void ggml_cuda_op_mul_mat_cublas(
         (GGML_CUDA_CC_IS_MTHREADS(cc) && cc >= GGML_CUDA_CC_QY2);
 
     const bool use_fp16 = (src0->type == GGML_TYPE_F16 || ggml_is_quantized(src0->type)) && ggml_is_contiguous(src0) && row_diff == src0->ne[1] && dst->op_params[0] == GGML_PREC_DEFAULT;
-
+#ifdef GGML_USE_TINYBLAS
+    if (0) {
+#else
     if (supports_bf16 && src0->type == GGML_TYPE_BF16 && ggml_is_contiguous(src0) && row_diff == src0->ne[1]) {
         ggml_cuda_pool_alloc<nv_bfloat16> src1_as_bf16(ctx.pool(id));
         if (src1->type != GGML_TYPE_BF16) {
@@ -1276,6 +1282,7 @@ static void ggml_cuda_op_mul_mat_cublas(
 
         const to_fp32_cuda_t to_fp32_cuda = ggml_get_to_fp32_cuda(GGML_TYPE_BF16);
         to_fp32_cuda(dst_bf16.get(), dst_dd_i, row_diff*src1_ncols, stream);
+#endif //GGML_USE_TINYBLAS
     } else if (fast_fp16_hardware_available(cc) && use_fp16) {
         // convert src0 and src1 to fp16, multiply as fp16, convert dst to fp32
         ggml_cuda_pool_alloc<half> src0_as_f16(ctx.pool(id));
@@ -1803,6 +1810,7 @@ struct batched_mul_mat_traits<GGML_TYPE_F32> {
     static inline auto get_nc_converter(ggml_type src_type) { return ggml_get_to_fp32_nc_cuda(src_type); }
 };
 
+#ifndef GGML_USE_TINYBLAS
 template<>
 struct batched_mul_mat_traits<GGML_TYPE_BF16> {
     using cuda_type = nv_bfloat16;
@@ -1815,6 +1823,7 @@ struct batched_mul_mat_traits<GGML_TYPE_BF16> {
     static inline const void* get_beta() { static const float val = beta; return &val; }
     static inline auto get_nc_converter(ggml_type src_type) { return ggml_get_to_bf16_nc_cuda(src_type); }
 };
+#endif
 
 template<>
 struct batched_mul_mat_traits<GGML_TYPE_F16> {
@@ -2004,9 +2013,11 @@ static void ggml_cuda_mul_mat_batched_cublas(ggml_backend_cuda_context & ctx, co
         case GGML_TYPE_F32:
             ggml_cuda_mul_mat_batched_cublas_impl<GGML_TYPE_F32>(ctx, src0, src1, dst);
             break;
+#ifndef GGML_USE_TINYBLAS
         case GGML_TYPE_BF16:
             ggml_cuda_mul_mat_batched_cublas_impl<GGML_TYPE_BF16>(ctx, src0, src1, dst);
             break;
+#endif
         case GGML_TYPE_F16:
             ggml_cuda_mul_mat_batched_cublas_impl<GGML_TYPE_F16>(ctx, src0, src1, dst);
             break;
@@ -2206,16 +2217,20 @@ static void ggml_cuda_mul_mat(ggml_backend_cuda_context & ctx, const ggml_tensor
             const int cc            = ggml_cuda_info().devices[id].cc;
             const int warp_size     = ggml_cuda_info().devices[id].warp_size;
             use_mul_mat_q           = use_mul_mat_q             && ggml_cuda_should_use_mmq(src0->type, cc, src1->ne[1]);
+#ifndef GGML_USE_TINYBLAS
             use_mul_mat_f           = use_mul_mat_f             && ggml_cuda_should_use_mmf(src0->type, cc, warp_size, src0->ne, src0->nb, src1->ne[1], /*mul_mat_id=*/false);
             use_mul_mat_vec_f       = use_mul_mat_vec_f         && ggml_cuda_should_use_mmvf(src0->type, cc, src0->ne, src0->nb, src1->ne[1]);
+#endif
             any_gpus_with_slow_fp16 = any_gpus_with_slow_fp16   || !fast_fp16_hardware_available(cc);
         }
     } else {
         const int cc            = ggml_cuda_info().devices[ctx.device].cc;
         const int warp_size     = ggml_cuda_info().devices[ctx.device].warp_size;
         use_mul_mat_q           = use_mul_mat_q             && ggml_cuda_should_use_mmq(src0->type, cc, src1->ne[1]);
+#ifndef GGML_USE_TINYBLAS
         use_mul_mat_f           = use_mul_mat_f             && ggml_cuda_should_use_mmf(src0->type, cc, warp_size, src0->ne, src0->nb, src1->ne[1], /*mul_mat_id=*/false);
         use_mul_mat_vec_f       = use_mul_mat_vec_f         && ggml_cuda_should_use_mmvf(src0->type, cc, src0->ne, src0->nb, src1->ne[1]);
+#endif
         any_gpus_with_slow_fp16 = any_gpus_with_slow_fp16   || !fast_fp16_hardware_available(cc);
     }
 
@@ -2236,9 +2251,13 @@ static void ggml_cuda_mul_mat(ggml_backend_cuda_context & ctx, const ggml_tensor
     if (!split && use_mul_mat_vec_f) {
         // the custom F16 vector kernel can be used over batched cuBLAS GEMM
         // but this is only faster for GPUs without tensor cores or with a thin src0 matrix (particularly KQV in attention)
+#ifdef GGML_USE_TINYBLAS
+        ggml_cuda_mul_mat_vec(ctx, src0, src1, nullptr, dst);
+#else
         ggml_cuda_mul_mat_vec_f(ctx, src0, src1, nullptr, dst);
     } else if (!split && use_mul_mat_f) {
         ggml_cuda_mul_mat_f(ctx, src0, src1, nullptr, dst);
+#endif
     } else if (!split && use_mul_mat_vec_q) {
         ggml_cuda_mul_mat_vec_q(ctx, src0, src1, nullptr, dst);
     } else if (!split && use_mul_mat_q) {
@@ -2248,7 +2267,11 @@ static void ggml_cuda_mul_mat(ggml_backend_cuda_context & ctx, const ggml_tensor
         // general KQ + KQV multi-batch without FlashAttention
         ggml_cuda_mul_mat_batched_cublas(ctx, src0, src1, dst);
     } else if (use_mul_mat_vec_f) {
+#ifdef GGML_USE_TINYBLAS
+        ggml_cuda_op_mul_mat(ctx, src0, src1, dst, ggml_cuda_op_mul_mat_vec, nullptr);
+#else
         ggml_cuda_op_mul_mat(ctx, src0, src1, dst, ggml_cuda_op_mul_mat_vec_f, nullptr);
+#endif
     } else if (use_mul_mat_vec_q) {
         ggml_cuda_op_mul_mat(ctx, src0, src1, dst, ggml_cuda_op_mul_mat_vec_q, quantize_row_q8_1_cuda);
     } else if (use_mul_mat_q) {
@@ -2276,7 +2299,11 @@ static void ggml_cuda_mul_mat_id(ggml_backend_cuda_context & ctx, ggml_tensor *
             if (ggml_is_quantized(src0->type)) {
                 ggml_cuda_mul_mat_vec_q(ctx, src0, src1, ids, dst);
             } else {
+#ifdef GGML_USE_TINYBLAS
+                ggml_cuda_mul_mat_vec(ctx, src0, src1, ids, dst);
+#else
                 ggml_cuda_mul_mat_vec_f(ctx, src0, src1, ids, dst);
+#endif
             }
             return;
         }
@@ -2687,9 +2714,11 @@ static bool ggml_cuda_compute_forward(ggml_backend_cuda_context & ctx, struct gg
         case GGML_OP_ARGSORT:
             ggml_cuda_op_argsort(ctx, dst);
             break;
+#ifndef GGML_CUDA_NO_FA
         case GGML_OP_FLASH_ATTN_EXT:
             ggml_cuda_flash_attn_ext(ctx, dst);
             break;
+#endif
         case GGML_OP_CROSS_ENTROPY_LOSS:
             ggml_cuda_cross_entropy_loss(ctx, dst);
             break;
diff --git a/ggml/src/ggml-cuda/mmq.cu b/ggml/src/ggml-cuda/mmq.cu
index a2c8760ab..58fc1e7b6 100644
--- a/ggml/src/ggml-cuda/mmq.cu
+++ b/ggml/src/ggml-cuda/mmq.cu
@@ -37,6 +37,7 @@ static void ggml_cuda_mul_mat_q_switch_type(ggml_backend_cuda_context & ctx, con
         case GGML_TYPE_Q6_K:
             mul_mat_q_case<GGML_TYPE_Q6_K>(ctx, args, stream);
             break;
+#ifndef GGML_CUDA_NO_IQUANTS
         case GGML_TYPE_IQ2_XXS:
             mul_mat_q_case<GGML_TYPE_IQ2_XXS>(ctx, args, stream);
             break;
@@ -61,6 +62,7 @@ static void ggml_cuda_mul_mat_q_switch_type(ggml_backend_cuda_context & ctx, con
         case GGML_TYPE_IQ4_NL:
             mul_mat_q_case<GGML_TYPE_IQ4_NL>(ctx, args, stream);
             break;
+#endif // GGML_CUDA_NO_IQUANTS
         default:
             GGML_ABORT("fatal error");
             break;
diff --git a/ggml/src/ggml-cuda/mmq.cuh b/ggml/src/ggml-cuda/mmq.cuh
index 2e133b6bd..dd99bf3af 100644
--- a/ggml/src/ggml-cuda/mmq.cuh
+++ b/ggml/src/ggml-cuda/mmq.cuh
@@ -3735,6 +3735,7 @@ extern DECL_MMQ_CASE(GGML_TYPE_Q3_K);
 extern DECL_MMQ_CASE(GGML_TYPE_Q4_K);
 extern DECL_MMQ_CASE(GGML_TYPE_Q5_K);
 extern DECL_MMQ_CASE(GGML_TYPE_Q6_K);
+#ifndef GGML_CUDA_NO_IQUANTS
 extern DECL_MMQ_CASE(GGML_TYPE_IQ2_XXS);
 extern DECL_MMQ_CASE(GGML_TYPE_IQ2_XS);
 extern DECL_MMQ_CASE(GGML_TYPE_IQ2_S);
@@ -3743,6 +3744,7 @@ extern DECL_MMQ_CASE(GGML_TYPE_IQ3_S);
 extern DECL_MMQ_CASE(GGML_TYPE_IQ1_S);
 extern DECL_MMQ_CASE(GGML_TYPE_IQ4_NL);
 extern DECL_MMQ_CASE(GGML_TYPE_IQ4_XS);
+#endif // GGML_CUDA_NO_IQUANTS
 
 // -------------------------------------------------------------------------------------------------------------------------
 
diff --git a/ggml/src/ggml-cuda/mmv.cu b/ggml/src/ggml-cuda/mmv.cu
new file mode 100644
index 000000000..d8c385e23
--- /dev/null
+++ b/ggml/src/ggml-cuda/mmv.cu
@@ -0,0 +1,336 @@
+#include "ggml.h"
+#include "common.cuh"
+#include "mmv.cuh"
+
+template <typename T, typename type_acc, int block_size>
+static __global__ void mul_mat_vec(
+        const T * __restrict__ x, const float * __restrict__ y, const int32_t * __restrict__ ids, float * __restrict__ dst,
+        const int64_t ncols2, const int64_t nchannels_y, const int64_t stride_row,
+        const int64_t channel_ratio, const int64_t stride_channel_x, const int64_t stride_channel_y, const int64_t stride_channel_dst,
+        const int64_t sample_ratio, const int64_t stride_sample_x, const int64_t stride_sample_y, const int64_t stride_sample_dst) {
+    const int64_t row         = blockIdx.x;
+    const int64_t channel_dst = blockIdx.y;
+    const int64_t channel_x   = ids ? ids[channel_dst]          : channel_dst / channel_ratio;
+    const int64_t channel_y   = ids ? channel_dst % nchannels_y : channel_dst;
+    const int64_t sample_dst  = blockIdx.z;
+    const int64_t sample_x    = sample_dst / sample_ratio;
+    const int64_t sample_y    = sample_dst;
+    const int     tid         = threadIdx.x;
+    constexpr int warp_size   = ggml_cuda_get_physical_warp_size();
+
+    x   += sample_x  *stride_sample_x   + channel_x  *stride_channel_x   + row*stride_row;
+    y   += sample_y  *stride_sample_y   + channel_y  *stride_channel_y;
+    dst += sample_dst*stride_sample_dst + channel_dst*stride_channel_dst;
+
+    const float2 * y2 = (const float2 *) y;
+
+    extern __shared__ char data_mmv[];
+    float * buf_iw = (float *) data_mmv;
+
+    if (block_size > warp_size) {
+        if (tid < warp_size) {
+            buf_iw[tid] = 0.0f;
+        }
+        __syncthreads();
+    }
+
+    float sumf = 0.0f;
+
+    if constexpr (std::is_same<T, float>::value) {
+        const float2 * x2 = (const float2 *) x;
+
+        for (int64_t col2 = tid; col2 < ncols2; col2 += block_size) {
+            const float2 tmpx = x2[col2];
+            const float2 tmpy = y2[col2];
+            sumf += tmpx.x*tmpy.x;
+            sumf += tmpx.y*tmpy.y;
+        }
+    } else if constexpr (std::is_same<T, half>::value) {
+        const half2 * x2 = (const half2 *) x;
+
+        if (std::is_same<type_acc, float>::value) {
+            for (int64_t col2 = tid; col2 < ncols2; col2 += block_size) {
+                const float2 tmpx = __half22float2(x2[col2]);
+                const float2 tmpy = y2[col2];
+                sumf += tmpx.x * tmpy.x;
+                sumf += tmpx.y * tmpy.y;
+            }
+        } else {
+#ifdef FP16_AVAILABLE
+            half2 sumh2 = make_half2(0.0f, 0.0f);
+
+            for (int64_t col2 = tid; col2 < ncols2; col2 += block_size) {
+                const float2 tmp = y2[col2];
+                sumh2 += x2[col2] * make_half2(tmp.x, tmp.y);
+            }
+
+            sumf = __low2float(sumh2) + __high2float(sumh2);
+#else
+            NO_DEVICE_CODE;
+#endif // FP16_AVAILABLE
+        }
+    } else if constexpr (std::is_same<T, nv_bfloat16>::value) {
+        const int * x2 = (const int *) x;
+        for (int64_t col2 = tid; col2 < ncols2; col2 += block_size) {
+            const int    tmpx = x2[col2];
+            const float2 tmpy = y2[col2];
+            sumf += float(reinterpret_cast<const nv_bfloat16 *>(&tmpx)[0]) * tmpy.x;
+            sumf += float(reinterpret_cast<const nv_bfloat16 *>(&tmpx)[1]) * tmpy.y;
+        }
+    } else {
+        static_assert(std::is_same<T, void>::value, "unsupported type");
+    }
+
+    sumf = warp_reduce_sum<warp_size>(sumf);
+
+    if (block_size > warp_size) {
+        buf_iw[tid/warp_size] = sumf;
+        __syncthreads();
+        if (tid >= warp_size) {
+            return;
+        }
+        sumf = buf_iw[tid];
+        sumf = warp_reduce_sum<warp_size>(sumf);
+    }
+
+    if (tid != 0) {
+        return;
+    }
+
+    dst[row] = sumf;
+}
+
+template <typename T, typename type_acc>
+static void launch_mul_mat_vec_cuda(
+        const T * x, const float * y, const int32_t * ids, float * dst,
+        const int64_t ncols, const int64_t nrows, const int64_t stride_row, const int64_t nchannels_x, const int64_t nchannels_y, const int64_t nchannels_dst,
+        const int64_t stride_channel_x, const int64_t stride_channel_y, const int64_t stride_channel_dst, const int64_t nsamples_x,
+        const int64_t nsamples_dst, const int64_t stride_sample_x, const int64_t stride_sample_y, const int64_t stride_sample_dst,
+        cudaStream_t stream) {
+    GGML_ASSERT(ncols      % 2 == 0);
+    GGML_ASSERT(stride_row % 2 == 0);
+    GGML_ASSERT(ids || nchannels_dst % nchannels_x == 0);
+    GGML_ASSERT(       nsamples_dst  % nsamples_x  == 0);
+    const int64_t channel_ratio = nchannels_dst / nchannels_x;
+    const int64_t sample_ratio  = nsamples_dst  / nsamples_x;
+    int device;
+    int warp_size;
+
+    CUDA_CHECK(cudaGetDevice(&device));
+    warp_size = ggml_cuda_info().devices[device].warp_size;
+
+    int64_t block_size_best = warp_size;
+    int64_t niter_best      = (ncols + 2*warp_size - 1) / (2*warp_size);
+    int64_t max_block_size  = 256;
+    if(ggml_cuda_info().devices[device].cc > GGML_CUDA_CC_OFFSET_AMD && ggml_cuda_info().devices[device].cc < GGML_CUDA_CC_RDNA1) {
+        max_block_size = 128;
+    }
+    for (int64_t block_size = 2*warp_size; block_size <= max_block_size; block_size += warp_size) {
+        const int64_t niter = (ncols + 2*block_size - 1) / (2*block_size);
+        if (niter < niter_best) {
+            niter_best      = niter;
+            block_size_best = block_size;
+        }
+    }
+
+    const int smem = warp_size*sizeof(float);
+    const dim3 block_nums(nrows, nchannels_dst, nsamples_dst);
+    const dim3 block_dims(block_size_best, 1, 1);
+    switch (block_size_best) {
+        case   32: {
+            mul_mat_vec<T, type_acc,  32><<<block_nums, block_dims, smem, stream>>>
+                (x, y, ids, dst, ncols/2, nchannels_y, stride_row, channel_ratio, stride_channel_x, stride_channel_y,
+                 stride_channel_dst, sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst);
+        } break;
+        case   64: {
+            mul_mat_vec<T, type_acc,  64><<<block_nums, block_dims, smem, stream>>>
+                (x, y, ids, dst, ncols/2, nchannels_y, stride_row, channel_ratio, stride_channel_x, stride_channel_y,
+                 stride_channel_dst, sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst);
+        } break;
+        case   96: {
+            mul_mat_vec<T, type_acc,  96><<<block_nums, block_dims, smem, stream>>>
+                (x, y, ids, dst, ncols/2, nchannels_y, stride_row, channel_ratio, stride_channel_x, stride_channel_y,
+                 stride_channel_dst, sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst);
+        } break;
+        case  128: {
+            mul_mat_vec<T, type_acc, 128><<<block_nums, block_dims, smem, stream>>>
+                (x, y, ids, dst, ncols/2, nchannels_y, stride_row, channel_ratio, stride_channel_x, stride_channel_y,
+                 stride_channel_dst, sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst);
+        } break;
+        case  160: {
+            mul_mat_vec<T, type_acc, 160><<<block_nums, block_dims, smem, stream>>>
+                (x, y, ids, dst, ncols/2, nchannels_y, stride_row, channel_ratio, stride_channel_x, stride_channel_y,
+                 stride_channel_dst, sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst);
+        } break;
+        case  192: {
+            mul_mat_vec<T, type_acc, 192><<<block_nums, block_dims, smem, stream>>>
+                (x, y, ids, dst, ncols/2, nchannels_y, stride_row, channel_ratio, stride_channel_x, stride_channel_y,
+                 stride_channel_dst, sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst);
+        } break;
+        case  224: {
+            mul_mat_vec<T, type_acc, 224><<<block_nums, block_dims, smem, stream>>>
+                (x, y, ids, dst, ncols/2, nchannels_y, stride_row, channel_ratio, stride_channel_x, stride_channel_y,
+                 stride_channel_dst, sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst);
+        } break;
+        case  256: {
+            mul_mat_vec<T, type_acc, 256><<<block_nums, block_dims, smem, stream>>>
+                (x, y, ids, dst, ncols/2, nchannels_y, stride_row, channel_ratio, stride_channel_x, stride_channel_y,
+                 stride_channel_dst, sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst);
+        } break;
+        default: {
+            GGML_ABORT("fatal error");
+        } break;
+    }
+}
+
+template<typename T>
+static void mul_mat_vec_cuda(
+        const T * x, const float * y, const int32_t * ids, float * dst,
+        const int64_t ncols, const int64_t nrows, const int64_t stride_row, const int64_t nchannels_x, const int64_t nchannels_y, const int64_t nchannels_dst,
+        const int64_t stride_channel_x, const int64_t stride_channel_y, const int64_t stride_channel_dst, const int64_t nsamples_x,
+        const int64_t nsamples_dst, const int64_t stride_sample_x, const int64_t stride_sample_y, const int64_t stride_sample_dst,
+        enum ggml_prec prec, cudaStream_t stream) {
+    if constexpr(std::is_same<T, half>::value) {
+        if (prec == GGML_PREC_DEFAULT) {
+            launch_mul_mat_vec_cuda<T, half>
+                (x, y, ids, dst, ncols, nrows, stride_row, nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,
+                 stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+            return;
+        }
+    }
+    launch_mul_mat_vec_cuda<T, float>
+        (x, y, ids, dst, ncols, nrows, stride_row, nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,
+         stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
+}
+
+void ggml_cuda_mul_mat_vec(ggml_backend_cuda_context & ctx, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * ids, ggml_tensor * dst) {
+    GGML_ASSERT(        src1->type == GGML_TYPE_F32);
+    GGML_ASSERT(!ids ||  ids->type == GGML_TYPE_I32);
+    GGML_ASSERT(         dst->type == GGML_TYPE_F32);
+
+    GGML_TENSOR_BINARY_OP_LOCALS;
+
+    const size_t ts_src0 = ggml_type_size(src0->type);
+    const size_t ts_src1 = ggml_type_size(src1->type);
+    const size_t ts_dst  = ggml_type_size(dst->type);
+
+    GGML_ASSERT(!ids || ne12 == 1); // Implementation is only correct for  batch size 1.
+    GGML_ASSERT(ne13 == ne3);
+
+    GGML_ASSERT(        nb00       == ts_src0);
+    GGML_ASSERT(        nb10       == ts_src1);
+    GGML_ASSERT(!ids || ids->nb[0] == ggml_type_size(ids->type));
+    GGML_ASSERT(        nb0        == ts_dst);
+
+    const int cc = ggml_cuda_info().devices[ggml_cuda_get_device()].cc;
+    const enum ggml_prec prec = fast_fp16_available(cc) ? ggml_prec(dst->op_params[0]) : GGML_PREC_F32;
+
+    const float   * src1_d =       (const float   *) src1->data;
+    const int32_t *  ids_d = ids ? (const int32_t *)  ids->data : nullptr;
+    float         *  dst_d =       (float         *)  dst->data;
+
+    const int64_t s01 = src0->nb[1] / ts_src0;
+    const int64_t s11 = src1->nb[1] / ts_src1;
+    const int64_t s1  =  dst->nb[1] / ts_dst;
+    const int64_t s02 = src0->nb[2] / ts_src0;
+    const int64_t s12 = src1->nb[2] / ts_src1;
+    const int64_t s2  =  dst->nb[2] / ts_dst;
+    const int64_t s03 = src0->nb[3] / ts_src0;
+    const int64_t s13 = src1->nb[3] / ts_src1;
+    const int64_t s3  =  dst->nb[3] / ts_dst;
+
+    // For MUL_MAT_ID the memory layout is different than for MUL_MAT:
+    const int64_t ncols_dst          = ids ? ne2  : ne1;
+    const int64_t nchannels_y        = ids ? ne11 : ne12;
+    const int64_t nchannels_dst      = ids ? ne1  : ne2;
+    const int64_t stride_channel_dst = ids ? s1   : s2;
+    const int64_t stride_channel_y   = ids ? s11  : s12;
+
+    GGML_ASSERT(ncols_dst == 1);
+
+    switch (src0->type) {
+        case GGML_TYPE_F32: {
+            const float * src0_d = (const float *) src0->data;
+            mul_mat_vec_cuda(src0_d, src1_d, ids_d, dst_d, ne00, ne01, s01,
+                ne02, nchannels_y, nchannels_dst, s02, stride_channel_y, stride_channel_dst,
+                ne03,              ne3,           s03, s13,              s3,                 prec, ctx.stream());
+        } break;
+        case GGML_TYPE_F16: {
+            const half * src0_d = (const half *) src0->data;
+            mul_mat_vec_cuda(src0_d, src1_d, ids_d, dst_d, ne00, ne01, s01,
+                ne02, nchannels_y, nchannels_dst, s02, stride_channel_y, stride_channel_dst,
+                ne03,              ne3,           s03, s13,              s3,                 prec, ctx.stream());
+        } break;
+        case GGML_TYPE_BF16: {
+            const nv_bfloat16 * src0_d = (const nv_bfloat16 *) src0->data;
+            mul_mat_vec_cuda(src0_d, src1_d, ids_d, dst_d, ne00, ne01, s01,
+                ne02, nchannels_y, nchannels_dst, s02, stride_channel_y, stride_channel_dst,
+                ne03,              ne3,           s03, s13,              s3,                 prec, ctx.stream());
+        } break;
+        default:
+            GGML_ABORT("unsupported type: %s", ggml_type_name(src0->type));
+    }
+}
+
+void ggml_cuda_op_mul_mat_vec(
+    ggml_backend_cuda_context & ctx,
+    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const char * src0_dd_i, const float * src1_ddf_i,
+    const char * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,
+    const int64_t src1_padded_row_size, cudaStream_t stream) {
+
+    GGML_ASSERT(src1->type == GGML_TYPE_F32);
+    GGML_ASSERT(dst->type  == GGML_TYPE_F32);
+
+    const int64_t ne00 = src0->ne[0];
+    const int64_t row_diff = row_high - row_low;
+
+    GGML_ASSERT(src1_ncols == 1);
+
+    const int cc = ggml_cuda_info().devices[ggml_cuda_get_device()].cc;
+    const enum ggml_prec prec = fast_fp16_available(cc) ? ggml_prec(dst->op_params[0]) : GGML_PREC_F32;
+
+
+    // ggml_cuda_op provides single, contiguous matrices
+    const int64_t stride_row         = ne00;
+    const int64_t nchannels_x        = 1;
+    const int64_t nchannels_y        = 1;
+    const int64_t nchannels_dst      = 1;
+    const int64_t stride_channel_x   = 0;
+    const int64_t stride_channel_y   = 0;
+    const int64_t stride_channel_dst = 0;
+    const int64_t nsamples_x         = 1;
+    const int64_t nsamples_dst       = 1;
+    const int64_t stride_sample_x    = 0;
+    const int64_t stride_sample_y    = 0;
+    const int64_t stride_sample_dst  = 0;
+
+    switch (src0->type) {
+        case GGML_TYPE_F32: {
+            const float * src0_d = (const float *) src0_dd_i;
+            mul_mat_vec_cuda(src0_d, src1_ddf_i, nullptr, dst_dd_i, ne00, row_diff, stride_row,
+                nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
+                nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, prec, stream);
+        } break;
+        case GGML_TYPE_F16: {
+            const half * src0_d = (const half *) src0_dd_i;
+            mul_mat_vec_cuda(src0_d, src1_ddf_i, nullptr, dst_dd_i, ne00, row_diff, stride_row,
+                nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
+                nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, prec, stream);
+        } break;
+        case GGML_TYPE_BF16: {
+            const nv_bfloat16 * src0_d = (const nv_bfloat16 *) src0_dd_i;
+            mul_mat_vec_cuda(src0_d, src1_ddf_i, nullptr, dst_dd_i, ne00, row_diff, stride_row,
+                nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
+                nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, prec, stream);
+        } break;
+        default:
+            GGML_ABORT("unsupported type: %s", ggml_type_name(src0->type));
+    }
+
+    GGML_UNUSED(ctx);
+    GGML_UNUSED(src1);
+    GGML_UNUSED(dst);
+    GGML_UNUSED(src1_ddq_i);
+    GGML_UNUSED(src1_ncols);
+    GGML_UNUSED(src1_padded_row_size);
+}
diff --git a/ggml/src/ggml-cuda/mmv.cuh b/ggml/src/ggml-cuda/mmv.cuh
new file mode 100644
index 000000000..756e7e1cc
--- /dev/null
+++ b/ggml/src/ggml-cuda/mmv.cuh
@@ -0,0 +1,12 @@
+#include "common.cuh"
+
+// maximum number of src0 rows with which to use mul_mat_vec over cuBLAS if FP16 tensor cores are available
+#define MMV_MAX_ROWS 512
+
+void ggml_cuda_mul_mat_vec(ggml_backend_cuda_context & ctx, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * ids, ggml_tensor * dst);
+
+void ggml_cuda_op_mul_mat_vec(
+    ggml_backend_cuda_context & ctx,
+    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const char * src0_dd_i, const float * src1_ddf_i,
+    const char * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,
+    const int64_t src1_padded_row_size, cudaStream_t stream);
diff --git a/ggml/src/ggml-cuda/mmvq.cu b/ggml/src/ggml-cuda/mmvq.cu
index d671551c1..72ce3f880 100644
--- a/ggml/src/ggml-cuda/mmvq.cu
+++ b/ggml/src/ggml-cuda/mmvq.cu
@@ -542,6 +542,7 @@ static void mul_mat_vec_q_switch_type(
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
                  nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
             break;
+#ifndef GGML_CUDA_NO_IQUANTS
         case GGML_TYPE_IQ2_XXS:
             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ2_XXS>
                 (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,
@@ -596,6 +597,7 @@ static void mul_mat_vec_q_switch_type(
                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,
                  nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);
             break;
+#endif // GGML_CUDA_NO_IQUANTS
         default:
             GGML_ABORT("fatal error");
             break;
diff --git a/ggml/src/ggml-cuda/tinyblas.cu b/ggml/src/ggml-cuda/tinyblas.cu
new file mode 100644
index 000000000..1d4da0844
--- /dev/null
+++ b/ggml/src/ggml-cuda/tinyblas.cu
@@ -0,0 +1,981 @@
+// -*- mode:c++;indent-tabs-mode:nil;c-basic-offset:4;coding:utf-8 -*-
+// vi: set et ft=cpp ts=4 sts=4 sw=4 fenc=utf-8 :vi
+//
+// Copyright 2024 Mozilla Foundation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#include "tinyblas.h"
+
+//
+//
+//                                ██████╗ ██╗   █████╗ ██████╗
+//         ██████╗██╗██╗ ██╗██═██╗██╔══██╗██║  ██╔══██╗██╔═══╝
+//         ╚═██╔═╝██║███▄██║██ ██║██████╔╝██║  ███████║██████╗
+//           ██║  ██║██▀███║╚███╔╝██╔══██╗██║  ██╔══██║╔═══██║
+//           ██║  ██║██║ ██║ ███║ ██████╔╝████╗██║  ██║██████║
+//           ╚═╝  ╚═╝╚═╝ ╚═╝ ╚══╝ ╚═════╝ ╚═══╝╚═╝  ╚═╝╚═════╝
+//
+//                   BASIC LINEAR ALGEBRA SUBPROGRAMS
+//
+//
+// In this file you'll find GPU subroutines implementing general matrix
+// multiplication, that are API compatible with NVIDIA's cuBLAS library
+// and implement similar tricks[1] for performance.
+//
+// [1] S. Boehm, ‘How to Optimize a CUDA Matmul Kernel for cuBLAS-like
+//     Performance’, 2022. [Online]. Available:
+//     https://siboehm.com/articles/22/CUDA-MMM. [Accessed:
+//     05-Mar-2024].
+
+#include <algorithm>
+#include <cstdlib>
+#include <type_traits>
+
+#ifndef __HIP__
+#include <cuda_fp16.h>
+#include <cuda_runtime.h>
+#define __shfl_down(var, srcLane, warpSize) __shfl_down_sync(-1u, var, srcLane, warpSize)
+#else
+#include <hip/hip_fp16.h>
+#include <hip/hip_runtime.h>
+#define cudaSuccess hipSuccess
+#define cudaStream_t hipStream_t
+#define cudaGetLastError hipGetLastError
+#endif
+
+#define WARPSIZE 32
+#define THREAD_COUNT ((BM * BN) / (TM * TN))
+#define KERNEL __launch_bounds__(THREAD_COUNT)
+#define CEIL_DIV(M, N) (((M) + (N) - 1) / (N))
+
+#define IGNORE_BETA 1
+#define IGNORE_ALPHA 2
+#define ASSUME_A_OP_N 4
+#define ASSUME_B_OP_T 8
+#define ASSUME_M_SAFE 16
+#define ASSUME_N_SAFE 32
+#define ASSUME_K_SAFE 64
+#define ASSUME_A_OP_T 128
+#define ASSUME_B_OP_N 256
+
+struct tinyblasContext {
+    cudaStream_t stream;
+};
+
+inline bool isone(float x) {
+    return x == 1;
+}
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+// tinyBLAS specialized matrix vector product kernel
+
+__forceinline__ __device__ float warpSum(float x) {
+    for (int i = WARPSIZE / 2; i > 0; i /= 2)
+        x += __shfl_down(x, i, WARPSIZE);
+    return x;
+}
+
+template <typename WORD, typename SRC>
+__device__ __forceinline__ void madd(WORD *tally, WORD *kahan, SRC a, SRC b) {
+    WORD x = a;
+    WORD y = b;
+    WORD z = x * y - *kahan;
+    WORD t = *tally + z;
+    *kahan = (t - *tally) - z;
+    *tally = t;
+}
+
+template <typename WORD, typename SRC, typename DST>
+static __device__ void matvec(int m, int k, const SRC *A, int lda, const SRC *B, DST *C) {
+    WORD Ct[WARPSIZE] = {0};
+    WORD Ce[WARPSIZE] = {0};
+    int i = blockIdx.y * WARPSIZE;
+    for (int l = threadIdx.x; l < k; l += WARPSIZE)
+        for (int j = 0; j < WARPSIZE; ++j)
+            madd(&Ct[j], &Ce[j], A[lda * (i + j) + l], B[l]);
+    for (int j = 0; j < WARPSIZE; ++j) {
+        WORD c = warpSum(Ct[j]);
+        if (!threadIdx.x)
+            C[i + j] = c;
+    }
+}
+
+template <typename WORD, typename SRC, typename DST>
+static __global__ __launch_bounds__(WARPSIZE) void matvec_entry(int m, int k, const SRC *A, int lda,
+                                                                const SRC *B, DST *C) {
+    matvec<WORD>(m, k, A, lda, B, C);
+}
+
+template <typename WORD, typename SRC, typename DST>
+static tinyblasStatus_t matvec_launch(tinyblasHandle_t handle, int m, int k, const SRC *A, int lda,
+                                      const SRC *B, DST *C) {
+    dim3 blocks(WARPSIZE, m / WARPSIZE);
+    matvec_entry<WORD><<<blocks, WARPSIZE, 0, handle->stream>>>(m, k, A, lda, B, C);
+    if (cudaGetLastError() != cudaSuccess)
+        return TINYBLAS_STATUS_EXECUTION_FAILED;
+    return TINYBLAS_STATUS_SUCCESS;
+}
+
+template <typename WORD>
+static bool can_use_matvec(tinyblasOperation_t aT, tinyblasOperation_t bT, int m, int n, int k,
+                           WORD alpha, WORD beta) {
+    return n == 1 && k >= 4096 && aT && !bT && //
+           !(m % WARPSIZE) && !(k % WARPSIZE) && //
+           isone(alpha) && !beta;
+}
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+// tinyBLAS block tiling outer product GEMM kernel
+
+template <int CONFIG, int BM, int BN, int TM, int TN, typename WORD, typename SRC, typename DST>
+static __device__ void matmul_block2d(tinyblasOperation_t transa, tinyblasOperation_t transb, int m,
+                                      int n, int k, WORD alpha, const SRC *A, int lda, const SRC *B,
+                                      int ldb, WORD beta, DST *C, int ldc) {
+
+    constexpr int BK = THREAD_COUNT;
+    static_assert(BM % TM == 0, "can't divide work for threads");
+    static_assert(BN % TN == 0, "can't divide work for threads");
+    static_assert(BM > 0 && BN > 0 && BK > 0 && TM > 0 && TN > 0,
+                  "one of the constexpr configuration values was non-positive");
+    static_assert((BK * BM * sizeof(SRC)) + (BK * BN * sizeof(SRC)) <= 65536,
+                  "you're almost almost certainly using too much shared memory");
+
+    constexpr bool msafe = !!(CONFIG & ASSUME_M_SAFE);
+    constexpr bool nsafe = !!(CONFIG & ASSUME_N_SAFE);
+    constexpr bool ksafe = !!(CONFIG & ASSUME_K_SAFE);
+
+    const int th = threadIdx.x;
+    const int ii = blockIdx.x * BM;
+    const int jj = blockIdx.y * BN;
+    const int ti = th / (BN / TN) * TM;
+    const int tj = th % (BN / TN) * TN;
+
+    __shared__ SRC As[BK * BM];
+    __shared__ SRC Bs[BK * BN];
+
+    WORD At[TM];
+    WORD Bt[TN];
+    WORD Ct[TM * TN] = {0};
+
+    if (CONFIG & ASSUME_A_OP_T)
+        transa = TINYBLAS_OP_T;
+    if (CONFIG & ASSUME_A_OP_N)
+        transa = TINYBLAS_OP_N;
+    if (CONFIG & ASSUME_B_OP_N)
+        transb = TINYBLAS_OP_N;
+    if (CONFIG & ASSUME_B_OP_T)
+        transb = TINYBLAS_OP_T;
+
+    for (int ll = 0; ll < k; ll += BK) {
+
+        if (!ksafe || !msafe)
+            for (int i = 0; i < BM; ++i)
+                As[BM * th + i] = 0;
+        for (int i = 0; i < BM && (ll + th < k || ksafe) && (ii + i < m || msafe); ++i)
+            As[BM * th + i] = A[transa ? lda * (ii + i) + (ll + th) : lda * (ll + th) + (ii + i)];
+
+        if (!ksafe || !nsafe)
+            for (int j = 0; j < BN; ++j)
+                Bs[BN * th + j] = 0;
+        for (int j = 0; j < BN && (ll + th < k || ksafe) && (jj + j < n || nsafe); ++j)
+            Bs[BN * th + j] = B[transb ? ldb * (ll + th) + (jj + j) : ldb * (jj + j) + (ll + th)];
+
+        __syncthreads();
+
+        for (int l = 0; l < BK; ++l) {
+            for (int j = 0; j < TM; ++j)
+                At[j] = As[BM * l + ti + j];
+            for (int h = 0; h < TN; ++h)
+                Bt[h] = Bs[BN * l + tj + h];
+            for (int j = 0; j < TM; ++j)
+                for (int h = 0; h < TN; ++h)
+                    Ct[TN * j + h] += At[j] * Bt[h];
+        }
+
+        __syncthreads();
+    }
+
+    for (int j = 0; j < TN && (jj + tj + j < n || nsafe); ++j)
+        for (int i = 0; i < TM && (ii + ti + i < m || msafe); ++i) {
+            WORD r, d = Ct[TN * i + j];
+            if ((CONFIG & IGNORE_BETA) || !beta) {
+                if (CONFIG & IGNORE_ALPHA)
+                    r = d;
+                else
+                    r = alpha * d;
+            } else {
+                WORD c = C[ldc * (jj + tj + j) + (ii + ti + i)];
+                if (CONFIG & IGNORE_ALPHA)
+                    r = beta * c + d;
+                else
+                    r = alpha * d + beta * c;
+            }
+            C[ldc * (jj + tj + j) + (ii + ti + i)] = r;
+        }
+}
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+// tinyBLAS warp block tiling outer product GEMM kernel
+
+template <int CONFIG, int BM, int BN, int BK, int VE, int WM, int WN, int WNI, int TM, int TN,
+          int TT, typename WORD, typename SRC, typename DST>
+static __device__ void matmul_warp2d(tinyblasOperation_t aT, //
+                                     tinyblasOperation_t bT, //
+                                     int m, int n, int k, WORD alpha, //
+                                     const SRC *A, int lda, //
+                                     const SRC *B, int ldb, WORD beta, //
+                                     DST *C, int ldc) {
+
+    const SRC zero = 0;
+    const int warpIdx = threadIdx.x / WARPSIZE;
+    const int warpCol = warpIdx % (BN / WN);
+    const int warpRow = warpIdx / (BN / WN);
+
+    constexpr int WARPS = TT / WARPSIZE;
+    constexpr int WMI = (WM * WN) / (WARPSIZE * TM * TN * WNI);
+    constexpr int WSUBM = WM / WMI;
+    constexpr int WSUBN = WN / WNI;
+
+    constexpr bool msafe = !!(CONFIG & ASSUME_M_SAFE);
+    constexpr bool nsafe = !!(CONFIG & ASSUME_N_SAFE);
+    constexpr bool ksafe = !!(CONFIG & ASSUME_K_SAFE);
+
+    const int threadIdxInWarp = threadIdx.x % WARPSIZE;
+    const int threadColInWarp = threadIdxInWarp % (WSUBN / TN);
+    const int threadRowInWarp = threadIdxInWarp / (WSUBN / TN);
+
+    // want to tune these magic numbers?
+    // use llamafile/pick_a_warp_kernel.c
+    static_assert(!(BN % WN) && !(BM % WM), "");
+    static_assert(!(WM % WMI) && !(WN % WNI), "");
+    static_assert((BN / WN) * (BM / WM) == WARPS, "");
+    static_assert((WM * WN) % (WARPSIZE * TM * TN * WNI) == 0, "");
+    static_assert((BM * BK) % (VE * TT) == 0, "");
+    static_assert((BN * BK) % (VE * TT) == 0, "");
+    static_assert(BK % VE == 0, "");
+    static_assert(BN % VE == 0, "");
+
+    __shared__ SRC As[BK * BM];
+    __shared__ SRC Bs[BK * BN];
+
+    WORD Ar[WMI * TM] = {0};
+    WORD Br[WNI * TN] = {0};
+    WORD Ct[WMI * TM * WNI * TN] = {0};
+
+    if (CONFIG & ASSUME_A_OP_T)
+        aT = TINYBLAS_OP_T;
+    if (CONFIG & ASSUME_A_OP_N)
+        aT = TINYBLAS_OP_N;
+    if (CONFIG & ASSUME_B_OP_N)
+        bT = TINYBLAS_OP_N;
+    if (CONFIG & ASSUME_B_OP_T)
+        bT = TINYBLAS_OP_T;
+
+    for (int ll = 0; ll < k; ll += BK) {
+
+        for (int h = 0; h < BM; h += (TT * VE) / BK)
+            for (int v = 0; v < VE; ++v) {
+                int l = ll + threadIdx.x % (BK / VE) * VE + v;
+                int i = blockIdx.y * BM + threadIdx.x / (BK / VE) + h;
+                As[BM * (threadIdx.x % (BK / VE) * VE + v) + (threadIdx.x / (BK / VE) + h)] =
+                    (((i < m || msafe) && //
+                      (l < k || ksafe))
+                         ? A[aT ? lda * l + i : lda * i + l]
+                         : zero);
+            }
+
+        for (int h = 0; h < BK; h += TT / (BN / VE))
+            for (int v = 0; v < VE; ++v) {
+                int l = ll + threadIdx.x / (BN / VE) + h;
+                int j = blockIdx.x * BN + threadIdx.x % (BN / VE) * VE + v;
+                Bs[BN * (threadIdx.x / (BN / VE) + h) + (threadIdx.x % (BN / VE) * VE + v)] =
+                    (((j < n || nsafe) && //
+                      (l < k || ksafe))
+                         ? B[bT ? ldb * j + l : ldb * l + j]
+                         : zero);
+            }
+
+        __syncthreads();
+
+        for (int l = 0; l < BK; ++l) {
+            for (int ii = 0; ii < WMI; ++ii)
+                for (int i = 0; i < TM; ++i)
+                    Ar[TM * ii + i] =
+                        As[BM * l + WM * warpRow + WSUBM * ii + TM * threadRowInWarp + i];
+            for (int jj = 0; jj < WNI; ++jj)
+                for (int j = 0; j < TN; ++j)
+                    Br[TN * jj + j] =
+                        Bs[BN * l + WN * warpCol + WSUBN * jj + TN * threadColInWarp + j];
+            for (int ii = 0; ii < WMI; ++ii)
+                for (int jj = 0; jj < WNI; ++jj)
+                    for (int i = 0; i < TM; ++i)
+                        for (int j = 0; j < TN; ++j)
+                            Ct[(WNI * TN) * (TM * ii + i) + (TN * jj) + j] +=
+                                Ar[TM * ii + i] * Br[TN * jj + j];
+        }
+
+        __syncthreads();
+    }
+
+    for (int ii = 0; ii < WMI; ++ii)
+        for (int jj = 0; jj < WNI; ++jj)
+            for (int i = 0; i < TM; i += 1)
+                for (int j = 0; j < TN; j += 1) {
+                    int row = (BM * blockIdx.y + WM * warpRow) + (WSUBM * ii) +
+                              (threadRowInWarp * TM + i);
+                    int col = (BN * blockIdx.x + WN * warpCol) + (WSUBN * jj) +
+                              (threadColInWarp * TN + j);
+                    if ((row < m || msafe) && (col < n || nsafe)) {
+                        WORD r, d = Ct[(WNI * TN) * (TM * ii + i) + (TN * jj + j)];
+                        if ((CONFIG & IGNORE_BETA) || !beta) {
+                            if (CONFIG & IGNORE_ALPHA)
+                                r = d;
+                            else
+                                r = alpha * d;
+                        } else {
+                            WORD c = C[ldc * row + col];
+                            if (CONFIG & IGNORE_ALPHA)
+                                r = beta * c + d;
+                            else
+                                r = alpha * d + beta * c;
+                        }
+                        C[ldc * row + col] = r;
+                    }
+                }
+}
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+// tinyBLAS canonical cuBLAS-like interface
+
+/**
+ * Creates new tinyBLAS handle.
+ *
+ * Before calling tinyBLAS GEMM functions a handle must first be
+ * created, using this function. It should be freed later, using
+ * tinyblasDestroy(). After a handle is created the caller needs
+ * tinyblasSetStream() to specify the CUDA stream.
+ *
+ * @param out_handle receives pointer to newly created handle
+ * @return TINYBLAS_STATUS_SUCCESS on success otherwise error
+ */
+tinyblasStatus_t tinyblasCreate(tinyblasHandle_t *out_handle) {
+    tinyblasHandle_t handle;
+    if ((handle = (tinyblasHandle_t)malloc(sizeof(struct tinyblasContext)))) {
+        *out_handle = handle;
+        return TINYBLAS_STATUS_SUCCESS;
+    } else {
+        return TINYBLAS_STATUS_ALLOC_FAILED;
+    }
+}
+
+/**
+ * Destroys tinyBLAS handle.
+ *
+ * @param handle is pointer to handle created by tinyblasCreate()
+ * @return TINYBLAS_STATUS_SUCCESS on success otherwise error
+ */
+tinyblasStatus_t tinyblasDestroy(tinyblasHandle_t handle) {
+    free(handle);
+    return TINYBLAS_STATUS_SUCCESS;
+}
+
+/**
+ * Associates CUDA handle with tinyBLAS handle.
+ *
+ * The provided stream will be used when tinyBLAS launches kernels.
+ *
+ * @param handle is pointer to handle created by tinyblasCreate()
+ * @param stream is pointer to stream created by cudaStreamCreate()
+ * @return TINYBLAS_STATUS_SUCCESS on success otherwise error
+ */
+tinyblasStatus_t tinyblasSetStream(tinyblasHandle_t handle, void *stream) {
+    handle->stream = (cudaStream_t)stream;
+    return TINYBLAS_STATUS_SUCCESS;
+}
+
+/**
+ * Gets CUDA stream associated with tinyBLAS handle.
+ *
+ * @param handle is pointer to handle created by tinyblasCreate()
+ * @param out_stream receives pointer to any cudaStream_t object
+ * @return TINYBLAS_STATUS_SUCCESS on success otherwise error
+ */
+tinyblasStatus_t tinyblasGetStream(tinyblasHandle_t handle, void **out_stream) {
+    *out_stream = handle->stream;
+    return TINYBLAS_STATUS_SUCCESS;
+}
+
+/**
+ * Returns string describing tinyBLAS status code.
+ */
+const char *tinyblasGetStatusString(tinyblasStatus_t err) {
+    switch (err) {
+    case TINYBLAS_STATUS_SUCCESS:
+        return "Success";
+    case TINYBLAS_STATUS_ALLOC_FAILED:
+        return "Alloc failed";
+    case TINYBLAS_STATUS_INVALID_VALUE:
+        return "Invalid value";
+    case TINYBLAS_STATUS_NOT_SUPPORTED:
+        return "Not supported";
+    case TINYBLAS_STATUS_EXECUTION_FAILED:
+        return "Execution failed";
+    case TINYBLAS_STATUS_DIMENSION_OVERLAP:
+        return "Dimension overlap";
+    case TINYBLAS_STATUS_DIMENSION_OVERFLOW:
+        return "Dimension overflow";
+    default:
+        return "Unknown error";
+    }
+}
+
+/**
+ * Performs single-precision general matrix multiplication.
+ *
+ * This is a column major GEMM subroutine for computing C = α*A*B + β*C.
+ *
+ * @param handle was created by tinyblasCreate()
+ * @param transa if `A` should be transposed
+ * @param transb if `B` should be transposed
+ * @param m is rows in `A` and `C`
+ * @param n is cols in `B` and `C`
+ * @param k is cols in `A` and rows in `B`
+ * @param alpha points to scalar that's multiplied against input
+ * @param A is input array of first matrix
+ * @param lda is row stride of `A`
+ * @param B is input array of second matrix
+ * @param ldb is row stride of `B`
+ * @param beta points to scalar that's multiplied against the existing
+ *     output, but this multiplication only happens if beta is nonzero
+ * @param C is input/output array of output matrix
+ * @param ldc is row stride of `C`
+ */
+tinyblasStatus_t tinyblasSgemm(tinyblasHandle_t handle, tinyblasOperation_t transa,
+                               tinyblasOperation_t transb, int m, int n, int k, const float *alpha,
+                               const float *A, int lda, const float *B, int ldb, const float *beta,
+                               float *C, int ldc) {
+    return tinyblasGemmEx(handle, transa, transb, m, n, k, alpha, A, TINYBLAS_R_32F, lda, B,
+                          TINYBLAS_R_32F, ldb, beta, C, TINYBLAS_R_32F, ldc, TINYBLAS_COMPUTE_32F,
+                          TINYBLAS_GEMM_DEFAULT);
+}
+
+template <int CONFIG, int BM, int BN, int BK, int VE, int WM, int WN, int WNI, int TM, int TN,
+          int TT, typename WORD, typename SRC, typename DST>
+static __global__ void __launch_bounds__(TT) tinyblasGE_entry(tinyblasOperation_t aT, //
+                                                              tinyblasOperation_t bT, //
+                                                              int m, int n, int k, WORD alpha, //
+                                                              const SRC *A, int lda, //
+                                                              const SRC *B, int ldb, //
+                                                              WORD beta, DST *C, int ldc) {
+    matmul_warp2d<CONFIG, BM, BN, BK, VE, WM, WN, WNI, TM, TN, TT>(aT, bT, m, n, k, alpha, A, lda,
+                                                                   B, ldb, beta, C, ldc);
+}
+
+template <int BM, int BN, int BK, int VE, int WM, int WN, int WNI, int TM, int TN, int TT,
+          typename WORD, typename SRC, typename DST>
+static tinyblasStatus_t tinyblasGE_launcher(tinyblasHandle_t handle, tinyblasOperation_t aT,
+                                            tinyblasOperation_t bT, int m, int n, int k, WORD alpha,
+                                            const SRC *A, int lda, const SRC *B, int ldb, WORD beta,
+                                            DST *C, int ldc) {
+    dim3 blocks(CEIL_DIV(n, BN), CEIL_DIV(m, BM));
+    if ((!beta && //
+         isone(alpha) && //
+         n % BN == 0 && //
+         k % BK == 0 && //
+         aT == TINYBLAS_OP_N && //
+         bT == TINYBLAS_OP_T)) {
+        constexpr int CONFIG = IGNORE_BETA | IGNORE_ALPHA | ASSUME_A_OP_N | ASSUME_B_OP_T |
+                               ASSUME_N_SAFE | ASSUME_K_SAFE;
+        tinyblasGE_entry<CONFIG, BM, BN, BK, VE, WM, WN, WNI, TM, TN, TT>
+            <<<blocks, TT, 0, handle->stream>>>(aT, bT, m, n, k, alpha, A, lda, B, ldb, beta, C,
+                                                ldc);
+    } else {
+        tinyblasGE_entry<0, BM, BN, BK, VE, WM, WN, WNI, TM, TN, TT>
+            <<<blocks, TT, 0, handle->stream>>>(aT, bT, m, n, k, alpha, A, lda, B, ldb, beta, C,
+                                                ldc);
+    }
+    if (cudaGetLastError() != cudaSuccess)
+        return TINYBLAS_STATUS_EXECUTION_FAILED;
+    return TINYBLAS_STATUS_SUCCESS;
+}
+
+template <typename WORD, typename SRC, typename DST>
+tinyblasStatus_t tinyblasGE_launch(tinyblasHandle_t handle, tinyblasOperation_t aT,
+                                   tinyblasOperation_t bT, int m, int n, int k, WORD alpha,
+                                   const SRC *A, int lda, const SRC *B, int ldb, WORD beta, DST *C,
+                                   int ldc) {
+    if (can_use_matvec(aT, bT, m, n, k, alpha, beta))
+        return matvec_launch<WORD>(handle, m, k, A, lda, B, C);
+    constexpr int TT = 256;
+    constexpr int BM = 128;
+    constexpr int BN = 64;
+    constexpr int BK = 64;
+    constexpr int VE = 16;
+    constexpr int WM = 32;
+    constexpr int WN = 32;
+    constexpr int WNI = 1;
+    constexpr int TM = 8;
+    constexpr int TN = 4;
+    return tinyblasGE_launcher<BM, BN, BK, VE, WM, WN, WNI, TM, TN, TT>(
+        handle, bT, aT, n, m, k, alpha, B, ldb, A, lda, beta, C, ldc);
+}
+
+/**
+ * Performs extended general matrix multiplication.
+ *
+ * This is a column major GEMM subroutine for computing C = α*A*B + β*C.
+ *
+ * @param handle was created by tinyblasCreate()
+ * @param transa if `A` should be transposed
+ * @param transb if `B` should be transposed
+ * @param m is rows in `A` and `C`
+ * @param n is cols in `B` and `C`
+ * @param k is cols in `A` and rows in `B`
+ * @param alpha points to scalar that's multiplied against input
+ * @param A is input array of first matrix
+ * @param Atype is data type of `C`
+ * @param lda is row stride of `A`
+ * @param B is input array of second matrix
+ * @param Btype is data type of `C`
+ * @param ldb is row stride of `B`
+ * @param beta points to scalar that's multiplied against the existing
+ *     output, but this multiplication only happens if beta is nonzero
+ * @param C is input/output array of output matrix
+ * @param Ctype is data type of `C`
+ * @param ldc is row stride of `C`
+ * @param computeType is data type of `alpha`, `beta`, and dot product
+ * @param algo specifies algorithm to use
+ */
+tinyblasStatus_t tinyblasGemmEx(tinyblasHandle_t handle, //
+                                tinyblasOperation_t transa, //
+                                tinyblasOperation_t transb, //
+                                int m, int n, int k, //
+                                const void *alpha, //
+                                const void *A, tinyblasDataType_t Atype, int lda, //
+                                const void *B, tinyblasDataType_t Btype, int ldb, //
+                                const void *beta, //
+                                void *C, tinyblasDataType_t Ctype, int ldc, //
+                                tinyblasComputeType_t computeType, //
+                                tinyblasGemmAlgo_t algo) {
+
+    if (m < 0 || n < 0 || k < 0)
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (lda < std::max(1, transa ? k : m))
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (ldb < std::max(1, transb ? n : k))
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (ldc < std::max(1, m))
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (1ll * lda * ((transa ? k : m) - 1) + ((transa ? m : k) - 1) > INT_MAX)
+        return TINYBLAS_STATUS_DIMENSION_OVERFLOW;
+    if (1ll * ldb * ((transb ? n : k) - 1) + ((transb ? k : n) - 1) > INT_MAX)
+        return TINYBLAS_STATUS_DIMENSION_OVERFLOW;
+    if (1ll * ldc * (n - 1) + (m - 1) > INT_MAX)
+        return TINYBLAS_STATUS_DIMENSION_OVERFLOW;
+    if (algo != TINYBLAS_GEMM_DEFAULT)
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (Atype != Btype)
+        return TINYBLAS_STATUS_NOT_SUPPORTED;
+
+    switch (Atype) {
+    case TINYBLAS_R_16F:
+        switch (Ctype) {
+        case TINYBLAS_R_16F:
+            switch (computeType) {
+            case TINYBLAS_COMPUTE_16F:
+                return tinyblasGE_launch(
+                    handle, transa, transb, m, n, k, (float)*(const half *)alpha, (const half *)A,
+                    lda, (const half *)B, ldb, (float)*(const half *)beta, (half *)C, ldc);
+            case TINYBLAS_COMPUTE_32F:
+                return tinyblasGE_launch(handle, transa, transb, m, n, k, *(const float *)alpha,
+                                         (const half *)A, lda, (const half *)B, ldb,
+                                         *(const float *)beta, (half *)C, ldc);
+            default:
+                return TINYBLAS_STATUS_INVALID_VALUE;
+            }
+        case TINYBLAS_R_32F:
+            switch (computeType) {
+            case TINYBLAS_COMPUTE_16F:
+                return TINYBLAS_STATUS_NOT_SUPPORTED;
+            case TINYBLAS_COMPUTE_32F:
+                return tinyblasGE_launch(handle, transa, transb, m, n, k, *(const float *)alpha,
+                                         (const half *)A, lda, (const half *)B, ldb,
+                                         *(const float *)beta, (float *)C, ldc);
+            default:
+                return TINYBLAS_STATUS_INVALID_VALUE;
+            }
+        default:
+            return TINYBLAS_STATUS_INVALID_VALUE;
+        }
+    case TINYBLAS_R_32F:
+        switch (Ctype) {
+        case TINYBLAS_R_16F:
+            return TINYBLAS_STATUS_NOT_SUPPORTED;
+        case TINYBLAS_R_32F:
+            switch (computeType) {
+            case TINYBLAS_COMPUTE_16F:
+                return TINYBLAS_STATUS_NOT_SUPPORTED;
+            case TINYBLAS_COMPUTE_32F:
+                return tinyblasGE_launch(handle, transa, transb, m, n, k, *(const float *)alpha,
+                                         (const float *)A, lda, (const float *)B, ldb,
+                                         *(const float *)beta, (float *)C, ldc);
+            default:
+                return TINYBLAS_STATUS_INVALID_VALUE;
+            }
+        default:
+            return TINYBLAS_STATUS_INVALID_VALUE;
+        }
+    default:
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    }
+}
+
+template <typename WORD, typename SRC, typename DST>
+static __global__ __launch_bounds__(WARPSIZE) void matvecGBE_entry(int m, int k, //
+                                                                   const SRC *const A[], int lda,
+                                                                   const SRC *const B[],
+                                                                   DST *const C[]) {
+    matvec<WORD>(m, k, A[blockIdx.z], lda, B[blockIdx.z], C[blockIdx.z]);
+}
+
+template <int BM, int BN, int TM, int TN, typename WORD, typename SRC, typename DST>
+static __global__ void KERNEL tinyblasGBE_entry(tinyblasOperation_t transa,
+                                                tinyblasOperation_t transb, int m, int n, int k,
+                                                WORD alpha, const SRC *const Aarray[], int lda,
+                                                const SRC *const Barray[], int ldb, WORD beta,
+                                                DST *const Carray[], int ldc, int batchCount) {
+    matmul_block2d<0, BM, BN, TM, TN>(transa, transb, m, n, k, alpha, Aarray[blockIdx.z], lda,
+                                      Barray[blockIdx.z], ldb, beta, Carray[blockIdx.z], ldc);
+}
+
+template <typename WORD, typename SRC, typename DST>
+static tinyblasStatus_t tinyblasGBE_launch(tinyblasHandle_t handle, tinyblasOperation_t transa,
+                                           tinyblasOperation_t transb, int m, int n, int k,
+                                           WORD alpha, const SRC *const *Aarray, int lda,
+                                           const SRC *const *Barray, int ldb, WORD beta,
+                                           DST *const *Carray, int ldc, int batchCount) {
+    if (can_use_matvec(transa, transb, m, n, k, alpha, beta)) {
+        dim3 blocks(WARPSIZE, m / WARPSIZE, batchCount);
+        matvecGBE_entry<WORD>
+            <<<blocks, WARPSIZE, 0, handle->stream>>>(m, k, Aarray, lda, Barray, Carray);
+    } else {
+        constexpr int BM = 16;
+        constexpr int BN = 16;
+        constexpr int TM = 4;
+        constexpr int TN = 4;
+        dim3 blocks(CEIL_DIV(m, BM), CEIL_DIV(n, BN), batchCount);
+        tinyblasGBE_entry<BM, BN, TM, TN><<<blocks, THREAD_COUNT, 0, handle->stream>>>(
+            transa, transb, m, n, k, alpha, Aarray, lda, Barray, ldb, beta, Carray, ldc,
+            batchCount);
+    }
+    if (cudaGetLastError() != cudaSuccess)
+        return TINYBLAS_STATUS_EXECUTION_FAILED;
+    return TINYBLAS_STATUS_SUCCESS;
+}
+
+/**
+ * Multiplies matrices.
+ *
+ * This is a column major GEMM subroutine for computing C = α*A*B + β*C.
+ *
+ * @param handle was created by tinyblasCreate()
+ * @param transa if `A` should be transposed
+ * @param transb if `B` should be transposed
+ * @param m is rows in `A` and `C`
+ * @param n is cols in `B` and `C`
+ * @param k is cols in `A` and rows in `B`
+ * @param alpha points to scalar that's multiplied against input
+ * @param A is input array of device memory pointing to first matrices
+ * @param Atype is data type of `C`
+ * @param lda is row stride of `A`
+ * @param B is input array of device memory pointing to second matrices
+ * @param Btype is data type of `C`
+ * @param ldb is row stride of `B`
+ * @param beta points to scalar that's multiplied against the existing
+ *     output, but this multiplication only happens if beta is nonzero
+ * @param C is input/output array of output matrices
+ * @param Ctype is data type of `C`
+ * @param ldc is row stride of `C`
+ * @param batchCount is number of elements in `A`, `B`, and `C`
+ * @param computeType is data type of `alpha`, `beta`, and dot product
+ * @param algo specifies algorithm to use
+ */
+tinyblasStatus_t tinyblasGemmBatchedEx(tinyblasHandle_t handle, tinyblasOperation_t transa,
+                                       tinyblasOperation_t transb, int m, int n, int k,
+                                       const void *alpha, const void *const Aarray[],
+                                       tinyblasDataType_t Atype, int lda,
+                                       const void *const Barray[], tinyblasDataType_t Btype,
+                                       int ldb, const void *beta, void *const Carray[],
+                                       tinyblasDataType_t Ctype, int ldc, int batchCount,
+                                       tinyblasComputeType_t computeType, tinyblasGemmAlgo_t algo) {
+
+    if (m < 0 || n < 0 || k < 0)
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (lda < std::max(1, transa ? k : m))
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (ldb < std::max(1, transb ? n : k))
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (ldc < std::max(1, m))
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (1ll * lda * ((transa ? k : m) - 1) + ((transa ? m : k) - 1) > INT_MAX)
+        return TINYBLAS_STATUS_DIMENSION_OVERFLOW;
+    if (1ll * ldb * ((transb ? n : k) - 1) + ((transb ? k : n) - 1) > INT_MAX)
+        return TINYBLAS_STATUS_DIMENSION_OVERFLOW;
+    if (1ll * ldc * (n - 1) + (m - 1) > INT_MAX)
+        return TINYBLAS_STATUS_DIMENSION_OVERFLOW;
+    if (algo != TINYBLAS_GEMM_DEFAULT)
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (Atype != Btype)
+        return TINYBLAS_STATUS_NOT_SUPPORTED;
+
+    switch (Atype) {
+    case TINYBLAS_R_16F:
+        switch (Ctype) {
+        case TINYBLAS_R_16F:
+            switch (computeType) {
+            case TINYBLAS_COMPUTE_16F:
+                return tinyblasGBE_launch(
+                    handle, transa, transb, m, n, k, (float)*(const half *)alpha,
+                    (const half *const *)Aarray, lda, (const half *const *)Barray, ldb,
+                    (float)*(const half *)beta, (half *const *)Carray, ldc, batchCount);
+            case TINYBLAS_COMPUTE_32F:
+                return tinyblasGBE_launch(handle, transa, transb, m, n, k, *(const float *)alpha,
+                                          (const half *const *)Aarray, lda,
+                                          (const half *const *)Barray, ldb, *(const float *)beta,
+                                          (half *const *)Carray, ldc, batchCount);
+            default:
+                return TINYBLAS_STATUS_INVALID_VALUE;
+            }
+        case TINYBLAS_R_32F:
+            switch (computeType) {
+            case TINYBLAS_COMPUTE_16F:
+                return TINYBLAS_STATUS_NOT_SUPPORTED;
+            case TINYBLAS_COMPUTE_32F:
+                return tinyblasGBE_launch(handle, transa, transb, m, n, k, *(const float *)alpha,
+                                          (const half *const *)Aarray, lda,
+                                          (const half *const *)Barray, ldb, *(const float *)beta,
+                                          (float *const *)Carray, ldc, batchCount);
+            default:
+                return TINYBLAS_STATUS_INVALID_VALUE;
+            }
+        default:
+            return TINYBLAS_STATUS_INVALID_VALUE;
+        }
+    case TINYBLAS_R_32F:
+        switch (Ctype) {
+        case TINYBLAS_R_16F:
+            return TINYBLAS_STATUS_NOT_SUPPORTED;
+        case TINYBLAS_R_32F:
+            switch (computeType) {
+            case TINYBLAS_COMPUTE_16F:
+                return TINYBLAS_STATUS_NOT_SUPPORTED;
+            case TINYBLAS_COMPUTE_32F:
+                return tinyblasGBE_launch(handle, transa, transb, m, n, k, *(const float *)alpha,
+                                          (const float *const *)Aarray, lda,
+                                          (const float *const *)Barray, ldb, *(const float *)beta,
+                                          (float *const *)Carray, ldc, batchCount);
+            default:
+                return TINYBLAS_STATUS_INVALID_VALUE;
+            }
+        default:
+            return TINYBLAS_STATUS_INVALID_VALUE;
+        }
+    default:
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    }
+}
+
+template <typename WORD, typename SRC, typename DST>
+static __global__ __launch_bounds__(WARPSIZE) void matvecGSBE_entry(int m, int k, const SRC *A,
+                                                                    int lda, long long strideA,
+                                                                    const SRC *B, long long strideB,
+                                                                    DST *C, long long strideC) {
+    matvec<WORD>(m, k, A + blockIdx.z * strideA, lda, B + blockIdx.z * strideB,
+                 C + blockIdx.z * strideC);
+}
+
+template <int CONFIG, int BM, int BN, int TM, int TN, typename SRC, typename DST, typename WORD>
+static __global__ void KERNEL tinyblasGSBE_entry(tinyblasOperation_t transa,
+                                                 tinyblasOperation_t transb, int m, int n, int k,
+                                                 WORD alpha, const SRC *A, int lda,
+                                                 long long strideA, const SRC *B, int ldb,
+                                                 long long strideB, WORD beta, DST *C, int ldc,
+                                                 long long strideC, int batchCount) {
+    matmul_block2d<CONFIG, BM, BN, TM, TN>(transa, transb, m, n, k, alpha, A + strideA * blockIdx.z,
+                                           lda, B + strideB * blockIdx.z, ldb, beta,
+                                           C + strideC * blockIdx.z, ldc);
+}
+
+template <typename WORD, typename SRC, typename DST>
+static tinyblasStatus_t tinyblasGSBE_launch(tinyblasHandle_t handle, tinyblasOperation_t transa,
+                                            tinyblasOperation_t transb, int m, int n, int k,
+                                            WORD alpha, const SRC *A, int lda, long long strideA,
+                                            const SRC *B, int ldb, long long strideB, WORD beta,
+                                            DST *C, int ldc, long long strideC, int batchCount) {
+    if (can_use_matvec(transa, transb, m, n, k, alpha, beta)) {
+        dim3 blocks(WARPSIZE, m / WARPSIZE, batchCount);
+        matvecGSBE_entry<WORD><<<blocks, WARPSIZE, 0, handle->stream>>>(m, k, A, lda, strideA, B,
+                                                                        strideB, C, strideC);
+    } else {
+        constexpr int BM = 16;
+        constexpr int BN = 16;
+        constexpr int TM = 4;
+        constexpr int TN = 4;
+        constexpr int BK = THREAD_COUNT;
+        dim3 blocks(CEIL_DIV(m, BM), CEIL_DIV(n, BN), batchCount);
+        if ((!beta && //
+             isone(alpha) && //
+             m % BM == 0 && //
+             k % BK == 0 && //
+             transa == TINYBLAS_OP_T && //
+             transb == TINYBLAS_OP_N)) {
+            constexpr int CONFIG = IGNORE_BETA | IGNORE_ALPHA | ASSUME_A_OP_T | ASSUME_B_OP_N |
+                                   ASSUME_M_SAFE | ASSUME_K_SAFE;
+            tinyblasGSBE_entry<CONFIG, BM, BN, TM, TN><<<blocks, THREAD_COUNT, 0, handle->stream>>>(
+                transa, transb, m, n, k, alpha, A, lda, strideA, B, ldb, strideB, beta, C, ldc,
+                strideC, batchCount);
+        } else {
+            tinyblasGSBE_entry<0, BM, BN, TM, TN><<<blocks, THREAD_COUNT, 0, handle->stream>>>(
+                transa, transb, m, n, k, alpha, A, lda, strideA, B, ldb, strideB, beta, C, ldc,
+                strideC, batchCount);
+        }
+    }
+    if (cudaGetLastError() != cudaSuccess)
+        return TINYBLAS_STATUS_EXECUTION_FAILED;
+    return TINYBLAS_STATUS_SUCCESS;
+}
+
+/**
+ * Multiplies matrices.
+ *
+ * This is a column major GEMM subroutine for computing C = α*A*B + β*C.
+ *
+ * @param handle was created by tinyblasCreate()
+ * @param transa if `A` should be transposed
+ * @param transb if `B` should be transposed
+ * @param m is rows in `A` and `C`
+ * @param n is cols in `B` and `C`
+ * @param k is cols in `A` and rows in `B`
+ * @param alpha points to scalar that's multiplied against input
+ * @param A is input array of first matrices
+ * @param Atype is data type of `A`
+ * @param lda is row stride of `A`
+ * @param strideA is distance between matrices in `A`
+ * @param B is input array of second matrices
+ * @param Btype is data type of `B`
+ * @param ldb is row stride of `B`
+ * @param strideB is distance between matrices in `B`
+ * @param beta points to scalar that's multiplied against the existing
+ *     output, but this multiplication only happens if beta is nonzero
+ * @param C is input/output array of output matrices
+ * @param Ctype is data type of `C`
+ * @param ldc is row stride of `C`
+ * @param strideC is distance between matrices in `C`, which must not overlap
+ * @param batchCount is number of matrices to multiply
+ * @param computeType is data type of `alpha`, `beta`, and dot product
+ * @param algo specifies algorithm to use
+ */
+tinyblasStatus_t tinyblasGemmStridedBatchedEx(tinyblasHandle_t handle, //
+                                              tinyblasOperation_t transa, //
+                                              tinyblasOperation_t transb, //
+                                              int m, int n, int k, //
+                                              const void *alpha, //
+                                              const void *A, tinyblasDataType_t Atype, int lda,
+                                              long long strideA, //
+                                              const void *B, tinyblasDataType_t Btype, int ldb,
+                                              long long strideB, //
+                                              const void *beta, //
+                                              void *C, tinyblasDataType_t Ctype, int ldc,
+                                              long long strideC, //
+                                              int batchCount, //
+                                              tinyblasComputeType_t computeType, //
+                                              tinyblasGemmAlgo_t algo) {
+
+    if (m < 0 || n < 0 || k < 0)
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (lda < std::max(1, transa ? k : m))
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (ldb < std::max(1, transb ? n : k))
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (ldc < std::max(1, m))
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (std::max(0ll, strideC) < std::min(1ll * ldc * n, strideC * 2))
+        return TINYBLAS_STATUS_DIMENSION_OVERLAP;
+    if (1ll * lda * ((transa ? k : m) - 1) + ((transa ? m : k) - 1) > INT_MAX)
+        return TINYBLAS_STATUS_DIMENSION_OVERFLOW;
+    if (1ll * ldb * ((transb ? n : k) - 1) + ((transb ? k : n) - 1) > INT_MAX)
+        return TINYBLAS_STATUS_DIMENSION_OVERFLOW;
+    if (1ll * ldc * (n - 1) + (m - 1) > INT_MAX)
+        return TINYBLAS_STATUS_DIMENSION_OVERFLOW;
+    if (algo != TINYBLAS_GEMM_DEFAULT)
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    if (Atype != Btype)
+        return TINYBLAS_STATUS_NOT_SUPPORTED;
+
+    switch (Atype) {
+    case TINYBLAS_R_16F:
+        switch (Ctype) {
+        case TINYBLAS_R_16F:
+            switch (computeType) {
+            case TINYBLAS_COMPUTE_16F:
+                return tinyblasGSBE_launch(
+                    handle, transa, transb, m, n, k, (float)*(const half *)alpha, (const half *)A,
+                    lda, strideA, (const half *)B, ldb, strideB, (float)*(const half *)beta,
+                    (half *)C, ldc, strideC, batchCount);
+            case TINYBLAS_COMPUTE_32F:
+                return tinyblasGSBE_launch(handle, transa, transb, m, n, k, *(const float *)alpha,
+                                           (const half *)A, lda, strideA, (const half *)B, ldb,
+                                           strideB, *(const float *)beta, (half *)C, ldc, strideC,
+                                           batchCount);
+            default:
+                return TINYBLAS_STATUS_INVALID_VALUE;
+            }
+        case TINYBLAS_R_32F:
+            switch (computeType) {
+            case TINYBLAS_COMPUTE_16F:
+                return TINYBLAS_STATUS_NOT_SUPPORTED;
+            case TINYBLAS_COMPUTE_32F:
+                return tinyblasGSBE_launch(handle, transa, transb, m, n, k, *(const float *)alpha,
+                                           (const half *)A, lda, strideA, (const half *)B, ldb,
+                                           strideB, *(const float *)beta, (float *)C, ldc, strideC,
+                                           batchCount);
+            default:
+                return TINYBLAS_STATUS_INVALID_VALUE;
+            }
+        default:
+            return TINYBLAS_STATUS_INVALID_VALUE;
+        }
+    case TINYBLAS_R_32F:
+        switch (Ctype) {
+        case TINYBLAS_R_16F:
+            return TINYBLAS_STATUS_NOT_SUPPORTED;
+        case TINYBLAS_R_32F:
+            switch (computeType) {
+            case TINYBLAS_COMPUTE_16F:
+                return TINYBLAS_STATUS_NOT_SUPPORTED;
+            case TINYBLAS_COMPUTE_32F:
+                return tinyblasGSBE_launch(handle, transa, transb, m, n, k, *(const float *)alpha,
+                                           (const float *)A, lda, strideA, (const float *)B, ldb,
+                                           strideB, *(const float *)beta, (float *)C, ldc, strideC,
+                                           batchCount);
+            default:
+                return TINYBLAS_STATUS_INVALID_VALUE;
+            }
+        default:
+            return TINYBLAS_STATUS_INVALID_VALUE;
+        }
+    default:
+        return TINYBLAS_STATUS_INVALID_VALUE;
+    }
+}
diff --git a/ggml/src/ggml-cuda/tinyblas.h b/ggml/src/ggml-cuda/tinyblas.h
new file mode 100644
index 000000000..d6c2aae1a
--- /dev/null
+++ b/ggml/src/ggml-cuda/tinyblas.h
@@ -0,0 +1,66 @@
+// -*- mode:c++;indent-tabs-mode:nil;c-basic-offset:4;coding:utf-8 -*-
+// vi: set et ft=cpp ts=4 sts=4 sw=4 fenc=utf-8 :vi
+#pragma once
+
+typedef enum tinyblasOperation {
+    TINYBLAS_OP_N,
+    TINYBLAS_OP_T,
+} tinyblasOperation_t;
+
+typedef enum tinyblasDataType {
+    TINYBLAS_R_32F,
+    TINYBLAS_R_16F,
+} tinyblasDataType_t;
+
+typedef enum tinyblasComputeType {
+    TINYBLAS_COMPUTE_32F,
+    TINYBLAS_COMPUTE_16F,
+} tinyblasComputeType_t;
+
+typedef enum tinyblasGemmAlgo {
+    TINYBLAS_GEMM_DEFAULT,
+} tinyblasGemmAlgo_t;
+
+typedef enum tinyblasStatus {
+    TINYBLAS_STATUS_SUCCESS,
+    TINYBLAS_STATUS_ALLOC_FAILED,
+    TINYBLAS_STATUS_INVALID_VALUE,
+    TINYBLAS_STATUS_NOT_SUPPORTED,
+    TINYBLAS_STATUS_EXECUTION_FAILED,
+    TINYBLAS_STATUS_DIMENSION_OVERLAP,
+    TINYBLAS_STATUS_DIMENSION_OVERFLOW,
+} tinyblasStatus_t;
+
+struct tinyblasContext;
+typedef struct tinyblasContext *tinyblasHandle_t;
+
+const char *tinyblasGetStatusString(tinyblasStatus_t);
+
+tinyblasStatus_t tinyblasCreate(tinyblasHandle_t *);
+tinyblasStatus_t tinyblasDestroy(tinyblasHandle_t);
+tinyblasStatus_t tinyblasSetStream(tinyblasHandle_t, void *);
+tinyblasStatus_t tinyblasGetStream(tinyblasHandle_t, void **);
+
+tinyblasStatus_t tinyblasSgemm(tinyblasHandle_t, tinyblasOperation_t, tinyblasOperation_t, int, int,
+                               int, const float *, const float *, int, const float *, int,
+                               const float *, float *, int);
+
+tinyblasStatus_t tinyblasGemmEx(tinyblasHandle_t, tinyblasOperation_t, tinyblasOperation_t, int,
+                                int, int, const void *, const void *, tinyblasDataType_t, int,
+                                const void *, tinyblasDataType_t, int, const void *, void *,
+                                tinyblasDataType_t, int, tinyblasComputeType_t, tinyblasGemmAlgo_t);
+
+tinyblasStatus_t tinyblasGemmBatchedEx(tinyblasHandle_t, tinyblasOperation_t, tinyblasOperation_t,
+                                       int, int, int, const void *, const void *const[],
+                                       tinyblasDataType_t, int, const void *const[],
+                                       tinyblasDataType_t, int, const void *, void *const[],
+                                       tinyblasDataType_t, int, int, tinyblasComputeType_t,
+                                       tinyblasGemmAlgo_t);
+
+tinyblasStatus_t tinyblasGemmStridedBatchedEx(tinyblasHandle_t, tinyblasOperation_t,
+                                              tinyblasOperation_t, int, int, int, const void *,
+                                              const void *, tinyblasDataType_t, int, long long,
+                                              const void *, tinyblasDataType_t, int, long long,
+                                              const void *, void *, tinyblasDataType_t, int,
+                                              long long, int, tinyblasComputeType_t,
+                                              tinyblasGemmAlgo_t);
diff --git a/ggml/src/ggml-cuda/vendors/cuda.h b/ggml/src/ggml-cuda/vendors/cuda.h
index 3b3086778..5a69c0ec2 100644
--- a/ggml/src/ggml-cuda/vendors/cuda.h
+++ b/ggml/src/ggml-cuda/vendors/cuda.h
@@ -1,10 +1,22 @@
-#pragma once
-
 #include <cuda_runtime.h>
 #include <cuda.h>
+
+#ifndef GGML_USE_TINYBLAS // 1
 #include <cublas_v2.h>
-#include <cuda_bf16.h>
+#endif //GGML_USE_TINYBLAS 1
+
 #include <cuda_fp16.h>
+#include <cuda_bf16.h>
+
+#ifdef GGML_USE_TINYBLAS // 2
+
+#include "tinyblas_vendor.h"
+#define cublasGemmAlgo_t tinyblasGemmAlgo_t
+#define cublasOperation_t tinyblasOperation_t
+
+#else
+
+#define BLAS_NAME GGML_CUBLAS_NAME
 
 #if CUDART_VERSION >= 12050
 #include <cuda_fp8.h>
@@ -17,3 +29,5 @@
 #define CUBLAS_COMPUTE_32F CUDA_R_32F
 #define cublasComputeType_t cudaDataType_t
 #endif // CUDART_VERSION < 11020
+
+#endif // GGML_USE_TINYBLAS 2
diff --git a/ggml/src/ggml-cuda/vendors/hip.h b/ggml/src/ggml-cuda/vendors/hip.h
index 890c10364..7fef5d23a 100644
--- a/ggml/src/ggml-cuda/vendors/hip.h
+++ b/ggml/src/ggml-cuda/vendors/hip.h
@@ -1,8 +1,4 @@
-#pragma once
-
-#define HIP_DISABLE_WARP_SYNC_BUILTINS 1
 #include <hip/hip_runtime.h>
-#include <hipblas/hipblas.h>
 #include <hip/hip_fp16.h>
 #include <hip/hip_bf16.h>
 
@@ -10,6 +6,17 @@
 #include <rocwmma/rocwmma-version.hpp>
 #endif // defined(GGML_HIP_ROCWMMA_FATTN)
 
+#ifdef GGML_USE_TINYBLAS
+
+#include "tinyblas_vendor.h"
+#define CUBLAS_COMPUTE_32F_FAST_16F TINYBLAS_COMPUTE_32F
+#define CUBLAS_TF32_TENSOR_OP_MATH 0
+
+#else
+
+#define BLAS_NAME GGML_CUBLAS_NAME
+
+#include <hipblas/hipblas.h>
 #define CUBLAS_GEMM_DEFAULT HIPBLAS_GEMM_DEFAULT
 #define CUBLAS_GEMM_DEFAULT_TENSOR_OP HIPBLAS_GEMM_DEFAULT
 #define CUBLAS_OP_N HIPBLAS_OP_N
@@ -19,17 +26,24 @@
 #define CUDA_R_16F  HIPBLAS_R_16F
 #define CUDA_R_16BF HIPBLAS_R_16B
 #define CUDA_R_32F  HIPBLAS_R_32F
+#endif //GGML_USE_TINYBLAS
+
+#ifndef GGML_USE_TINYBLAS //2
 #define CU_DEVICE_ATTRIBUTE_VIRTUAL_MEMORY_MANAGEMENT_SUPPORTED hipDeviceAttributeVirtualMemoryManagementSupported
 #define CU_MEM_ALLOC_GRANULARITY_RECOMMENDED hipMemAllocationGranularityRecommended
 #define CU_MEM_ALLOCATION_TYPE_PINNED hipMemAllocationTypePinned
 #define CU_MEM_LOCATION_TYPE_DEVICE hipMemLocationTypeDevice
 #define CU_MEM_ACCESS_FLAGS_PROT_READWRITE hipMemAccessFlagsProtReadWrite
 #define CU_CHECK(fn) {hipError_t err = fn; if(err != hipSuccess) { GGML_ABORT("HipVMM Failure: %s\n", hipGetErrorString(err)); }}
+#endif //GGML_USE_TINYBLAS 2
+
 #define __shfl_sync(mask, var, laneMask, width) __shfl(var, laneMask, width)
 #define __shfl_up_sync(mask, var, laneMask, width) __shfl_up(var, laneMask, width)
 #define __shfl_xor_sync(mask, var, laneMask, width) __shfl_xor(var, laneMask, width)
 #define __all_sync(mask, var) __all(var)
 #define __any_sync(mask, var) __any(var)
+
+#ifndef GGML_USE_TINYBLAS //3
 #define cublasCreate hipblasCreate
 #define cublasDestroy hipblasDestroy
 #define cublasGemmEx hipblasGemmEx
@@ -41,6 +55,8 @@
 #define cublasSgemm hipblasSgemm
 #define cublasStatus_t hipblasStatus_t
 #define cublasOperation_t hipblasOperation_t
+#endif //GGML_USE_TINYBLAS 3
+
 #define cudaDeviceCanAccessPeer hipDeviceCanAccessPeer
 #define cudaDeviceDisablePeerAccess hipDeviceDisablePeerAccess
 #define cudaDeviceEnablePeerAccess hipDeviceEnablePeerAccess
@@ -84,6 +100,8 @@
 #define cudaMemGetInfo hipMemGetInfo
 #define cudaOccupancyMaxPotentialBlockSize hipOccupancyMaxPotentialBlockSize
 #define cudaSetDevice hipSetDevice
+
+#ifndef GGML_USE_TINYBLAS //4
 #define cuDeviceGet hipDeviceGet
 #define CUdevice hipDevice_t
 #define CUdeviceptr hipDeviceptr_t
@@ -99,6 +117,8 @@
 #define cuMemGetAllocationGranularity hipMemGetAllocationGranularity
 #define CUmemAllocationProp hipMemAllocationProp
 #define cuDeviceGetAttribute hipDeviceGetAttribute
+#endif //GGML_USE_TINYBLAS 4
+
 #define cudaStreamCreateWithFlags hipStreamCreateWithFlags
 #define cudaStreamDestroy hipStreamDestroy
 #define cudaStreamFireAndForget hipStreamFireAndForget
@@ -106,6 +126,8 @@
 #define cudaStreamPerThread hipStreamPerThread
 #define cudaStreamSynchronize hipStreamSynchronize
 #define cudaStreamWaitEvent(stream, event, flags) hipStreamWaitEvent(stream, event, flags)
+
+#ifndef GGML_USE_TINYBLAS //5
 #define cudaGraphExec_t hipGraphExec_t
 #define cudaGraphNode_t hipGraphNode_t
 #define cudaKernelNodeParams hipKernelNodeParams
@@ -128,10 +150,14 @@
 #define cudaStreamCaptureModeRelaxed hipStreamCaptureModeRelaxed
 #define cudaStreamBeginCapture hipStreamBeginCapture
 #define cudaGraph_t hipGraph_t
+#endif //GGML_USE_TINYBLAS 5
+
 #define cudaStream_t hipStream_t
 #define cudaSuccess hipSuccess
 #define cudaOccupancyMaxActiveBlocksPerMultiprocessor hipOccupancyMaxActiveBlocksPerMultiprocessor
 #define __trap() do { abort(); __builtin_unreachable(); } while(0)
+
+#ifndef GGML_USE_TINYBLAS //6
 #define CUBLAS_STATUS_SUCCESS HIPBLAS_STATUS_SUCCESS
 #define CUBLAS_STATUS_NOT_INITIALIZED HIPBLAS_STATUS_NOT_INITIALIZED
 #define CUBLAS_STATUS_ALLOC_FAILED HIPBLAS_STATUS_ALLOC_FAILED
@@ -141,6 +167,7 @@
 #define CUBLAS_STATUS_EXECUTION_FAILED HIPBLAS_STATUS_EXECUTION_FAILED
 #define CUBLAS_STATUS_INTERNAL_ERROR HIPBLAS_STATUS_INTERNAL_ERROR
 #define CUBLAS_STATUS_NOT_SUPPORTED HIPBLAS_STATUS_NOT_SUPPORTED
+#endif //GGML_USE_TINYBLAS 6
 
 #if HIP_VERSION >= 60500000
 #define CUBLAS_COMPUTE_16F HIPBLAS_COMPUTE_16F
diff --git a/ggml/src/ggml-cuda/vendors/tinyblas_vendor.h b/ggml/src/ggml-cuda/vendors/tinyblas_vendor.h
new file mode 100644
index 000000000..0ddcf366c
--- /dev/null
+++ b/ggml/src/ggml-cuda/vendors/tinyblas_vendor.h
@@ -0,0 +1,27 @@
+#pragma once
+
+#include "../tinyblas.h"
+#define CUBLAS_COMPUTE_16F TINYBLAS_COMPUTE_16F
+#define CUBLAS_COMPUTE_32F TINYBLAS_COMPUTE_32F
+#define CUBLAS_OP_N TINYBLAS_OP_N
+#define CUBLAS_OP_T TINYBLAS_OP_T
+#define CUDA_R_16F TINYBLAS_R_16F
+#define CUDA_R_32F TINYBLAS_R_32F
+#define CUBLAS_GEMM_DEFAULT TINYBLAS_GEMM_DEFAULT
+#define CUBLAS_GEMM_DEFAULT_TENSOR_OP TINYBLAS_GEMM_DEFAULT
+#define CUBLAS_STATUS_SUCCESS TINYBLAS_STATUS_SUCCESS
+#define cublasGemmAlgo_t tinyblasGemmAlgo_t
+#define cublasOperation_t tinyblasOperation_t
+#define cublasComputeType_t tinyblasComputeType_t
+#define cublasHandle_t tinyblasHandle_t
+#define cublasStatus_t tinyblasStatus_t
+#define cublasSgemm tinyblasSgemm
+#define cublasGemmEx tinyblasGemmEx
+#define cublasCreate tinyblasCreate
+#define cublasDestroy tinyblasDestroy
+#define cublasSetStream tinyblasSetStream
+#define cublasGemmBatchedEx tinyblasGemmBatchedEx
+#define cublasGemmStridedBatchedEx tinyblasGemmStridedBatchedEx
+#define cublasGetStatusString tinyblasGetStatusString
+#define cudaDataType_t tinyblasDataType_t
+#define cublasSetMathMode(handle, mode) CUBLAS_STATUS_SUCCESS
diff --git a/ggml/src/ggml-hip/CMakeLists.txt b/ggml/src/ggml-hip/CMakeLists.txt
index 23b688991..eb391f92f 100644
--- a/ggml/src/ggml-hip/CMakeLists.txt
+++ b/ggml/src/ggml-hip/CMakeLists.txt
@@ -56,12 +56,27 @@ file(GLOB   GGML_HEADERS_ROCM "../ggml-cuda/*.cuh")
 list(APPEND GGML_HEADERS_ROCM "../../include/ggml-cuda.h")
 
 file(GLOB   GGML_SOURCES_ROCM "../ggml-cuda/*.cu")
+
+if (GGML_USE_TINYBLAS)
+    file(GLOB REMOVE_SOURCES_ROCM "../ggml-cuda/mm*f.cu")
+else()
+    file(GLOB REMOVE_SOURCES_ROCM "../ggml-cuda/mmv.cu")
+endif()
+list(REMOVE_ITEM GGML_SOURCES_ROCM ${REMOVE_SOURCES_ROCM})
+
+file(GLOB   SRCS "../ggml-cuda/template-instances/mmq-instance-q*.cu")
+list(APPEND GGML_SOURCES_ROCM "${SRCS};../ggml-cuda/template-instances/mmq-instance-mxfp4.cu")
+
+if (NOT GGML_CUDA_NO_IQUANTS)
+file(GLOB   SRCS "../ggml-cuda/template-instances/mmq-instance-i*.cu")
+list(APPEND GGML_SOURCES_ROCM ${SRCS})
+endif()
+
+if (GGML_CUDA_FA)
 file(GLOB   SRCS "../ggml-cuda/template-instances/fattn-tile*.cu")
 list(APPEND GGML_SOURCES_ROCM ${SRCS})
 file(GLOB   SRCS "../ggml-cuda/template-instances/fattn-mma*.cu")
 list(APPEND GGML_SOURCES_ROCM ${SRCS})
-file(GLOB   SRCS "../ggml-cuda/template-instances/mmq*.cu")
-list(APPEND GGML_SOURCES_ROCM ${SRCS})
 
 if (GGML_CUDA_FA_ALL_QUANTS)
     file(GLOB   SRCS "../ggml-cuda/template-instances/fattn-vec*.cu")
@@ -75,6 +90,9 @@ else()
     file(GLOB   SRCS "../ggml-cuda/template-instances/fattn-vec*f16-f16.cu")
     list(APPEND GGML_SOURCES_ROCM ${SRCS})
 endif()
+endif()
+
+message(STATUS "GGML_SOURCES_ROCM: ${GGML_SOURCES_ROCM}")
 
 ggml_add_backend_library(ggml-hip
                          ${GGML_HEADERS_ROCM}
@@ -124,6 +142,10 @@ if (NOT GGML_CUDA_FA)
     add_compile_definitions(GGML_CUDA_NO_FA)
 endif()
 
+if (GGML_CUDA_NO_IQUANTS)
+    add_compile_definitions(GGML_CUDA_NO_IQUANTS)
+endif()
+
 if (CXX_IS_HIPCC)
     set_source_files_properties(${GGML_SOURCES_ROCM} PROPERTIES LANGUAGE CXX)
     target_link_libraries(ggml-hip PRIVATE hip::device)
